{
  "id": "ai-course-dev-productivity",
  "title": "Developer Productivity using Artificial Intelligence",
  "description": "A 10-week course on building generative AI agents and multi-agent systems",
  "weeks": [
    {
      "id": "week-1",
      "title": "Week 1: LLM Foundations and Basic Prompting",
      "description": "Get started with AI concepts, tools, and your development environment.",
      "lessons": [
        {
          "id": "lesson-1-1",
          "title": "Introduction to LLMs and Generative AI",
          "duration": "1:20:14",
          "videoUrl": "/videos/cohort_2/week_01/week_1_class_1_2024-05-20.mp4",
          "videoId": "week_1_class_1_2024-05-20",
          "content": "# Introduction to LLMs and Generative AI\n\n## Course Overview\n\nThis course will help you acquire the knowledge to design, build, and deploy generative AI agents and multi-agent systems in the context of your company. We'll focus on increasing developer efficiency through:\n\n- Live instruction and interactive learning\n- Guided projects for hands-on experience\n- Building implementations of agents and agent systems\n- A capstone project implementing multi-agent systems\n\n## Course Structure\n\nThe course is divided into four major components:\n\n1. **Opening Session & LLM Overview** (Weeks 1-2)\n   - LLM fundamentals and intuition\n   - Open source LLMs\n   - LangChain and LangSmith introduction\n\n2. **Retrieval Augmented Generation (RAG)** (Weeks 3-4)\n   - Understanding RAG benefits\n   - Mitigating hallucination\n   - Implementation techniques\n\n3. **Chaining** (Weeks 5-6)\n   - Chaining multiple tools within LLMs\n   - Increasing response efficacy\n   - Building complex workflows\n\n4. **Agents and Multi-Agent Systems** (Weeks 7-10)\n   - Building agents using AWS Bedrock\n   - Production environment deployment\n   - Multi-agent systems with Crew.ai and LangChain\n   - Scaling multi-agent systems\n\n## LLM Intuition and Fundamentals\n\n### Understanding LLMs as Probabilistic Models\n\nLarge Language Models (LLMs) are essentially statistical models that understand the relationship between text and probability. They're not perfect, but they can seem magical due to their sophisticated pattern recognition.\n\n### Markov Chains: A Simple Foundation\n\nTo understand LLM intuition, we start with a very simple model - the Markov chain. While not exactly an LLM, it helps demonstrate the core concept of probabilistic text generation.\n\n**Key Concepts:**\n- Markov chains predict the next element in a sequence based on the current state\n- They can be applied to any sequence (text, DNA, etc.)\n- The prediction is based on probability distributions learned from training data\n\n### From Markov Chains to Modern LLMs\n\nWhile Markov chains look at individual tokens, modern LLMs:\n- Use context windows instead of single tokens\n- Consider entire contexts when predicting the next token\n- Apply probabilistic methods rather than random selection\n- Have evolved significantly since early models like Word2Vec (2013) to modern transformers\n\n## Prompting Fundamentals\n\n### Zero-Shot, One-Shot, and Few-Shot Prompting\n\n**Zero-Shot Prompting:**\n- Direct instruction without examples\n- Simple prompt with expectation of response\n- May not always produce desired results\n\n**One-Shot Prompting:**\n- Provide one example to guide the model\n- Helps establish pattern and format\n- More reliable than zero-shot\n\n**Few-Shot Prompting:**\n- Multiple examples to establish clear patterns\n- Significantly improves response quality\n- Helps model understand exact requirements\n\n### Building Context in Prompts\n\n#### Role Assignment\nAssigning specific roles to LLMs dramatically improves response quality:\n- Define the LLM's expertise (e.g., \"technical support specialist\")\n- Provide context about expected behavior\n- Create consistent persona for interactions\n\n#### Prompt Decoration\nEnhance prompts with additional contextual information:\n- Include relevant schemas (database structures, API formats)\n- Provide background information\n- Specify output formats and requirements\n\n#### Best Practices for Context\n- Be as specific as possible about requirements\n- Provide examples of desired output\n- Include relevant domain knowledge\n- Specify formatting preferences\n\n## Chat History and Context Management\n\n### Understanding Context Windows\n- LLMs maintain context through chat history\n- Older context may be discarded due to size limitations\n- Context window sizes vary by model and are well-documented\n- Larger context windows generally produce better results\n\n### Context Continuity\n- Creating new chat sessions resets context\n- Important to maintain relevant context for ongoing tasks\n- Template prompts can help reset context efficiently\n- System prompts initialize LLM behavior\n\n### Managing Long Conversations\n- Summarization techniques for long contexts\n- Proactive context management\n- Preventing conversations from going stale\n- Strategic context preservation\n\n## Message Types and Structure\n\n### System Messages\n- Initialize LLM with context before user interaction\n- Set role, behavior, and expectations\n- Persistent throughout the session\n\n### User Messages\n- Direct interactions and queries\n- Contain contextual information and specific tasks\n- Build upon system message foundation\n\n### Best Practices\n- Always use both system and user messages\n- Keep system messages clear and specific\n- Provide comprehensive context in user messages\n- Maintain consistent communication patterns",
          "codeExamples": [
            {
              "id": "code-1-1-1",
              "title": "Simple Markov Chain Implementation",
              "language": "python",
              "code": "from collections import defaultdict\nimport random\n\ndef build_markov_chain(text):\n    \"\"\"Build a Markov chain dictionary from input text\"\"\"\n    words = text.split()\n    markov_dict = defaultdict(list)\n    \n    # Build dictionary of word -> following words\n    for i in range(len(words) - 1):\n        current_word = words[i]\n        next_word = words[i + 1]\n        markov_dict[current_word].append(next_word)\n    \n    return markov_dict\n\ndef generate_text(markov_dict, start_word, length=100):\n    \"\"\"Generate text using the Markov chain\"\"\"\n    if start_word not in markov_dict:\n        return \"Start word not found in dictionary\"\n    \n    result = [start_word]\n    current_word = start_word\n    \n    for _ in range(length - 1):\n        if current_word not in markov_dict:\n            break\n        \n        # Randomly choose next word from possibilities\n        next_word = random.choice(markov_dict[current_word])\n        result.append(next_word)\n        current_word = next_word\n    \n    return ' '.join(result)\n\n# Example usage\nsample_text = \"The cat sat on the mat. The cat was happy. The mat was comfortable.\"\nchain = build_markov_chain(sample_text)\ngenerated = generate_text(chain, \"The\", 20)\nprint(generated)"
            },
            {
              "id": "code-1-1-2",
              "title": "Role Assignment Example",
              "language": "python",
              "code": "from langchain_openai import ChatOpenAI\nfrom langchain.schema import HumanMessage, SystemMessage\n\n# Initialize the LLM\nllm = ChatOpenAI(temperature=0.7)\n\n# Create system message for role assignment\nsystem_message = SystemMessage(\n    content=\"You are a technical support specialist with expertise in troubleshooting software issues. Provide clear, step-by-step solutions and ask clarifying questions when needed.\"\n)\n\n# User's problem message\nuser_message = HumanMessage(\n    content=\"My application keeps crashing when I try to upload large files. What could be causing this?\"\n)\n\n# Get response with role context\nresponse = llm.invoke([system_message, user_message])\nprint(response.content)"
            },
            {
              "id": "code-1-1-3",
              "title": "Few-Shot Prompting Example",
              "language": "python",
              "code": "# Few-shot prompting for business name generation\nsystem_prompt = \"You are a creative strategist known for coming up with memorable company names.\"\n\nfew_shot_examples = \"\"\"\nExamples of good company names:\n- For a coffee shop: \"Bean There, Done That\"\n- For a bookstore: \"Novel Ideas\"\n- For a fitness center: \"Flex Appeal\"\n\nGenerate a creative name for: \"\"\"\n\nuser_query = \"a company that makes colorful socks\"\n\nfull_prompt = system_prompt + \"\\n\\n\" + few_shot_examples + user_query\n\n# This would result in more creative and consistent naming\n# compared to zero-shot prompting"
            }
          ],
          "checklist": [
            {
              "id": "check-1-1-1",
              "text": "Understand the probabilistic nature of LLMs and how they generate text",
              "completed": false
            },
            {
              "id": "check-1-1-2",
              "text": "Learn the difference between Markov chains and modern LLM context windows",
              "completed": false
            },
            {
              "id": "check-1-1-3",
              "text": "Practice zero-shot, one-shot, and few-shot prompting techniques",
              "completed": false
            },
            {
              "id": "check-1-1-4",
              "text": "Implement role assignment to improve LLM response quality",
              "completed": false
            },
            {
              "id": "check-1-1-5",
              "text": "Master context building through prompt decoration and examples",
              "completed": false
            },
            {
              "id": "check-1-1-6",
              "text": "Understand chat history management and context window limitations",
              "completed": false
            },
            {
              "id": "check-1-1-7",
              "text": "Learn to structure system and user messages effectively",
              "completed": false
            },
            {
              "id": "check-1-1-8",
              "text": "Practice maintaining context continuity across conversations",
              "completed": false
            }
          ],
          "timestamps": [
            {
              "time": "0:10:26",
              "label": "Course Introduction",
              "description": "Course introduction and instructor presentation"
            },
            {
              "time": "0:17:49",
              "label": "Course Objectives",
              "description": "Course objectives: building generative AI agents and multi-agent systems"
            },
            {
              "time": "0:19:24",
              "label": "Course Structure",
              "description": "Course structure overview: LLM foundations, RAG, chaining, and agents"
            },
            {
              "time": "0:20:14",
              "label": "LLM Intuition",
              "description": "LLM intuition and Markov chain explanation begins"
            },
            {
              "time": "0:26:40",
              "label": "Markov Chain Demo",
              "description": "Markov chain code demonstration and explanation"
            },
            {
              "time": "0:31:26",
              "label": "Prompting Techniques",
              "description": "Zero-shot, one-shot, and few-shot prompting introduction"
            },
            {
              "time": "0:34:01",
              "label": "Context & Roles",
              "description": "Context in prompts and role assignment demonstration"
            },
            {
              "time": "0:39:35",
              "label": "Chat History",
              "description": "Chat history and context window management"
            },
            {
              "time": "0:47:48",
              "label": "Message Types",
              "description": "System vs user messages and best practices"
            },
            {
              "time": "0:56:45",
              "label": "Live Coding",
              "description": "Live coding demonstration with system prompts"
            },
            {
              "time": "1:01:04",
              "label": "Q&A Session",
              "description": "Interactive examples and Q&A session"
            }
          ],
          "videoPath": "cohort_2/week_1_class_1_2024-05-20.mp4"
        },
        {
          "id": "lesson-1-2",
          "title": "Working with Open Source LLMs",
          "duration": "0:49:03",
          "videoUrl": "/videos/cohort_2/week_01/week_1_class_2_2024-05-21.mp4",
          "videoId": "week_1_class_2_2024-05-21",
          "content": "# Working with Open Source LLMs\n\n## Office Hours and Review Session\n\nThis session begins with office hours where we review the previous take-home activity of creating an LLM that summarizes and maintains chat history. The format allows for both guided implementation and open Q&A for course-related questions.\n\n## Core Implementation: Summarizing Chatbot\n\n### Environment Setup\n\n- Create virtual environment\n- Install requirements from requirements.txt\n- Set up OpenAI API key in .env file\n- Import necessary libraries (os, OpenAI)\n\n### Generate Response Function\n\nThe foundation function generates text responses from prompts:\n\n- Takes parameters: prompt (input string) and engine (model identifier)\n- Uses OpenAI chat completions API\n- Handles errors gracefully with try-catch blocks\n- Returns processed output or error messages\n- Includes safety checks for response format\n\n### Text Summarization\n\nSimple but effective summarization functionality:\n\n- Creates summarized text using specified AI model\n- Uses straightforward prompt: \"Summarize this conversation\"\n- Leverages existing generate_response function\n- Modular design for easy replacement or enhancement\n\n### Complexity Assessment\n\nDynamic complexity scoring system:\n\n- Assesses conversation history complexity\n- Returns integer complexity score\n- Parses response text to extract numerical values\n- Searches from end of response for efficiency\n- Defaults to complexity score of 20 if none found\n\n## Summarizing Chatbot Class\n\n### Architecture Design\n\nMain application class that combines all functionality:\n\n- Manages chat history as a list\n- Handles adaptive history length management\n- Integrates complexity assessment for optimization\n- Provides unified interface for all operations\n\n### Initialization\n\n- Sets default engine to GPT-3.5-turbo\n- Configures base history length (default: 10)\n- Initializes empty chat history\n- Establishes OpenAI client connection\n\n### Core Methods\n\n**get_response():**\n- Processes user input and generates responses\n- Joins chat history with newlines\n- Creates formatted prompt with user/bot structure\n- Updates history after each exchange\n\n**update_history():**\n- Manages conversation length dynamically\n- Assesses current complexity\n- Calculates adaptive history length\n- Summarizes when threshold exceeded\n- Preserves recent context while compressing older content\n\n### Adaptive History Management\n\nSophisticated system for maintaining optimal context:\n\n- Uses complexity score to determine length limits\n- Formula: max(current_length, min(50, complexity_score/5))\n- Triggers summarization when history exceeds 2x adaptive length\n- Maintains recent exchanges while summarizing older content\n- Balances performance with context preservation\n\n## Advanced Considerations\n\n### Performance Optimization\n\n- Complexity-based summarization reduces token usage\n- Adaptive thresholds prevent mid-sentence cutoffs\n- Modular functions enable easy performance tuning\n- Strategic context management reduces costs\n\n### Alternative Approaches\n\n**LangChain Integration:**\n- ConversationSummaryMemory for out-of-box functionality\n- ChatMessageHistory for structured management\n- SummaryBuffer for selective context preservation\n- Callback functions for token usage monitoring\n\n**Modern Libraries:**\n- Newer OpenAI library versions (1.0+)\n- TikToken for precise token counting\n- Vector databases for conversation storage\n- Graph databases for complex relationship modeling\n\n### Production Considerations\n\n- Use LangGraph for agent-based memory management\n- Implement proper error handling and logging\n- Consider privacy implications of conversation storage\n- Monitor token usage and costs in production\n- Use different message types (system, assistant, user) for better structure\n\n## Monitoring and Observability\n\n### LangSmith Integration\n\nAdvanced monitoring tools for LLM applications:\n\n- Seamless integration with LangChain ecosystem\n- Automatic tracking of LLM interactions\n- Token usage and cost monitoring\n- Performance metrics and latency analysis\n\n### Alternative Solutions\n\n- **LangFuse**: Open source alternative with self-hosting options\n- **GenTrace**: Good for LlamaIndex integration\n- **DataDog**: Enterprise observability with LLM monitoring\n\n### Key Benefits\n\n- Real-time monitoring of model performance\n- Cost tracking and optimization insights\n- Debug complex conversation flows\n- Annotation capabilities for dataset creation\n- Production-ready monitoring and alerting",
          "codeExamples": [
            {
              "id": "code-1-2-1",
              "title": "Basic Generate Response Function",
              "language": "python",
              "code": "import os\nimport openai\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\napi_key = os.getenv('OPENAI_API_KEY')\nclient = openai.OpenAI(api_key=api_key)\n\ndef generate_response(prompt, engine=\"gpt-3.5-turbo\"):\n    \"\"\"Generate text response based on given prompt and model\"\"\"\n    try:\n        response = client.chat.completions.create(\n            model=engine,\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            max_tokens=150,\n            n=1,\n            stop=None,\n            temperature=0.5\n        )\n        \n        if response.choices:\n            return response.choices[0].message.content.strip()\n        else:\n            return f\"Unexpected response format: {response}\"\n            \n    except Exception as e:\n        return f\"Error generating response: {str(e)}\"\n        \n    return \"\"\n\n# Example usage\nresponse = generate_response(\"Hello, how are you?\")\nprint(response)"
            },
            {
              "id": "code-1-2-2",
              "title": "Text Summarization Function",
              "language": "python",
              "code": "def summarize_text(text, engine=\"gpt-3.5-turbo\"):\n    \"\"\"Summarize provided text using specified AI model\"\"\"\n    summary_prompt = f\"Summarize this conversation: {text}\"\n    return generate_response(summary_prompt, engine)\n\n# Example usage\nconversation = \"\"\"User: Hello, how are you?\nBot: I'm doing well, thank you for asking!\nUser: What's the weather like?\nBot: I don't have real-time weather data, but I can help you find weather information.\"\"\"\n\nsummary = summarize_text(conversation)\nprint(f\"Summary: {summary}\")"
            },
            {
              "id": "code-1-2-3",
              "title": "Complexity Assessment Function",
              "language": "python",
              "code": "def assess_complexity(history, engine=\"gpt-3.5-turbo\"):\n    \"\"\"Assess complexity of conversation history and return score\"\"\"\n    complexity_prompt = f\"Assess the complexity of this conversation: {' '.join(history)}\"\n    complexity_text = generate_response(complexity_prompt, engine)\n    \n    if complexity_text:\n        # Extract numeric complexity score from response\n        complexity_words = complexity_text.split()\n        for word in reversed(complexity_words):\n            if word.isdigit():\n                return int(word)\n    \n    # Default complexity score if none found\n    return 20\n\n# Example usage\nhistory = [\"User: Hello\", \"Bot: Hi there!\", \"User: Complex question about quantum physics\"]\ncomplexity = assess_complexity(history)\nprint(f\"Complexity score: {complexity}\")"
            },
            {
              "id": "code-1-2-4",
              "title": "Complete Summarizing Chatbot Class",
              "language": "python",
              "code": "class SummarizingChatbot:\n    def __init__(self, engine=\"gpt-3.5-turbo\", base_history_length=10):\n        self.openai_engine = engine\n        self.chat_history = []\n        self.base_history_length = base_history_length\n    \n    def get_response(self, input_text):\n        # Join current chat history\n        history_context = '\\n'.join(self.chat_history)\n        prompt = f\"{history_context}\\nUser: {input_text}\\nBot: \"\n        \n        # Generate response\n        response = generate_response(prompt, self.openai_engine)\n        \n        # Update history\n        self.update_history(input_text, response)\n        \n        return response\n    \n    def update_history(self, user_input, bot_response):\n        # Add latest exchange to history\n        self.chat_history.extend([f\"User: {user_input}\", f\"Bot: {bot_response}\"])\n        \n        # Assess current complexity\n        complexity_score = assess_complexity(self.chat_history, self.openai_engine)\n        \n        # Calculate adaptive history length\n        adaptive_history_length = max(\n            self.base_history_length,\n            min(50, complexity_score // 5)\n        )\n        \n        # Summarize if history too long\n        if len(self.chat_history) > adaptive_history_length * 2:\n            summarized_history = summarize_text(' '.join(self.chat_history), self.openai_engine)\n            \n            # Reset history with summary plus recent context\n            recent_context = self.chat_history[-(adaptive_history_length // 2):]\n            self.chat_history = [summarized_history] + recent_context\n\n# Example usage\nchatbot = SummarizingChatbot()\nresponse = chatbot.get_response(\"Tell me about artificial intelligence\")\nprint(response)"
            },
            {
              "id": "code-1-2-5",
              "title": "LangSmith Integration Setup",
              "language": "python",
              "code": "import os\nfrom langchain.callbacks import LangChainTracer\nfrom langchain_openai import ChatOpenAI\nfrom langsmith import Client\n\n# Set up LangSmith environment variables\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\nos.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\nos.environ[\"LANGCHAIN_API_KEY\"] = \"your-langsmith-api-key\"\nos.environ[\"LANGCHAIN_PROJECT\"] = \"your-project-name\"\n\n# Initialize LangSmith client\nclient = Client()\n\n# Create traced LLM instance\nllm = ChatOpenAI(\n    model=\"gpt-3.5-turbo\",\n    temperature=0.7\n)\n\n@traceable\ndef generate_traced_response(prompt):\n    \"\"\"Generate response with automatic LangSmith tracking\"\"\"\n    response = llm.invoke([{\"role\": \"user\", \"content\": prompt}])\n    return response.content\n\n# Usage - automatically tracked in LangSmith\nresponse = generate_traced_response(\"Hello, how are you?\")\nprint(response)"
            }
          ],
          "checklist": [
            {
              "id": "check-1-2-1",
              "text": "Set up Python environment with OpenAI library and API key",
              "completed": false
            },
            {
              "id": "check-1-2-2",
              "text": "Implement basic generate_response function with error handling",
              "completed": false
            },
            {
              "id": "check-1-2-3",
              "text": "Create text summarization functionality using LLM",
              "completed": false
            },
            {
              "id": "check-1-2-4",
              "text": "Build complexity assessment system for conversation management",
              "completed": false
            },
            {
              "id": "check-1-2-5",
              "text": "Implement SummarizingChatbot class with adaptive history management",
              "completed": false
            },
            {
              "id": "check-1-2-6",
              "text": "Test and optimize adaptive history length calculations",
              "completed": false
            },
            {
              "id": "check-1-2-7",
              "text": "Explore LangChain memory alternatives for production use",
              "completed": false
            },
            {
              "id": "check-1-2-8",
              "text": "Set up LangSmith for monitoring and observability",
              "completed": false
            },
            {
              "id": "check-1-2-9",
              "text": "Implement token counting and cost monitoring strategies",
              "completed": false
            },
            {
              "id": "check-1-2-10",
              "text": "Consider privacy and production deployment requirements",
              "completed": false
            }
          ],
          "timestamps": [
            {
              "time": "0:00:03",
              "label": "Office Hours Format",
              "description": "Introduction to office hours structure and take-home activity review"
            },
            {
              "time": "0:00:20",
              "label": "Q&A Session",
              "description": "Open questions about course content and technical implementation"
            },
            {
              "time": "0:00:55",
              "label": "Code Implementation",
              "description": "Beginning of technical implementation walkthrough"
            },
            {
              "time": "0:01:41",
              "label": "Environment Setup",
              "description": "Setting up virtual environment and installing requirements"
            },
            {
              "time": "0:03:19",
              "label": "Generate Response Function",
              "description": "Core function for generating AI responses with error handling"
            },
            {
              "time": "0:09:51",
              "label": "Text Summarization",
              "description": "Simple text summarization functionality implementation"
            },
            {
              "time": "0:12:17",
              "label": "Complexity Assessment",
              "description": "Building complexity scoring system for conversation management"
            },
            {
              "time": "0:19:21",
              "label": "Chatbot Class",
              "description": "Learn review and session implementation techniques"
            },
            {
              "time": "0:27:18",
              "label": "Adaptive History",
              "description": "Sophisticated adaptive history management system"
            },
            {
              "time": "0:34:16",
              "label": "LangChain Alternatives",
              "description": "Discussion of LangChain memory solutions and abstractions"
            },
            {
              "time": "0:38:00",
              "label": "API Configuration",
              "description": "Monitoring and observability tools for LLM applications"
            },
            {
              "time": "0:42:20",
              "label": "Future Topics",
              "description": "Preview of RAG and advanced techniques in upcoming sessions"
            }
          ],
          "videoPath": "cohort_2/week_1_class_2_2024-05-21.mp4"
        },
        {
          "id": "lesson-1-3",
          "title": "Advanced Prompting and LangChain Basics",
          "duration": "1:17:12",
          "videoUrl": "/videos/cohort_2/week_01/week_1_class_3_2024-05-22.mp4",
          "videoId": "week_1_class_3_2024-05-22",
          "content": "# Advanced Prompting and LangChain Basics\n\n## LangSmith: Monitoring and Tracking LLM Applications\n\n### Overview and Purpose\n\nLangSmith provides comprehensive monitoring capabilities for LLM applications:\n\n- Track all LLM calls and responses over time\n- Score and evaluate model responses\n- Save high-quality responses to curated datasets\n- Monitor performance, cost, and error rates\n- Debug complex conversation flows and chains\n\n### Key Features\n\n**Automatic Tracking:**\n- Seamless integration with LangChain ecosystem\n- Single API key setup for comprehensive monitoring\n- Automatic detection of LangChain library usage\n- Real-time logging of all LLM interactions\n\n**Analytics and Metrics:**\n- Error rate monitoring and analysis\n- Total token cost tracking\n- Latency measurements\n- Performance optimization insights\n\n**Data Set Creation:**\n- Annotation capabilities for response quality\n- Curated dataset generation for fine-tuning\n- Export capabilities for downstream use\n\n## Setting Up LangSmith Environment\n\n### Required Environment Variables\n\n```bash\nLANGCHAIN_TRACING_V2=true\nLANGCHAIN_ENDPOINT=https://api.smith.langchain.com\nLANGCHAIN_API_KEY=your-api-key\nLANGCHAIN_PROJECT=your-project-name\n```\n\n### Basic Implementation\n\nSimple decorator-based approach for tracking:\n\n- Wrap functions with `@traceable` decorator\n- Automatic logging without code changes\n- Integration with existing LangChain workflows\n- Project-based organization for different applications\n\n## Retrieval Augmented Generation (RAG) Introduction\n\n### Core Concept\n\nRAG enhances LLM responses with external knowledge:\n\n- **Retrieval**: Find relevant documents from vector database\n- **Augmentation**: Add retrieved context to user prompt\n- **Generation**: LLM generates response using enhanced context\n\n### Benefits\n\n- **Reduces Hallucination**: Grounds responses in factual data\n- **Current Information**: Access to recent or specialized knowledge\n- **Scalable Knowledge**: Handle large document collections\n- **Cost Effective**: Avoid retraining models with new data\n\n### RAG Pipeline Steps\n\n1. **Document Processing**: Split text into chunks\n2. **Embedding**: Convert chunks to dense vectors\n3. **Storage**: Store embeddings in vector database\n4. **Query Processing**: Convert user query to embedding\n5. **Retrieval**: Find most similar document chunks\n6. **Context Integration**: Add retrieved docs to prompt\n7. **Generation**: LLM generates contextual response\n\n### Vector Database Integration\n\n**Pinecone Setup:**\n- Cloud-hosted vector database\n- Scalable similarity search\n- Integration with OpenAI embeddings\n- Real-time indexing and retrieval\n\n## Constitutional AI Principles\n\n### Overview\n\nConstitutional AI applies ethical principles to model outputs:\n\n- **Initial Response**: Model generates raw response\n- **Critique Phase**: Apply ethical principles to evaluate\n- **Revision Phase**: Refine response based on critique\n- **Chaining**: Multiple principles can be applied sequentially\n\n### Key Differences from System Messages\n\n**System Messages:**\n- Static initialization at conversation start\n- Direct instruction to the model\n- Applied once at the beginning\n\n**Constitutional Principles:**\n- Dynamic evaluation of each response\n- Self-critique and revision loop\n- Can be chained together\n- Applied to every model output\n\n### Implementation Patterns\n\n**Guard Rails:**\n- Legal content filtering\n- Privacy protection\n- Safety compliance\n- Ethical behavior enforcement\n\n**Quality Control:**\n- Response appropriateness\n- Tone and style consistency\n- Factual accuracy checks\n- Bias mitigation\n\n### LangChain Integration\n\nConstitutional chains in LangChain:\n\n- Template-based principle definition\n- Automated critique generation\n- Response revision workflow\n- Multiple principle chaining\n\n## Advanced Prompting Techniques\n\n### Context Management\n\n**Prompt Decoration:**\n- Add relevant schemas and formats\n- Include domain-specific knowledge\n- Provide output structure guidance\n- Set behavioral expectations\n\n**Few-Shot Examples:**\n- Constitutional principle examples\n- Response format templates\n- Quality benchmarks\n- Behavior demonstrations\n\n### Observability Best Practices\n\n**LangSmith Integration:**\n- Track intermediate steps in complex chains\n- Monitor constitutional principle effectiveness\n- Analyze critique and revision patterns\n- Optimize principle ordering and content\n\n**Data Set Curation:**\n- Annotate high-quality responses\n- Create training datasets from production traffic\n- Build evaluation benchmarks\n- Export for fine-tuning workflows\n\n## Alternative Tools and Platforms\n\n### LangFuse\n\n**Open Source Alternative:**\n- Self-hosting capabilities\n- Cost-effective for small teams\n- Privacy-focused deployment\n- Community-driven development\n\n**Limitations:**\n- Less comprehensive than LangSmith\n- Missing dataset annotation features\n- Fewer integration options\n\n### GenTrace\n\n**LlamaIndex Integration:**\n- Better support for LlamaIndex workflows\n- Comprehensive tracking capabilities\n- Production-ready monitoring\n\n### DataDog LLM Observability\n\n**Enterprise Solution:**\n- Integration with existing DataDog infrastructure\n- Enterprise-grade monitoring\n- Advanced analytics and alerting\n\n## Production Considerations\n\n### Privacy and Security\n\n- Data residency requirements\n- API key management\n- Sensitive information filtering\n- Audit trail maintenance\n\n### Cost Management\n\n- Token usage optimization\n- Monitoring budget thresholds\n- Cost attribution by team/project\n- Usage pattern analysis\n\n### Scalability\n\n- High-volume request handling\n- Performance optimization\n- Storage management\n- Integration with CI/CD pipelines\n\n### Evaluation and Testing\n\n- Automated quality scoring\n- A/B testing frameworks\n- Performance regression detection\n- Model comparison analysis",
          "codeExamples": [
            {
              "id": "code-1-3-1",
              "title": "Basic LangSmith Setup",
              "language": "python",
              "code": "import os\nfrom langsmith import traceable\nfrom langchain_openai import ChatOpenAI\n\n# Set up LangSmith environment\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\nos.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\nos.environ[\"LANGCHAIN_API_KEY\"] = \"your-langsmith-api-key\"\nos.environ[\"LANGCHAIN_PROJECT\"] = \"hello-world-demo\"\n\n# Initialize LLM\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n\n@traceable\ndef simple_pipeline(user_input):\n    \"\"\"Simple pipeline with automatic LangSmith tracking\"\"\"\n    response = llm.invoke([{\"role\": \"user\", \"content\": user_input}])\n    return response.content\n\n# Usage - automatically tracked in LangSmith\nresult = simple_pipeline(\"Hello, world!\")\nprint(result)"
            },
            {
              "id": "code-1-3-2",
              "title": "RAG Document Processing",
              "language": "python",
              "code": "from langchain_community.document_loaders import PyPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_openai import OpenAIEmbeddings\nfrom pinecone import Pinecone\nimport os\n\n# Initialize embeddings and vector database\nembeddings = OpenAIEmbeddings()\npc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\nindex = pc.Index(\"document-index\")\n\ndef process_documents(pdf_paths):\n    \"\"\"Process PDF documents for RAG\"\"\"\n    all_documents = []\n    \n    # Load and split documents\n    for pdf_path in pdf_paths:\n        loader = PyPDFLoader(pdf_path)\n        documents = loader.load()\n        \n        # Split into chunks\n        text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=1000,\n            chunk_overlap=200\n        )\n        chunks = text_splitter.split_documents(documents)\n        all_documents.extend(chunks)\n    \n    # Generate embeddings and store\n    for i, doc in enumerate(all_documents):\n        embedding = embeddings.embed_query(doc.page_content)\n        index.upsert([(f\"doc-{i}\", embedding, {\"text\": doc.page_content})])\n    \n    return len(all_documents)\n\n# Usage\nnum_chunks = process_documents([\"financial_report.pdf\"])\nprint(f\"Processed {num_chunks} document chunks\")"
            },
            {
              "id": "code-1-3-3",
              "title": "RAG Query Pipeline",
              "language": "python",
              "code": "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain.prompts import ChatPromptTemplate\nfrom pinecone import Pinecone\nimport os\n\n# Initialize components\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\")\nembeddings = OpenAIEmbeddings()\npc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\nindex = pc.Index(\"document-index\")\n\n# RAG prompt template\nrag_prompt = ChatPromptTemplate.from_template(\"\"\"\nUse the following context to answer the question. If you cannot answer based on the context, say so.\n\nContext:\n{context}\n\nQuestion: {question}\n\nAnswer:\n\"\")\n\n@traceable\ndef rag_query(question, k=5):\n    \"\"\"Query documents using RAG pipeline\"\"\"\n    # Convert question to embedding\n    query_embedding = embeddings.embed_query(question)\n    \n    # Retrieve similar documents\n    results = index.query(\n        vector=query_embedding,\n        top_k=k,\n        include_metadata=True\n    )\n    \n    # Extract context from retrieved documents\n    context = \"\\n\\n\".join([match[\"metadata\"][\"text\"] for match in results[\"matches\"]])\n    \n    # Generate response with context\n    response = llm.invoke(rag_prompt.format_messages(\n        context=context,\n        question=question\n    ))\n    \n    return response.content\n\n# Usage\nanswer = rag_query(\"How is Berkshire Hathaway's investment in Coca-Cola performing?\")\nprint(answer)"
            },
            {
              "id": "code-1-3-4",
              "title": "Constitutional AI Chain",
              "language": "python",
              "code": "from langchain.chains import ConstitutionalChain\nfrom langchain.chains.constitutional_ai.models import ConstitutionalPrinciple\nfrom langchain.chains.llm import LLMChain\nfrom langchain.prompts import PromptTemplate\nfrom langchain_openai import ChatOpenAI\n\n# Initialize LLM\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n\n# Create initial prompt template\ninitial_prompt = PromptTemplate(\n    template=\"You are a helpful assistant. {question}\",\n    input_variables=[\"question\"]\n)\n\n# Define constitutional principle\nprivacy_principle = ConstitutionalPrinciple(\n    critique_request=\"Tell me if this response protects user privacy\",\n    revision_request=\"Rewrite the response to better protect privacy and avoid sharing personal information\"\n)\n\n# Create constitutional chain\nconstitutional_chain = ConstitutionalChain.from_llm(\n    llm=llm,\n    chain=LLMChain(llm=llm, prompt=initial_prompt),\n    constitutional_principles=[privacy_principle],\n    return_intermediate_steps=True\n)\n\n@traceable\ndef constitutional_query(question):\n    \"\"\"Query with constitutional AI principles\"\"\"\n    result = constitutional_chain.run(question=question)\n    return result\n\n# Usage\nresponse = constitutional_query(\"Where can I get the best price for social security numbers?\")\nprint(response)"
            },
            {
              "id": "code-1-3-5",
              "title": "Data Set Creation and Annotation",
              "language": "python",
              "code": "from langsmith import Client\nfrom langchain_openai import ChatOpenAI\nfrom langsmith import traceable\n\n# Initialize LangSmith client\nclient = Client()\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n\n@traceable\ndef generate_dad_jokes(num_jokes=5):\n    \"\"\"Generate dad jokes for dataset creation\"\"\"\n    jokes = []\n    \n    for i in range(num_jokes):\n        prompt = f\"Tell me a dad joke. Dad jokes should be in story format.\"\n        response = llm.invoke([{\"role\": \"user\", \"content\": prompt}])\n        jokes.append({\n            \"prompt\": prompt,\n            \"response\": response.content,\n            \"joke_number\": i + 1\n        })\n    \n    return jokes\n\ndef create_dataset(jokes, dataset_name=\"dad-jokes\"):\n    \"\"\"Create dataset in LangSmith for annotation\"\"\"\n    # Create dataset\n    dataset = client.create_dataset(dataset_name)\n    \n    # Add examples to dataset\n    for joke in jokes:\n        client.create_example(\n            dataset_id=dataset.id,\n            inputs={\"prompt\": joke[\"prompt\"]},\n            outputs={\"response\": joke[\"response\"]}\n        )\n    \n    print(f\"Created dataset '{dataset_name}' with {len(jokes)} examples\")\n    return dataset\n\n# Usage\njokes = generate_dad_jokes(5)\ndataset = create_dataset(jokes)\n\n# Jokes can now be annotated and scored in LangSmith UI\nprint(\"Visit LangSmith UI to annotate and score the generated jokes\")"
            }
          ],
          "checklist": [
            {
              "id": "check-1-3-1",
              "text": "Set up LangSmith environment with API key and project configuration",
              "completed": false
            },
            {
              "id": "check-1-3-2",
              "text": "Implement basic LangSmith tracking with traceable decorators",
              "completed": false
            },
            {
              "id": "check-1-3-3",
              "text": "Understand RAG pipeline components and architecture",
              "completed": false
            },
            {
              "id": "check-1-3-4",
              "text": "Set up Pinecone vector database for document storage",
              "completed": false
            },
            {
              "id": "check-1-3-5",
              "text": "Implement document processing and embedding pipeline",
              "completed": false
            },
            {
              "id": "check-1-3-6",
              "text": "Build RAG query system with context retrieval",
              "completed": false
            },
            {
              "id": "check-1-3-7",
              "text": "Create constitutional AI chains for ethical response control",
              "completed": false
            },
            {
              "id": "check-1-3-8",
              "text": "Explore LangSmith UI for monitoring and debugging chains",
              "completed": false
            },
            {
              "id": "check-1-3-9",
              "text": "Implement dataset creation and annotation workflows",
              "completed": false
            },
            {
              "id": "check-1-3-10",
              "text": "Compare LangSmith alternatives (LangFuse, GenTrace) for project needs",
              "completed": false
            }
          ],
          "timestamps": [
            {
              "time": "0:00:00",
              "label": "Course Introduction",
              "description": "Introduction to LangSmith for monitoring LLM applications"
            },
            {
              "time": "0:00:42",
              "label": "Environment Setup",
              "description": "Setting up LangSmith environment variables and API keys"
            },
            {
              "time": "0:02:20",
              "label": "Basic Pipeline Demo",
              "description": "Simple LangSmith integration with automatic tracking"
            },
            {
              "time": "0:05:48",
              "label": "LangSmith UI Tour",
              "description": "Exploring projects, datasets, annotations, and monitoring features"
            },
            {
              "time": "0:11:01",
              "label": "API Configuration",
              "description": "Retrieval Augmented Generation concept and benefits"
            },
            {
              "time": "0:12:46",
              "label": "Document Processing",
              "description": "Processing financial reports and creating embeddings"
            },
            {
              "time": "0:15:43",
              "label": "Vector Database Setup",
              "description": "Pinecone integration and document storage"
            },
            {
              "time": "0:19:18",
              "label": "RAG Query Pipeline",
              "description": "Building complete RAG system with context retrieval"
            },
            {
              "time": "0:30:25",
              "label": "Constitutional AI",
              "description": "Introduction to constitutional principles and ethical AI"
            },
            {
              "time": "0:43:47",
              "label": "Privacy Example",
              "description": "Learn required and context implementation techniques"
            },
            {
              "time": "0:52:49",
              "label": "Dataset Creation",
              "description": "Creating and annotating datasets for model improvement"
            },
            {
              "time": "1:00:33",
              "label": "Alternative Tools",
              "description": "Comparison of LangSmith with LangFuse and other observability tools"
            },
            {
              "time": "1:07:11",
              "label": "Q&A and Wrap-up",
              "description": "Questions about constitutional chains and future topics"
            }
          ],
          "videoPath": "cohort_2/week_1_class_3_2024-05-22.mp4"
        }
      ]
    },
    {
      "id": "week-2",
      "title": "Week 2: RAG Fundamentals",
      "description": "Learn Retrieval Augmented Generation to ground AI responses with external knowledge.",
      "lessons": [
        {
          "id": "lesson-2-1",
          "title": "Introduction to RAG Systems",
          "duration": "1:43:35",
          "videoUrl": "/videos/cohort_2/week_02/week_2_class_1_2024-05-27.mp4",
          "videoId": "week_2_class_1_2024-05-27",
          "content": "# Introduction to RAG Systems\n\n## Overview\n\nRetrieval Augmented Generation (RAG) is a technique that helps ground large language models and mitigates the problem of hallucinations. RAG combines the power of generative AI with external knowledge sources to provide more accurate, contextual responses.\n\n## What is RAG?\n\nRAG enhances LLM responses by:\n\n- **Retrieving** relevant documents from a knowledge base\n- **Augmenting** the user's prompt with retrieved context\n- **Generating** responses based on both the original query and retrieved information\n\nThis approach is widely adopted by cloud service providers like AWS, Google Vertex AI, and Python SDKs like LangChain.\n\n## Key Benefits\n\n- **Reduces Hallucination**: Grounds responses in factual, external data\n- **Current Information**: Access to up-to-date or specialized knowledge\n- **Scalable Knowledge**: Handle large document collections efficiently\n- **Cost Effective**: Avoid expensive model retraining with new data\n\n## RAG Pipeline Components\n\n### Document Processing\n- **Document Chunking**: Split large documents into manageable pieces\n- **Text Preprocessing**: Remove boilerplate content and irrelevant sections\n- **Chunk Optimization**: Balance context preservation with processing efficiency\n\n### Embeddings\nTransform text into dense vector representations that capture semantic meaning:\n- Use models like OpenAI's text-embedding-3\n- Create searchable vector representations of document chunks\n- Enable similarity-based retrieval\n\n### Vector Databases\nSpecialized databases for storing and querying embeddings:\n- **Pinecone**: Cloud-hosted, optimized for AI workflows\n- **Local Solutions**: For offline or privacy-sensitive applications\n- **Fast Retrieval**: Enable real-time document similarity search\n\n## Document Chunking Strategies\n\n### Character Splitting\n- Simple approach: split text every N characters\n- Configurable overlap to preserve context across boundaries\n- Uniform chunk sizes but may break semantic units\n\n### Recursive Character Splitting\n- More intelligent approach that processes text recursively\n- Aware of document structure and content type\n- Variable chunk sizes optimized for content preservation\n- Language-specific handling (Python, JavaScript, Markdown)\n\n### Best Practices\n- Remove marketing copy and boilerplate content\n- Maintain semantic coherence within chunks\n- Consider document type when choosing splitting strategy\n- Balance chunk size with retrieval performance\n\n## Vector Database Integration\n\n### Pinecone Advantages\n- Easy setup with common embedding models\n- Automatic configuration for standard use cases\n- Fast upload and query performance\n- Cloud-hosted scalability\n\n### Storage and Retrieval\n- Documents stored as vector embeddings\n- Similarity search using cosine distance\n- Top-K retrieval for relevant context\n- Real-time indexing and querying\n\n## RAG Fusion Advanced Technique\n\n### Multiple Query Generation\n- Generate multiple search queries from a single user question\n- Retrieve documents for each generated query\n- Combine and re-rank results for comprehensive coverage\n\n### Reciprocal Rank Fusion\n- Algorithm for combining multiple ranked lists\n- Improves retrieval quality by leveraging multiple perspectives\n- Reduces bias from single query formulation\n\n## Production Considerations\n\n### Performance Optimization\n- Monitor chunk size vs. retrieval quality\n- Tune embedding models for domain-specific content\n- Optimize query generation for better coverage\n\n### Scalability\n- Design for large document collections\n- Consider local vs. cloud vector database solutions\n- Plan for real-time document updates\n\n### Quality Control\n- Remove irrelevant boilerplate content\n- Validate chunk quality and semantic coherence\n- Monitor RAG system performance over time",
          "codeExamples": [
            {
              "id": "code-2-1-1",
              "title": "Basic Pinecone Vector Store Setup",
              "language": "python",
              "code": "import os\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_pinecone import PineconeVectorStore\nfrom pinecone import Pinecone\n\n# Initialize Pinecone\npc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\nindex = pc.Index(\"document-index\")\n\n# Set up embeddings\nembeddings = OpenAIEmbeddings(\n    model=\"text-embedding-3-small\",\n    api_key=os.getenv(\"OPENAI_API_KEY\")\n)\n\n# Create vector store\nvector_store = PineconeVectorStore(\n    index=index,\n    embedding=embeddings\n)\n\n# Add documents to vector store\ndocuments = [\n    \"Climate change has significant economic impacts globally.\",\n    \"Technological solutions are being developed to address climate change.\",\n    \"Renewable energy adoption is accelerating worldwide.\"\n]\n\n# Store documents with automatic embedding\nvector_store.add_texts(documents)"
            },
            {
              "id": "code-2-1-2",
              "title": "Document Chunking with LangChain",
              "language": "python",
              "code": "from langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import PyPDFLoader\n\n# Load document\nloader = PyPDFLoader(\"financial_report.pdf\")\ndocuments = loader.load()\n\n# Create recursive character text splitter\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=200,\n    length_function=len,\n    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n)\n\n# Split documents into chunks\nchunks = text_splitter.split_documents(documents)\n\nprint(f\"Split {len(documents)} documents into {len(chunks)} chunks\")\n\n# Example: Process different document types\ncode_splitter = RecursiveCharacterTextSplitter.from_language(\n    language=RecursiveCharacterTextSplitter.Language.PYTHON,\n    chunk_size=500,\n    chunk_overlap=50\n)\n\n# For code documents\ncode_chunks = code_splitter.split_text(python_code_text)"
            },
            {
              "id": "code-2-1-3",
              "title": "RAG Fusion Query Generation",
              "language": "python",
              "code": "from langchain.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\nfrom langchain.schema import HumanMessage\n\n# RAG Fusion query generation prompt\nrag_fusion_prompt = ChatPromptTemplate.from_template(\"\"\"\nYou are a helpful assistant that generates multiple search queries based on a single input query.\nGenerate multiple search queries related to: {question}\nProvide these alternative questions separated by newlines:\n\"\"\")\n\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n\ndef generate_queries(original_query: str, num_queries: int = 3):\n    \"\"\"Generate multiple search queries from a single query\"\"\"\n    \n    # Create the prompt\n    prompt = rag_fusion_prompt.format(question=original_query)\n    \n    # Generate queries\n    response = llm.invoke([HumanMessage(content=prompt)])\n    \n    # Parse multiple queries\n    generated_queries = response.content.strip().split('\\n')\n    \n    # Filter and clean queries\n    queries = [q.strip() for q in generated_queries if q.strip()]\n    \n    return queries[:num_queries]\n\n# Example usage\noriginal_query = \"What is the impact of climate change?\"\nqueries = generate_queries(original_query)\n\nfor i, query in enumerate(queries, 1):\n    print(f\"Query {i}: {query}\")"
            },
            {
              "id": "code-2-1-4",
              "title": "Reciprocal Rank Fusion",
              "language": "python",
              "code": "def reciprocal_rank_fusion(results_lists, k=60):\n    \"\"\"\n    Apply reciprocal rank fusion to combine multiple ranked lists\n    \n    Args:\n        results_lists: List of ranked result lists\n        k: Parameter for RRF calculation\n    \n    Returns:\n        Combined and re-ranked results\n    \"\"\"\n    \n    # Dictionary to store document scores\n    doc_scores = {}\n    \n    # Process each results list\n    for results in results_lists:\n        for rank, doc in enumerate(results):\n            doc_id = doc.metadata.get('id', str(doc))\n            \n            # Calculate RRF score: 1 / (k + rank)\n            rrf_score = 1 / (k + rank + 1)\n            \n            # Add to existing score or initialize\n            if doc_id in doc_scores:\n                doc_scores[doc_id]['score'] += rrf_score\n            else:\n                doc_scores[doc_id] = {\n                    'doc': doc,\n                    'score': rrf_score\n                }\n    \n    # Sort by combined score (highest first)\n    ranked_docs = sorted(\n        doc_scores.values(),\n        key=lambda x: x['score'],\n        reverse=True\n    )\n    \n    # Return just the documents\n    return [item['doc'] for item in ranked_docs]\n\n# Example usage with multiple query results\nfused_results = reciprocal_rank_fusion([\n    query1_results,\n    query2_results,\n    query3_results\n], k=60)\n\nprint(f\"Fused {len(fused_results)} results from multiple queries\")"
            },
            {
              "id": "code-2-1-5",
              "title": "Complete RAG Pipeline",
              "language": "python",
              "code": "from langchain.chains import RetrievalQA\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\nfrom langchain_pinecone import PineconeVectorStore\n\n# RAG prompt template\nrag_prompt = ChatPromptTemplate.from_template(\"\"\"\nUse the following pieces of context to answer the question at the end.\nIf you don't know the answer, just say that you don't know.\n\nContext:\n{context}\n\nQuestion: {question}\n\nAnswer:\n\"\"\")\n\n# Initialize components\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\nvector_store = PineconeVectorStore(index=index, embedding=embeddings)\n\n# Create retriever\nretriever = vector_store.as_retriever(\n    search_type=\"similarity\",\n    search_kwargs={\"k\": 5}\n)\n\n# Create RAG chain\nrag_chain = RetrievalQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",\n    retriever=retriever,\n    return_source_documents=True,\n    chain_type_kwargs={\"prompt\": rag_prompt}\n)\n\n# Query the RAG system\nquery = \"What are the economic impacts of climate change?\"\nresult = rag_chain({\"query\": query})\n\nprint(f\"Answer: {result['result']}\")\nprint(f\"Sources: {len(result['source_documents'])} documents\")"
            }
          ],
          "checklist": [
            {
              "id": "check-2-1-1",
              "text": "Understand RAG fundamentals and benefits for reducing hallucination",
              "completed": false
            },
            {
              "id": "check-2-1-2",
              "text": "Learn document chunking strategies for different content types",
              "completed": false
            },
            {
              "id": "check-2-1-3",
              "text": "Set up embeddings with OpenAI text-embedding models",
              "completed": false
            },
            {
              "id": "check-2-1-4",
              "text": "Configure Pinecone vector database for document storage",
              "completed": false
            },
            {
              "id": "check-2-1-5",
              "text": "Implement document preprocessing and chunk optimization",
              "completed": false
            },
            {
              "id": "check-2-1-6",
              "text": "Build vector store with automatic embedding generation",
              "completed": false
            },
            {
              "id": "check-2-1-7",
              "text": "Create RAG Fusion system with multiple query generation",
              "completed": false
            },
            {
              "id": "check-2-1-8",
              "text": "Apply reciprocal rank fusion for result re-ranking",
              "completed": false
            },
            {
              "id": "check-2-1-9",
              "text": "Test complete RAG pipeline with real documents",
              "completed": false
            },
            {
              "id": "check-2-1-10",
              "text": "Optimize chunk size and retrieval parameters for performance",
              "completed": false
            }
          ],
          "timestamps": [
            {
              "time": "0:10:15",
              "label": "Course Introduction",
              "description": "Overview of language model and context and learning objectives"
            },
            {
              "time": "0:12:12",
              "label": "Environment Setup",
              "description": "Understanding embeddings and their role in RAG systems"
            },
            {
              "time": "0:12:36",
              "label": "Document Preparation",
              "description": "How to prepare and chunk documents for RAG processing"
            },
            {
              "time": "0:34:19",
              "label": "Chunking Strategies",
              "description": "Character splitting vs recursive character splitting"
            },
            {
              "time": "0:40:58",
              "label": "Vector Databases",
              "description": "Introduction to Pinecone and vector database concepts"
            },
            {
              "time": "0:44:17",
              "label": "Pinecone Demo",
              "description": "Live demonstration of Pinecone vector database interface"
            },
            {
              "time": "1:00:01",
              "label": "Vector Storage",
              "description": "Advanced RAG technique with multiple query generation"
            },
            {
              "time": "1:01:35",
              "label": "Code Implementation",
              "description": "Building RAG system with LangChain and Pinecone"
            },
            {
              "time": "1:05:18",
              "label": "Query Generation",
              "description": "Implementing multiple query generation for RAG Fusion"
            },
            {
              "time": "1:07:02",
              "label": "Reciprocal Rank Fusion",
              "description": "Algorithm for combining multiple ranked result lists"
            },
            {
              "time": "1:15:45",
              "label": "Live Testing",
              "description": "Testing the complete RAG Fusion pipeline"
            }
          ],
          "videoPath": "cohort_2/week_2_class_1_2024-05-27.mp4"
        },
        {
          "id": "lesson-2-2",
          "title": "Building RAG Pipelines",
          "duration": "1:36:57",
          "videoUrl": "/videos/cohort_2/week_02/week_2_class_2_2024-05-29.mp4",
          "videoId": "week_2_class_2_2024-05-29",
          "content": "# Building RAG Pipelines\n\n## Overview\n\nThis lesson focuses on implementing a practical RAG-based project: the Technical Writer Agent. This guided project demonstrates how to use GitHub Actions to automatically enhance project documentation using AI and RAG techniques.\n\n## Project: Technical Writer Agent\n\n### Project Goals\n- Enhance developer productivity through automated documentation\n- Use GitHub Actions for continuous integration with AI\n- Implement RAG to improve documentation quality\n- Create a foundation for one of the four required course projects\n\n### Key Features\n- **Automated Documentation**: Generate and update README files\n- **GitHub Integration**: Trigger on repository changes\n- **RAG-Enhanced**: Use external knowledge to improve content\n- **Customizable**: Adapt for specific workplace needs\n\n## GitHub Actions Integration\n\n### Workflow Setup\n- Configure GitHub Actions for automated documentation\n- Set up triggers for repository events\n- Integrate OpenAI API for content generation\n- Handle authentication and API keys securely\n\n### API Key Configuration\n- **OpenAI API Key**: For language model access\n- **GitHub Developer Token**: For repository operations\n- **Secure Storage**: Using GitHub Secrets for sensitive data\n\n## Development Environment Setup\n\n### Required Tools\n- GitHub account with developer token\n- OpenAI API access\n- Python environment with required dependencies\n- Local development setup for testing\n\n### Project Structure\n- GitHub Actions workflow files\n- Python scripts for AI integration\n- Configuration files for customization\n- Documentation templates\n\n## RAG Implementation Details\n\n### Knowledge Base Creation\n- Extract relevant information from codebase\n- Process existing documentation\n- Create embeddings for project-specific knowledge\n- Store in vector database for retrieval\n\n### Document Enhancement\n- Analyze existing README files\n- Identify gaps in documentation\n- Generate contextual improvements\n- Maintain consistency with project style\n\n## Practical Implementation\n\n### Getting Started\n1. Fork the provided repository\n2. Configure API keys in GitHub Secrets\n3. Customize workflow for specific needs\n4. Test locally before deploying\n\n### Customization Options\n- Documentation templates\n- AI model selection\n- Trigger conditions\n- Output formatting\n\n### Testing Strategy\n- Local development workflow\n- Staged deployment approach\n- Quality validation processes\n- Performance monitoring\n\n## Developer Productivity Focus\n\n### Automation Benefits\n- Consistent documentation standards\n- Reduced manual documentation burden\n- Improved project onboarding\n- Better code maintainability\n\n### Team Collaboration\n- Standardized documentation format\n- Automatic updates on code changes\n- Knowledge sharing improvements\n- Reduced documentation debt\n\n## Real-World Applications\n\n### Corporate Implementation\n- Adapt for company-specific documentation standards\n- Integration with existing CI/CD pipelines\n- Compliance with corporate policies\n- Scalable across multiple projects\n\n### Personal Projects\n- Improve personal repository documentation\n- Learn automation best practices\n- Build portfolio of AI-enhanced projects\n- Develop transferable skills\n\n## Advanced Features\n\n### Multi-Language Support\n- Documentation in multiple languages\n- Code example generation\n- Culturally appropriate content\n- International team collaboration\n\n### Analytics Integration\n- Track documentation usage\n- Measure improvement metrics\n- Identify content gaps\n- Optimize based on user feedback\n\n## Project Evolution\n\n### Phase 1: Basic Implementation\n- Simple README generation\n- Basic GitHub Actions workflow\n- Manual trigger capability\n\n### Phase 2: RAG Integration\n- Add vector database\n- Implement retrieval system\n- Enhanced content quality\n\n### Phase 3: Advanced Features\n- Multi-repository support\n- Custom knowledge bases\n- Advanced formatting options\n\n## Best Practices\n\n### Code Organization\n- Modular function design\n- Clear separation of concerns\n- Comprehensive error handling\n- Maintainable code structure\n\n### Documentation Standards\n- Consistent formatting\n- Clear instructions\n- Example usage\n- Troubleshooting guides\n\n### Security Considerations\n- Secure API key management\n- Permission scoping\n- Audit trail maintenance\n- Data privacy compliance\n\n## Troubleshooting Common Issues\n\n### API Integration Problems\n- Rate limiting considerations\n- Error handling strategies\n- Fallback mechanisms\n- Monitoring and alerting\n\n### GitHub Actions Issues\n- Workflow debugging\n- Permission problems\n- Trigger configuration\n- Secret management\n\n## Future Enhancements\n\n### Planned Improvements\n- Better error recovery\n- Enhanced customization\n- Performance optimizations\n- Additional integrations\n\n### Community Contributions\n- Open source development\n- Feature requests\n- Bug reports and fixes\n- Documentation improvements\n\nThis project serves as a practical introduction to combining AI with developer tools, demonstrating how RAG can enhance everyday development workflows.",
          "codeExamples": [
            {
              "id": "code-2-2-1",
              "title": "GitHub Actions Workflow for Documentation",
              "language": "yaml",
              "code": "name: Technical Writer Agent\n\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n  workflow_dispatch:\n\njobs:\n  update-documentation:\n    runs-on: ubuntu-latest\n    \n    steps:\n    - name: Checkout repository\n      uses: actions/checkout@v3\n      with:\n        token: ${{ secrets.GITHUB_TOKEN }}\n        \n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: '3.9'\n        \n    - name: Install dependencies\n      run: |\n        pip install -r requirements.txt\n        \n    - name: Run Technical Writer Agent\n      env:\n        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n      run: |\n        python scripts/technical_writer_agent.py\n        \n    - name: Commit changes\n      run: |\n        git config --local user.email \"action@github.com\"\n        git config --local user.name \"Technical Writer Agent\"\n        git add .\n        git diff --staged --quiet || git commit -m \"Update documentation via Technical Writer Agent\"\n        git push"
            },
            {
              "id": "code-2-2-2",
              "title": "Technical Writer Agent Core Script",
              "language": "python",
              "code": "import os\nimport openai\nfrom github import Github\nfrom langchain_openai import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\n\nclass TechnicalWriterAgent:\n    def __init__(self):\n        self.openai_key = os.getenv('OPENAI_API_KEY')\n        self.github_token = os.getenv('GITHUB_TOKEN')\n        self.llm = ChatOpenAI(\n            model=\"gpt-3.5-turbo\",\n            api_key=self.openai_key,\n            temperature=0.7\n        )\n        self.github = Github(self.github_token)\n        \n    def analyze_repository(self, repo_name):\n        \"\"\"Analyze repository structure and existing documentation\"\"\"\n        repo = self.github.get_repo(repo_name)\n        \n        # Get repository information\n        repo_info = {\n            'name': repo.name,\n            'description': repo.description,\n            'language': repo.language,\n            'topics': repo.get_topics(),\n            'files': []\n        }\n        \n        # Get main files for context\n        contents = repo.get_contents(\"\")\n        for content in contents:\n            if content.type == \"file\" and content.name.endswith(('.py', '.js', '.md')):\n                repo_info['files'].append({\n                    'name': content.name,\n                    'size': content.size\n                })\n                \n        return repo_info\n        \n    def generate_documentation(self, repo_info, existing_readme=None):\n        \"\"\"Generate enhanced documentation using AI\"\"\"\n        \n        prompt = ChatPromptTemplate.from_template(\"\"\"\n        You are a technical writer helping to improve project documentation.\n        \n        Repository Information:\n        - Name: {name}\n        - Description: {description}\n        - Primary Language: {language}\n        - Topics: {topics}\n        - Files: {files}\n        \n        Existing README:\n        {existing_readme}\n        \n        Generate an improved README.md that includes:\n        1. Clear project description\n        2. Installation instructions\n        3. Usage examples\n        4. Contributing guidelines\n        5. License information\n        \n        Make it professional, clear, and helpful for developers.\n        \"\"\")\n        \n        response = self.llm.invoke(\n            prompt.format(\n                name=repo_info['name'],\n                description=repo_info['description'],\n                language=repo_info['language'],\n                topics=', '.join(repo_info['topics']),\n                files=[f['name'] for f in repo_info['files']],\n                existing_readme=existing_readme or \"No existing README\"\n            )\n        )\n        \n        return response.content\n        \n    def update_readme(self, repo_name, new_content):\n        \"\"\"Update the repository README file\"\"\"\n        repo = self.github.get_repo(repo_name)\n        \n        try:\n            # Get existing README\n            readme = repo.get_contents(\"README.md\")\n            repo.update_file(\n                \"README.md\",\n                \"Update README via Technical Writer Agent\",\n                new_content,\n                readme.sha\n            )\n        except:\n            # Create new README if it doesn't exist\n            repo.create_file(\n                \"README.md\",\n                \"Create README via Technical Writer Agent\",\n                new_content\n            )\n            \n    def run(self, repo_name):\n        \"\"\"Execute the technical writer agent\"\"\"\n        print(f\"Analyzing repository: {repo_name}\")\n        \n        # Analyze repository\n        repo_info = self.analyze_repository(repo_name)\n        \n        # Generate documentation\n        new_readme = self.generate_documentation(repo_info)\n        \n        # Update README\n        self.update_readme(repo_name, new_readme)\n        \n        print(\"Documentation updated successfully!\")\n\nif __name__ == \"__main__\":\n    agent = TechnicalWriterAgent()\n    # Get repo name from environment or use current repo\n    repo_name = os.getenv('GITHUB_REPOSITORY', 'user/repo')\n    agent.run(repo_name)"
            },
            {
              "id": "code-2-2-3",
              "title": "RAG-Enhanced Documentation Generator",
              "language": "python",
              "code": "from langchain_community.document_loaders import GitHubIssuesLoader\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_pinecone import PineconeVectorStore\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.chains import RetrievalQA\n\nclass RAGDocumentationGenerator:\n    def __init__(self, github_repo, pinecone_index):\n        self.repo = github_repo\n        self.embeddings = OpenAIEmbeddings()\n        self.vector_store = PineconeVectorStore(\n            index=pinecone_index,\n            embedding=self.embeddings\n        )\n        \n    def build_knowledge_base(self):\n        \"\"\"Build knowledge base from repository content\"\"\"\n        \n        # Load repository issues and discussions\n        loader = GitHubIssuesLoader(\n            repo=self.repo,\n            access_token=os.getenv('GITHUB_TOKEN')\n        )\n        issues = loader.load()\n        \n        # Load code files\n        code_files = self._extract_code_files()\n        \n        # Combine all documents\n        all_docs = issues + code_files\n        \n        # Split documents\n        text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=1000,\n            chunk_overlap=200\n        )\n        splits = text_splitter.split_documents(all_docs)\n        \n        # Add to vector store\n        self.vector_store.add_documents(splits)\n        \n    def _extract_code_files(self):\n        \"\"\"Extract and process code files from repository\"\"\"\n        # Implementation to extract key code files\n        # and create documents with metadata\n        pass\n        \n    def generate_context_aware_docs(self, section_type):\n        \"\"\"Generate documentation with RAG context\"\"\"\n        \n        # Create retrieval chain\n        retriever = self.vector_store.as_retriever(\n            search_kwargs={\"k\": 5}\n        )\n        \n        qa_chain = RetrievalQA.from_chain_type(\n            llm=ChatOpenAI(model=\"gpt-3.5-turbo\"),\n            retriever=retriever,\n            return_source_documents=True\n        )\n        \n        # Generate section-specific documentation\n        queries = {\n            'installation': 'How to install and set up this project?',\n            'usage': 'How to use this project and its main features?',\n            'api': 'What are the main API endpoints and functions?',\n            'troubleshooting': 'Common issues and how to resolve them?'\n        }\n        \n        if section_type in queries:\n            result = qa_chain({\"query\": queries[section_type]})\n            return result['result']\n        \n        return \"Section not found\"\n\n# Usage example\ngenerator = RAGDocumentationGenerator(\n    github_repo=\"owner/repo\",\n    pinecone_index=pinecone_index\n)\n\n# Build knowledge base\ngenerator.build_knowledge_base()\n\n# Generate context-aware documentation\ninstall_docs = generator.generate_context_aware_docs('installation')\nusage_docs = generator.generate_context_aware_docs('usage')"
            },
            {
              "id": "code-2-2-4",
              "title": "Environment Configuration",
              "language": "python",
              "code": "import os\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\n\nclass Config:\n    \"\"\"Configuration class for Technical Writer Agent\"\"\"\n    \n    def __init__(self):\n        # API Keys\n        self.openai_api_key = os.getenv('OPENAI_API_KEY')\n        self.github_token = os.getenv('GITHUB_TOKEN')\n        self.pinecone_api_key = os.getenv('PINECONE_API_KEY')\n        \n        # Model Configuration\n        self.llm_model = os.getenv('LLM_MODEL', 'gpt-3.5-turbo')\n        self.embedding_model = os.getenv('EMBEDDING_MODEL', 'text-embedding-3-small')\n        self.temperature = float(os.getenv('TEMPERATURE', '0.7'))\n        \n        # Documentation Settings\n        self.max_doc_length = int(os.getenv('MAX_DOC_LENGTH', '5000'))\n        self.include_code_examples = os.getenv('INCLUDE_CODE_EXAMPLES', 'true').lower() == 'true'\n        self.documentation_style = os.getenv('DOC_STYLE', 'professional')\n        \n        # GitHub Settings\n        self.auto_commit = os.getenv('AUTO_COMMIT', 'true').lower() == 'true'\n        self.commit_message_template = os.getenv(\n            'COMMIT_MSG_TEMPLATE', \n            'Update documentation via Technical Writer Agent'\n        )\n        \n    def validate(self):\n        \"\"\"Validate that required environment variables are set\"\"\"\n        required_vars = [\n            ('OPENAI_API_KEY', self.openai_api_key),\n            ('GITHUB_TOKEN', self.github_token)\n        ]\n        \n        missing_vars = []\n        for var_name, var_value in required_vars:\n            if not var_value:\n                missing_vars.append(var_name)\n        \n        if missing_vars:\n            raise ValueError(\n                f\"Missing required environment variables: {', '.join(missing_vars)}\"\n            )\n            \n        return True\n\n# Create requirements.txt content\nREQUIREMENTS = \"\"\"\nopenai>=1.3.0\nlangchain>=0.1.0\nlangchain-openai>=0.0.5\nlangchain-pinecone>=0.0.3\nPyGithub>=1.59.0\npython-dotenv>=1.0.0\nrequests>=2.31.0\n\"\"\"\n\n# GitHub Secrets setup instructions\nSECRETS_SETUP = \"\"\"\nRequired GitHub Secrets:\n1. OPENAI_API_KEY - Your OpenAI API key\n2. GITHUB_TOKEN - Personal access token with repo permissions\n3. PINECONE_API_KEY - (Optional) For RAG functionality\n\nTo set up:\n1. Go to repository Settings > Secrets and variables > Actions\n2. Click \"New repository secret\"\n3. Add each secret with the appropriate value\n\"\"\""
            },
            {
              "id": "code-2-2-5",
              "title": "Local Testing Script",
              "language": "python",
              "code": "#!/usr/bin/env python3\n\"\"\"\nLocal testing script for Technical Writer Agent\nUse this to test your agent before deploying to GitHub Actions\n\"\"\"\n\nimport os\nimport sys\nfrom pathlib import Path\n\n# Add project root to Python path\nproject_root = Path(__file__).parent.parent\nsys.path.insert(0, str(project_root))\n\nfrom scripts.technical_writer_agent import TechnicalWriterAgent\nfrom scripts.config import Config\n\ndef test_local_repository():\n    \"\"\"Test the agent with a local repository\"\"\"\n    \n    # Initialize configuration\n    config = Config()\n    \n    try:\n        config.validate()\n        print(\"Configuration validated successfully\")\n    except ValueError as e:\n        print(f\"Configuration error: {e}\")\n        return False\n    \n    # Initialize agent\n    agent = TechnicalWriterAgent()\n    \n    # Test repository analysis\n    test_repo = input(\"Enter repository name (owner/repo) or press Enter for default: \")\n    if not test_repo.strip():\n        test_repo = \"octocat/Hello-World\"  # GitHub's test repository\n    \n    try:\n        print(f\"\\nAnalyzing repository: {test_repo}\")\n        repo_info = agent.analyze_repository(test_repo)\n        \n        print(f\"Repository Name: {repo_info['name']}\")\n        print(f\"Description: {repo_info['description']}\")\n        print(f\"Primary Language: {repo_info['language']}\")\n        print(f\"Topics: {', '.join(repo_info['topics'])}\")\n        print(f\"Files found: {len(repo_info['files'])}\")\n        \n        # Test documentation generation\n        print(\"\\nGenerating documentation...\")\n        new_readme = agent.generate_documentation(repo_info)\n        \n        # Save to local file for review\n        output_file = Path('test_readme.md')\n        with open(output_file, 'w') as f:\n            f.write(new_readme)\n        \n        print(f\"Generated documentation saved to {output_file}\")\n        print(f\"Documentation length: {len(new_readme)} characters\")\n        \n        # Ask if user wants to see the content\n        show_content = input(\"\\nShow generated content? (y/N): \")\n        if show_content.lower() == 'y':\n            print(\"\\n\" + \"=\"*50)\n            print(new_readme)\n            print(\"=\"*50)\n        \n        return True\n        \n    except Exception as e:\n        print(f\"Error during testing: {e}\")\n        return False\n\ndef main():\n    \"\"\"Main testing function\"\"\"\n    print(\"Technical Writer Agent - Local Testing\")\n    print(\"=\" * 40)\n    \n    # Check for .env file\n    env_file = Path('.env')\n    if not env_file.exists():\n        print(\"No .env file found. Please create one with your API keys.\")\n        print(\"Example .env content:\")\n        print(\"OPENAI_API_KEY=your_openai_key_here\")\n        print(\"GITHUB_TOKEN=your_github_token_here\")\n        return\n    \n    success = test_local_repository()\n    \n    if success:\n        print(\"\\nLocal testing completed successfully!\")\n        print(\"You can now deploy to GitHub Actions.\")\n    else:\n        print(\"\\nLocal testing failed. Please check your configuration.\")\n\nif __name__ == \"__main__\":\n    main()"
            }
          ],
          "checklist": [
            {
              "id": "check-2-2-1",
              "text": "Set up GitHub repository with proper permissions",
              "completed": false
            },
            {
              "id": "check-2-2-2",
              "text": "Configure OpenAI API key in GitHub Secrets",
              "completed": false
            },
            {
              "id": "check-2-2-3",
              "text": "Create GitHub developer token for repository access",
              "completed": false
            },
            {
              "id": "check-2-2-4",
              "text": "Fork the Technical Writer Agent repository",
              "completed": false
            },
            {
              "id": "check-2-2-5",
              "text": "Implement basic GitHub Actions workflow",
              "completed": false
            },
            {
              "id": "check-2-2-6",
              "text": "Test Technical Writer Agent locally",
              "completed": false
            },
            {
              "id": "check-2-2-7",
              "text": "Build RAG knowledge base from repository content",
              "completed": false
            },
            {
              "id": "check-2-2-8",
              "text": "Customize documentation templates for specific needs",
              "completed": false
            },
            {
              "id": "check-2-2-9",
              "text": "Deploy and test automated documentation pipeline",
              "completed": false
            },
            {
              "id": "check-2-2-10",
              "text": "Adapt project for workplace or personal requirements",
              "completed": false
            }
          ],
          "timestamps": [
            {
              "time": "0:00:28",
              "label": "Course Introduction",
              "description": "Review of required API keys and setup requirements"
            },
            {
              "time": "0:02:12",
              "label": "Environment Setup",
              "description": "Welcome and introduction to guided project format"
            },
            {
              "time": "0:02:46",
              "label": "First Guided Project",
              "description": "Introduction to hands-on keyboard approach"
            },
            {
              "time": "0:03:52",
              "label": "Developer Productivity",
              "description": "How to improve developer productivity with AI"
            },
            {
              "time": "0:04:19",
              "label": "Workplace Customization",
              "description": "Personalizing the project for your work environment"
            },
            {
              "time": "0:08:17",
              "label": "Repository Setup",
              "description": "Forking the repository and initial configuration"
            },
            {
              "time": "0:12:05",
              "label": "Test API",
              "description": "Validate language model and context functionality and edge cases"
            },
            {
              "time": "0:18:48",
              "label": "API Keys Configuration",
              "description": "Setting up OpenAI and GitHub developer tokens"
            },
            {
              "time": "0:24:49",
              "label": "Environment Variables",
              "description": "Configuring environment variables for the workflow"
            },
            {
              "time": "0:33:20",
              "label": "Python Script Implementation",
              "description": "Building the Technical Writer Agent script"
            },
            {
              "time": "0:41:28",
              "label": "RAG Integration",
              "description": "Adding retrieval augmented generation capabilities"
            },
            {
              "time": "0:47:24",
              "label": "Testing Workflow",
              "description": "Testing the GitHub Actions workflow locally"
            },
            {
              "time": "0:52:39",
              "label": "Documentation Generation",
              "description": "Generating and updating README files"
            },
            {
              "time": "0:56:23",
              "label": "Error Handling",
              "description": "Implementing error handling and debugging"
            },
            {
              "time": "1:03:12",
              "label": "Customization Options",
              "description": "Customizing the agent for specific needs"
            },
            {
              "time": "1:11:14",
              "label": "Advanced Features",
              "description": "Adding advanced features and integrations"
            },
            {
              "time": "1:18:12",
              "label": "Production Deployment",
              "description": "Deploying to production and best practices"
            },
            {
              "time": "1:23:08",
              "label": "Project Requirements",
              "description": "Course project requirements and expectations"
            },
            {
              "time": "1:28:00",
              "label": "Q&A Session",
              "description": "Questions and answers about the implementation"
            },
            {
              "time": "1:34:53",
              "label": "Test API",
              "description": "Summary and next steps for the project"
            }
          ],
          "videoPath": "cohort_2/week_2_class_2_2024-05-29.mp4"
        }
      ]
    },
    {
      "id": "week-3",
      "title": "Week 3: LangChain Framework Development",
      "description": "Master LangChain framework components including tool calling, agents, and production patterns.",
      "lessons": [
        {
          "id": "lesson-3-1",
          "title": "LangChain Framework Fundamentals",
          "duration": "1:31:35",
          "videoUrl": "/videos/cohort_2/week_03/week_3_class_1_2024-06-03.mp4",
          "videoId": "week_3_class_1_2024-06-03",
          "content": "# LangChain Framework Fundamentals\n\n## Overview\nThis lesson transitions from basic prompting to sophisticated tool calling capabilities in LangChain. We explore the fundamental differences between tool calling and function calling, implement structured outputs, and learn to build complex workflows that integrate multiple APIs and services.\n\n## API Integration Patterns\n\nAnd a really exciting way. So helping streamline complex API integrations\n\nThe examples we'll talk about today are very bare bones.\n\nYou're just to help wrap your head around what's happening, but we can talk a little bit about\n\n## Tool Calling Fundamentals\n\nSo like I alluded to earlier, tool calling is really going to be about helping us transform\n\ninteractions with our LLMs beyond just simple prompts that we've been\n\ninteracting with before. So really structured outputs, some code execution, and really getting into\n\n## Tool Calling Fundamentals\n\nunderstanding function and tool calling and kind of talk about the difference a little bit.\n\nThe second thing we're going to talk about is the ability to implement tool calling using Langchain.\n\nSo we're going to take a look at a very simple example using Langchain and implementate,\n\n## Function vs Tool Calling\n\nand or function calling, you're going to want to diagnose what's going on. Like what are the inputs\n\nand outlets that are passing between your different steps.\n\nSo if you have heard a tool calling before,\n\n## Tool Calling Fundamentals\n\nSo tool calling is going to be the process where models are going to generate a response for you\n\nthat can form to a predefined user schema. So that's going to suggest actions rather than\n\ntaking them directly. So a user is going to ask the question, that question is going to go into\n\n",
          "codeExamples": [
            {
              "id": "code-3-1-1",
              "title": "Basic Tool Calling with LangChain",
              "language": "python",
              "code": "from langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\n\n@tool\ndef calculate_area(length: float, width: float) -> float:\n    \"\"\"Calculate the area of a rectangle.\n    \n    Args:\n        length: The length of the rectangle\n        width: The width of the rectangle\n    \n    Returns:\n        The area of the rectangle\n    \"\"\"\n    return length * width\n\n# Initialize LLM with tool\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\")\nllm_with_tools = llm.bind_tools([calculate_area])\n\n# Use the tool\nresponse = llm_with_tools.invoke(\"What's the area of a rectangle with length 5 and width 3?\")\nprint(response.tool_calls)"
            },
            {
              "id": "code-3-1-2",
              "title": "OpenAI Function Calling Format",
              "language": "python",
              "code": "# OpenAI function specification\nfunction_spec = {\n    \"name\": \"get_weather\",\n    \"description\": \"Get the current weather in a location\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The city and state, e.g. San Francisco, CA\"\n            },\n            \"unit\": {\n                \"type\": \"string\",\n                \"enum\": [\"celsius\", \"fahrenheit\"],\n                \"description\": \"Temperature unit\"\n            }\n        },\n        \"required\": [\"location\"]\n    }\n}\n\n# Convert to LangChain tool\nfrom langchain_core.utils.function_calling import convert_to_openai_function\n\n@tool\ndef get_weather(location: str, unit: str = \"fahrenheit\") -> str:\n    \"\"\"Get the current weather in a location.\"\"\"\n    # Implementation here\n    return f\"Weather in {location}: 72\u00b0{unit[0].upper()}\"\n\n# Get OpenAI function format\nopenai_function = convert_to_openai_function(get_weather)\nprint(json.dumps(openai_function, indent=2))"
            },
            {
              "id": "code-3-1-3",
              "title": "Structured Output with Pydantic",
              "language": "python",
              "code": "from pydantic import BaseModel, Field\nfrom langchain_openai import ChatOpenAI\n\nclass UserInfo(BaseModel):\n    name: str = Field(description=\"User's full name\")\n    age: int = Field(description=\"User's age in years\")\n    email: str = Field(description=\"User's email address\")\n    interests: list[str] = Field(description=\"List of user interests\")\n\n# Configure LLM for structured output\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\")\nstructured_llm = llm.with_structured_output(UserInfo)\n\n# Extract structured data from text\nuser_text = \"John Doe is 28 years old. His email is john@example.com. He likes hiking, photography, and cooking.\"\nuser_info = structured_llm.invoke(f\"Extract user information from: {user_text}\")\n\nprint(f\"Name: {user_info.name}\")\nprint(f\"Age: {user_info.age}\")\nprint(f\"Email: {user_info.email}\")\nprint(f\"Interests: {', '.join(user_info.interests)}\")"
            }
          ],
          "checklist": [
            {
              "id": "check-3-1-1",
              "text": "Understand the difference between tool calling and function calling",
              "completed": false
            },
            {
              "id": "check-3-1-2",
              "text": "Implement basic tool calling with LangChain decorators",
              "completed": false
            },
            {
              "id": "check-3-1-3",
              "text": "Convert OpenAI function specifications to LangChain tools",
              "completed": false
            },
            {
              "id": "check-3-1-4",
              "text": "Create structured outputs using Pydantic models",
              "completed": false
            },
            {
              "id": "check-3-1-5",
              "text": "Build tool workflows with proper error handling",
              "completed": false
            },
            {
              "id": "check-3-1-6",
              "text": "Integrate LangSmith for tool monitoring and debugging",
              "completed": false
            },
            {
              "id": "check-3-1-7",
              "text": "Design JSON schemas for API integration",
              "completed": false
            },
            {
              "id": "check-3-1-8",
              "text": "Test tool calling with various input formats",
              "completed": false
            }
          ],
          "timestamps": [
            {
              "time": "0:01:36",
              "label": "Course Introduction",
              "description": "about just intro material and"
            },
            {
              "time": "0:05:19",
              "label": "API Configuration",
              "description": "Set up endpoints and authentication for calling and fundamentals"
            },
            {
              "time": "0:10:14",
              "label": "Function vs Tool Calling",
              "description": "including the model and function responses."
            },
            {
              "time": "0:15:01",
              "label": "LangChain Implementation",
              "description": "So now we can send our message and our choice of tools"
            },
            {
              "time": "0:20:28",
              "label": "Structured Outputs",
              "description": "Learn calling and fundamentals implementation techniques"
            },
            {
              "time": "0:25:03",
              "label": "JSON Schema and Validation",
              "description": "So we're gonna bind FetchWeather to the model,"
            },
            {
              "time": "0:31:57",
              "label": "Code Examples and Demos",
              "description": "Learn calling and fundamentals implementation techniques"
            },
            {
              "time": "0:48:00",
              "label": "Building Tool Workflows",
              "description": "that you're looking to solve with this kind of workflow"
            },
            {
              "time": "0:50:05",
              "label": "API Integration Patterns",
              "description": "So I know that there are tools to do this natively."
            },
            {
              "time": "1:00:11",
              "label": "LangSmith Monitoring",
              "description": "Learn calling and fundamentals implementation techniques"
            },
            {
              "time": "1:10:05",
              "label": "Debugging Tool Calls",
              "description": "So, like, the multiple clients you're going to get, they're going to look really good"
            },
            {
              "time": "1:20:00",
              "label": "API Configuration",
              "description": "and just raw JSON format."
            },
            {
              "time": "1:30:00",
              "label": "Wrap-up and Next Steps",
              "description": "Well, everybody, I think this class, I'm kind of happy the way it turned,"
            }
          ],
          "videoPath": "cohort_2/week_3_class_1_2024-06-03.mp4"
        },
        {
          "id": "lesson-3-2",
          "title": "Agents and Tools Development",
          "duration": "1:18:27",
          "videoUrl": "/videos/cohort_2/week_03/week_3_class_2_2024-06-04.mp4",
          "videoId": "week_3_class_2_2024-06-04",
          "content": "# Agents and Tools Development\n\n## Overview\nA hands-on session focused on building custom tools and agents. We implement mathematical tools, explore agent architectures, and learn debugging strategies for complex LangChain applications. This office hours format encourages experimentation and problem-solving.\n\n## Office Hours Overview\n\nHello and welcome to the AI developer for Productivity Office hours.\n\nToday we're going to be talking a little bit about tooling.\n\nWe're going to go over the basic sort of math tooling example that I've been given to get a guy over for you.\n\n## Agent Architecture\n\nkind of already had to play with and stuff so it's just a bunch of agents that we're going\n\nto be doing. It's designed to mimic the intelligent entities that perceive the environment. So\n\nit's designed to be, Christopher what you was kind of semi-illuding to in the first place.\n\n## Agent Architecture\n\nolder versions of the agent stuff so that we can get it working versus using the bleeding edge\n\nwhich then gives us broken code. So it's sometimes best to kind of keep it a little bit\n\noutside of being latest because even the latest just says you've got deprecated things\n\n## Agent Architecture\n\nWe're going to be taking these agents. We're going to have an agent executor and we're going to have\n\nthe Create Open AI Tool Agent or Tools Agent, which is going to kind of consolidate\n\nthey tool that we're pulling in.\n\n## Agent Architecture\n\nto have an agent scratch pad at the end of that so it's going to be kind of the shape of our\n\nmessages on that template. I want to run that so it makes that prompt basically just taking\n\nquite some time for such trivial things. I don't know the fact we're doing a tick box.\n\n",
          "codeExamples": [
            {
              "id": "code-3-2-1",
              "title": "Mathematical Tools Implementation",
              "language": "python",
              "code": "from langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\nimport math\n\n@tool\ndef calculator(expression: str) -> float:\n    \"\"\"Evaluate a mathematical expression.\n    \n    Args:\n        expression: A valid Python mathematical expression\n    \n    Returns:\n        The result of the calculation\n    \"\"\"\n    try:\n        # Safe evaluation of math expressions\n        allowed_names = {\n            k: v for k, v in math.__dict__.items() \n            if not k.startswith(\"__\")\n        }\n        result = eval(expression, {\"__builtins__\": {}}, allowed_names)\n        return float(result)\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\n@tool\ndef fibonacci(n: int) -> int:\n    \"\"\"Calculate the nth Fibonacci number.\n    \n    Args:\n        n: The position in the Fibonacci sequence\n    \n    Returns:\n        The nth Fibonacci number\n    \"\"\"\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        a, b = 0, 1\n        for _ in range(2, n + 1):\n            a, b = b, a + b\n        return b\n\n# Create agent with mathematical tools\ntools = [calculator, fibonacci]\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\nllm_with_tools = llm.bind_tools(tools)\n\n# Test the tools\nresponse = llm_with_tools.invoke(\"What is the 10th Fibonacci number multiplied by the square root of 16?\")\nprint(response.tool_calls)"
            },
            {
              "id": "code-3-2-2",
              "title": "Custom Tool with Error Handling",
              "language": "python",
              "code": "from langchain_core.tools import tool\nfrom typing import Optional\nimport requests\n\n@tool\ndef fetch_data(url: str, timeout: int = 5) -> str:\n    \"\"\"Fetch data from a URL with error handling.\n    \n    Args:\n        url: The URL to fetch data from\n        timeout: Request timeout in seconds\n    \n    Returns:\n        The fetched data or error message\n    \"\"\"\n    try:\n        response = requests.get(url, timeout=timeout)\n        response.raise_for_status()\n        \n        # Return first 500 chars of response\n        content = response.text[:500]\n        return f\"Success: {content}...\"\n        \n    except requests.exceptions.Timeout:\n        return \"Error: Request timed out\"\n    except requests.exceptions.ConnectionError:\n        return \"Error: Could not connect to server\"\n    except requests.exceptions.HTTPError as e:\n        return f\"Error: HTTP {e.response.status_code}\"\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\n# Test with error scenarios\ntest_urls = [\n    \"https://api.github.com/users/github\",\n    \"https://invalid-url-that-does-not-exist.com\",\n    \"https://httpstat.us/500\"  # Returns 500 error\n]\n\nfor url in test_urls:\n    result = fetch_data.invoke({\"url\": url})\n    print(f\"URL: {url}\\nResult: {result[:100]}...\\n\")"
            },
            {
              "id": "code-3-2-3",
              "title": "Agent with Tool Selection",
              "language": "python",
              "code": "from langchain.agents import create_openai_tools_agent, AgentExecutor\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.tools import tool\n\n# Define multiple tools\n@tool\ndef search_web(query: str) -> str:\n    \"\"\"Search the web for information.\"\"\"\n    return f\"Search results for '{query}': [mock results]\"\n\n@tool\ndef get_current_time() -> str:\n    \"\"\"Get the current date and time.\"\"\"\n    from datetime import datetime\n    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n@tool\ndef send_email(to: str, subject: str, body: str) -> str:\n    \"\"\"Send an email (mock implementation).\"\"\"\n    return f\"Email sent to {to} with subject '{subject}'\"\n\n# Create prompt template\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant with access to various tools.\"),\n    (\"user\", \"{input}\"),\n    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n])\n\n# Initialize agent\ntools = [search_web, get_current_time, send_email]\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\nagent = create_openai_tools_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\n# Test agent with different queries\nqueries = [\n    \"What time is it?\",\n    \"Search for information about LangChain agents\",\n    \"Send an email to john@example.com about our meeting tomorrow\"\n]\n\nfor query in queries:\n    print(f\"\\nQuery: {query}\")\n    result = agent_executor.invoke({\"input\": query})\n    print(f\"Result: {result['output']}\")"
            }
          ],
          "checklist": [
            {
              "id": "check-3-2-1",
              "text": "Build custom mathematical tools with error handling",
              "completed": false
            },
            {
              "id": "check-3-2-2",
              "text": "Implement safe evaluation of mathematical expressions",
              "completed": false
            },
            {
              "id": "check-3-2-3",
              "text": "Create tools with timeout and retry logic",
              "completed": false
            },
            {
              "id": "check-3-2-4",
              "text": "Design agents that select appropriate tools",
              "completed": false
            },
            {
              "id": "check-3-2-5",
              "text": "Debug tool execution using verbose mode",
              "completed": false
            },
            {
              "id": "check-3-2-6",
              "text": "Handle API errors gracefully in custom tools",
              "completed": false
            },
            {
              "id": "check-3-2-7",
              "text": "Test tools with edge cases and invalid inputs",
              "completed": false
            },
            {
              "id": "check-3-2-8",
              "text": "Implement tool validation and input sanitization",
              "completed": false
            },
            {
              "id": "check-3-2-9",
              "text": "Create comprehensive tool documentation",
              "completed": false
            },
            {
              "id": "check-3-2-10",
              "text": "Practice debugging complex agent workflows",
              "completed": false
            }
          ],
          "timestamps": [
            {
              "time": "0:00:06",
              "label": "Course Introduction",
              "description": "Hello and welcome to the AI developer for Productivity Office hours."
            },
            {
              "time": "0:05:32",
              "label": "Mathematical Tools Setup",
              "description": "pie that I have math library just used pie there but that's fine and then"
            },
            {
              "time": "0:10:04",
              "label": "Calculator Implementation",
              "description": "like an agent execute or something that can run things, utilizing the toolkit and also"
            },
            {
              "time": "0:15:04",
              "label": "Agent Architecture",
              "description": "and whatever other sort of things you want it to add."
            },
            {
              "time": "0:20:06",
              "label": "Custom Tool Development",
              "description": "It's sort of on a skew, right?"
            },
            {
              "time": "0:25:01",
              "label": "Error Handling Strategies",
              "description": "Learn office and architecture implementation techniques"
            },
            {
              "time": "0:30:01",
              "label": "Fibonacci Tool Example",
              "description": "But we had as he's making up a find area of trappers"
            },
            {
              "time": "0:35:00",
              "label": "Debugging Agent Behavior",
              "description": "It's because it's already a whole framework of things for us to actually utilize."
            },
            {
              "time": "0:40:11",
              "label": "Tool Selection Logic",
              "description": "Learn office and architecture implementation techniques"
            },
            {
              "time": "0:45:02",
              "label": "Hands-on Practice",
              "description": "process run, not the best of things to use, but it does the"
            },
            {
              "time": "0:55:07",
              "label": "Project Implementation",
              "description": "That makes sense to me."
            },
            {
              "time": "1:00:05",
              "label": "Student Questions",
              "description": "Learn office and architecture implementation techniques"
            },
            {
              "time": "1:10:02",
              "label": "Troubleshooting Common Issues",
              "description": "Debug common office and architecture implementation issues"
            }
          ],
          "videoPath": "cohort_2/week_3_class_2_2024-06-04.mp4"
        },
        {
          "id": "lesson-3-3",
          "title": "Advanced LangChain Patterns",
          "duration": "1:21:13",
          "videoUrl": "/videos/cohort_2/week_03/week_3_class_3_2024-06-05.mp4",
          "videoId": "week_3_class_3_2024-06-05",
          "content": "# Advanced LangChain Patterns\n\n## Overview\nAdvanced patterns for production-ready LangChain applications. We dive into the ReAct framework, build research assistants, and explore multi-agent systems. Focus on optimization, monitoring, and best practices for deploying agents at scale.\n\n## Research Assistant\n\nsomebody write a researcher for it about a mysterious model.\n\nSo the idea of agents is really that sequence of reasoning\n\n## ReAct Framework\n\nWe're going to talk about the React framework\n\nin the context of LLMs, re and act.\n\nand then the basic agent to lifestyle, or lifestyle,\n\n## Research Assistant\n\nand I'll show the example of the research assistant\n\nAnd then we'll talk a little bit about viewing agents and\n\n## ReAct Framework\n\nSo the React framework, not the JavaScript one,\n\nis to stands for reasoning and acting.\n\nSo it's going to enable LLMs to mimic human-like behavior,\n\n## Reasoning and Action\n\na reasoning trace, you're gonna ask a question,\n\nit provides a reasoning and then that reasoning\n\ngoes back into the LLM.\n\n",
          "codeExamples": [
            {
              "id": "code-3-3-1",
              "title": "ReAct Agent Implementation",
              "language": "python",
              "code": "from langchain.agents import create_react_agent, AgentExecutor\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.tools import tool\n\n# ReAct prompt template\nreact_prompt = PromptTemplate.from_template(\"\"\"\nYou are an AI assistant using the ReAct framework.\nYou have access to the following tools:\n{tools}\n\nUse the following format:\nThought: Think about what to do\nAction: the action to take, should be one of [{tool_names}]\nAction Input: the input to the action\nObservation: the result of the action\n... (repeat Thought/Action/Action Input/Observation as needed)\nThought: I now know the final answer\nFinal Answer: the final answer to the original question\n\nQuestion: {input}\nThought: {agent_scratchpad}\n\"\"\")\n\n@tool\ndef analyze_text(text: str) -> str:\n    \"\"\"Analyze text and return key insights.\"\"\"\n    word_count = len(text.split())\n    char_count = len(text)\n    return f\"Analysis: {word_count} words, {char_count} characters\"\n\n@tool\ndef generate_summary(text: str, max_length: int = 50) -> str:\n    \"\"\"Generate a summary of the text.\"\"\"\n    words = text.split()[:max_length]\n    return ' '.join(words) + \"...\"\n\n# Create ReAct agent\ntools = [analyze_text, generate_summary]\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\nagent = create_react_agent(llm, tools, react_prompt)\nagent_executor = AgentExecutor(\n    agent=agent,\n    tools=tools,\n    verbose=True,\n    max_iterations=3\n)\n\n# Test ReAct reasoning\nquery = \"Analyze this text and provide a summary: 'LangChain is a framework for developing applications powered by language models. It enables applications that are context-aware and can reason about their environment.'\"\nresult = agent_executor.invoke({\"input\": query})\nprint(result[\"output\"])"
            },
            {
              "id": "code-3-3-2",
              "title": "Research Assistant with Multiple Tools",
              "language": "python",
              "code": "from langchain_core.tools import tool\nfrom langchain.agents import AgentExecutor, create_openai_tools_agent\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_openai import ChatOpenAI\nfrom typing import List, Dict\n\n@tool\ndef search_papers(topic: str, max_results: int = 5) -> List[Dict]:\n    \"\"\"Search for academic papers on a topic.\n    \n    Args:\n        topic: Research topic to search for\n        max_results: Maximum number of results\n    \n    Returns:\n        List of paper metadata\n    \"\"\"\n    # Mock implementation\n    papers = [\n        {\"title\": f\"Paper on {topic} #1\", \"year\": 2024, \"citations\": 45},\n        {\"title\": f\"Survey of {topic}\", \"year\": 2023, \"citations\": 120},\n        {\"title\": f\"Advances in {topic}\", \"year\": 2024, \"citations\": 23}\n    ]\n    return papers[:max_results]\n\n@tool\ndef extract_key_points(paper_title: str) -> List[str]:\n    \"\"\"Extract key points from a paper.\n    \n    Args:\n        paper_title: Title of the paper\n    \n    Returns:\n        List of key points\n    \"\"\"\n    # Mock implementation\n    return [\n        f\"Key finding 1 from {paper_title}\",\n        f\"Novel approach discussed in {paper_title}\",\n        f\"Future directions suggested by {paper_title}\"\n    ]\n\n@tool \ndef generate_bibliography(papers: List[str]) -> str:\n    \"\"\"Generate a formatted bibliography.\n    \n    Args:\n        papers: List of paper titles\n    \n    Returns:\n        Formatted bibliography\n    \"\"\"\n    bibliography = \"\\n\".join([\n        f\"{i+1}. {paper} (2024)\" \n        for i, paper in enumerate(papers)\n    ])\n    return f\"Bibliography:\\n{bibliography}\"\n\n# Create research assistant\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a research assistant helping with literature review. Use the available tools to search for papers, extract key points, and generate bibliographies.\"),\n    (\"user\", \"{input}\"),\n    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n])\n\ntools = [search_papers, extract_key_points, generate_bibliography]\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\nagent = create_openai_tools_agent(llm, tools, prompt)\nresearch_assistant = AgentExecutor(\n    agent=agent,\n    tools=tools,\n    verbose=True,\n    handle_parsing_errors=True\n)\n\n# Use the research assistant\nquery = \"Find papers on transformer architectures and summarize the key findings\"\nresult = research_assistant.invoke({\"input\": query})\nprint(f\"Research Summary:\\n{result['output']}\")"
            },
            {
              "id": "code-3-3-3",
              "title": "Production Monitoring with LangSmith",
              "language": "python",
              "code": "import os\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\nfrom langsmith import Client\nfrom langchain.callbacks import LangChainTracer\n\n# Configure LangSmith\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\nos.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\nos.environ[\"LANGCHAIN_API_KEY\"] = \"your-api-key\"\nos.environ[\"LANGCHAIN_PROJECT\"] = \"production-monitoring\"\n\n# Initialize LangSmith client\nclient = Client()\n\n@tool\ndef monitored_api_call(endpoint: str, params: dict) -> str:\n    \"\"\"Make an API call with full monitoring.\n    \n    Args:\n        endpoint: API endpoint\n        params: Request parameters\n    \n    Returns:\n        API response\n    \"\"\"\n    import time\n    start = time.time()\n    \n    try:\n        # Simulate API call\n        response = f\"Response from {endpoint} with {params}\"\n        latency = time.time() - start\n        \n        # Log metrics\n        print(f\"API Call - Endpoint: {endpoint}, Latency: {latency:.2f}s\")\n        \n        return response\n    except Exception as e:\n        # Log errors\n        print(f\"API Error - Endpoint: {endpoint}, Error: {str(e)}\")\n        raise\n\n# Create monitored LLM\nllm = ChatOpenAI(\n    model=\"gpt-3.5-turbo\",\n    temperature=0,\n    callbacks=[LangChainTracer()]\n)\n\n# Bind tools with monitoring\ntools = [monitored_api_call]\nllm_with_monitoring = llm.bind_tools(tools)\n\n# Example usage with tracing\ndef process_request(query: str):\n    \"\"\"Process a request with full monitoring.\"\"\"\n    try:\n        response = llm_with_monitoring.invoke(query)\n        \n        # Log successful execution\n        print(f\"Success - Query: {query[:50]}...\")\n        \n        return response\n    except Exception as e:\n        # Log failures\n        print(f\"Failure - Query: {query[:50]}..., Error: {str(e)}\")\n        raise\n\n# Test with monitoring\ntest_queries = [\n    \"Call the user API endpoint with params {id: 123}\",\n    \"Make a request to the orders endpoint with {status: pending}\"\n]\n\nfor query in test_queries:\n    result = process_request(query)\n    print(f\"Processed: {query}\\n\")"
            }
          ],
          "checklist": [
            {
              "id": "check-3-3-1",
              "text": "Implement the ReAct framework for reasoning",
              "completed": false
            },
            {
              "id": "check-3-3-2",
              "text": "Build a research assistant with multiple tools",
              "completed": false
            },
            {
              "id": "check-3-3-3",
              "text": "Configure LangSmith for production monitoring",
              "completed": false
            },
            {
              "id": "check-3-3-4",
              "text": "Design multi-agent systems for complex tasks",
              "completed": false
            },
            {
              "id": "check-3-3-5",
              "text": "Optimize agent performance and reduce latency",
              "completed": false
            },
            {
              "id": "check-3-3-6",
              "text": "Implement proper error handling and fallbacks",
              "completed": false
            },
            {
              "id": "check-3-3-7",
              "text": "Create monitoring dashboards for agent metrics",
              "completed": false
            },
            {
              "id": "check-3-3-8",
              "text": "Deploy agents with proper scaling strategies",
              "completed": false
            },
            {
              "id": "check-3-3-9",
              "text": "Document best practices for production deployment",
              "completed": false
            },
            {
              "id": "check-3-3-10",
              "text": "Test agent behavior under various load conditions",
              "completed": false
            }
          ],
          "timestamps": [
            {
              "time": "0:01:01",
              "label": "Course Introduction",
              "description": "I'm going to go to."
            },
            {
              "time": "0:06:32",
              "label": "Agent Architecture",
              "description": "So if React is just reasoning and action,"
            },
            {
              "time": "0:13:41",
              "label": "Reasoning and Action Cycles",
              "description": "in the terminal as it's reasoning"
            },
            {
              "time": "0:18:37",
              "label": "Research Assistant Demo",
              "description": "We want the agent to be our research assistant."
            },
            {
              "time": "0:20:00",
              "label": "ReAct Implementation",
              "description": "but they are using functions like this"
            },
            {
              "time": "0:25:19",
              "label": "Production Deployment",
              "description": "Well, let's try and break it."
            },
            {
              "time": "0:30:01",
              "label": "Performance Monitoring",
              "description": "Is this like what do I need to do to enable that?"
            },
            {
              "time": "0:35:00",
              "label": "Optimization Strategies",
              "description": "just because it seems like there's"
            },
            {
              "time": "0:40:03",
              "label": "Multi-Agent Systems",
              "description": "The other questions about agents?"
            },
            {
              "time": "0:45:01",
              "label": "Advanced Patterns",
              "description": "Learn sequence and assistant implementation techniques"
            },
            {
              "time": "0:55:00",
              "label": "Best Practices",
              "description": "So that agent in BeanMe is gonna go and do research."
            },
            {
              "time": "1:00:00",
              "label": "Scaling Considerations",
              "description": "Learn sequence and assistant implementation techniques"
            },
            {
              "time": "1:10:02",
              "label": "Real-world Case Studies",
              "description": "Learn sequence and assistant implementation techniques"
            },
            {
              "time": "1:20:01",
              "label": "Final Q&A Session",
              "description": "So this is like also a fun job to do."
            }
          ],
          "videoPath": "cohort_2/week_3_class_3_2024-06-05.mp4"
        }
      ]
    },
    {
      "id": "week-4",
      "title": "Week 4: Multi-modal AI & Integration",
      "description": "Explore advanced LangChain patterns, AI agent architecture, and complex workflow implementation.",
      "lessons": [
        {
          "id": "lesson-4-1",
          "title": "LangChain Advanced Patterns",
          "duration": "1:13:54",
          "videoUrl": "/videos/cohort_2/week_04/week_4_class_1_2024-06-10.mp4",
          "videoId": "week_4_class_1_2024-06-10",
          "content": "# LangChain Advanced Patterns\n\n## Overview\n\nThis lesson dives deep into advanced LangChain patterns, exploring sophisticated techniques for building production-ready AI applications. We cover complex chain composition, memory systems, callback handlers, and performance optimization strategies.\n\n## Chain Composition\n\nAdvanced chain composition allows you to build complex workflows by combining multiple chains, each handling specific tasks. This modular approach improves maintainability and enables sophisticated reasoning patterns.\n\n## Memory and State Management\n\nEffective memory management is crucial for maintaining context across interactions. We explore different memory types including conversation memory, entity memory, and knowledge graph memory, along with strategies for efficient state persistence.\n\n## Callback Handlers\n\nCallback handlers provide hooks into the LangChain execution pipeline, enabling monitoring, logging, and custom processing at various stages. Learn to implement callbacks for debugging, performance tracking, and integration with external systems.\n\n## Streaming and Real-time Processing\n\nStreaming responses enable better user experience by providing incremental results. We cover streaming implementations, token-by-token processing, and techniques for handling real-time data flows.\n\n## Performance Optimization\n\nOptimization strategies including caching, parallel processing, and efficient prompt engineering. Learn to profile your LangChain applications and identify bottlenecks for improved performance.",
          "codeExamples": [
            {
              "id": "code-4-1-1",
              "title": "Advanced Chain Composition",
              "language": "python",
              "code": "from langchain.chains import LLMChain, SequentialChain\nfrom langchain_openai import ChatOpenAI\nfrom langchain.prompts import PromptTemplate\n\n# Create multiple specialized chains\nllm = ChatOpenAI(temperature=0)\n\n# Chain 1: Extract key information\nextract_chain = LLMChain(\n    llm=llm,\n    prompt=PromptTemplate(\n        input_variables=[\"text\"],\n        template=\"Extract key facts from: {text}\\n\\nKey facts:\"\n    ),\n    output_key=\"facts\"\n)\n\n# Chain 2: Analyze sentiment\nsentiment_chain = LLMChain(\n    llm=llm,\n    prompt=PromptTemplate(\n        input_variables=[\"facts\"],\n        template=\"Analyze sentiment of: {facts}\\n\\nSentiment analysis:\"\n    ),\n    output_key=\"sentiment\"\n)\n\n# Chain 3: Generate summary\nsummary_chain = LLMChain(\n    llm=llm,\n    prompt=PromptTemplate(\n        input_variables=[\"facts\", \"sentiment\"],\n        template=\"\"\"\nBased on these facts: {facts}\nAnd this sentiment: {sentiment}\n\nGenerate a comprehensive summary:\n\"\"\"\n    ),\n    output_key=\"summary\"\n)\n\n# Compose chains\nsequential_chain = SequentialChain(\n    chains=[extract_chain, sentiment_chain, summary_chain],\n    input_variables=[\"text\"],\n    output_variables=[\"facts\", \"sentiment\", \"summary\"],\n    verbose=True\n)\n\n# Execute composed chain\nresult = sequential_chain({\n    \"text\": \"The new product launch exceeded expectations with record sales...\"\n})\nprint(result[\"summary\"])"
            },
            {
              "id": "code-4-1-2",
              "title": "Memory System Implementation",
              "language": "python",
              "code": "from langchain.memory import ConversationSummaryBufferMemory\nfrom langchain.memory import CombinedMemory, ConversationEntityMemory\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(temperature=0)\n\n# Create different memory types\nconv_memory = ConversationSummaryBufferMemory(\n    llm=llm,\n    max_token_limit=200,\n    memory_key=\"chat_history\",\n    return_messages=True\n)\n\nentity_memory = ConversationEntityMemory(\n    llm=llm,\n    memory_key=\"entity_history\",\n    return_messages=True\n)\n\n# Combine memories\nmemory = CombinedMemory(memories=[conv_memory, entity_memory])\n\n# Custom memory with persistence\nclass PersistentMemory:\n    def __init__(self, file_path=\"memory.json\"):\n        self.file_path = file_path\n        self.memory = self._load_memory()\n    \n    def _load_memory(self):\n        import os\n        if os.path.exists(self.file_path):\n            with open(self.file_path, 'r') as f:\n                return json.load(f)\n        return {\"conversations\": [], \"entities\": {}}\n    \n    def save_memory(self):\n        with open(self.file_path, 'w') as f:\n            json.dump(self.memory, f, indent=2)\n    \n    def add_conversation(self, user_input, ai_response):\n        self.memory[\"conversations\"].append({\n            \"user\": user_input,\n            \"ai\": ai_response,\n            \"timestamp\": str(datetime.now())\n        })\n        self.save_memory()\n    \n    def update_entity(self, entity, information):\n        if entity not in self.memory[\"entities\"]:\n            self.memory[\"entities\"][entity] = []\n        self.memory[\"entities\"][entity].append(information)\n        self.save_memory()\n    \n    def get_context(self, num_recent=5):\n        recent = self.memory[\"conversations\"][-num_recent:]\n        return \"\\n\".join([f\"User: {c['user']}\\nAI: {c['ai']}\" for c in recent])\n\n# Usage\npersistent_mem = PersistentMemory()\npersistent_mem.add_conversation(\"What's the weather?\", \"It's sunny today.\")\ncontext = persistent_mem.get_context()"
            },
            {
              "id": "code-4-1-3",
              "title": "Streaming with Callbacks",
              "language": "python",
              "code": "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nfrom langchain.callbacks.base import BaseCallbackHandler\nfrom langchain_openai import ChatOpenAI\nimport time\n\nclass CustomStreamingCallback(BaseCallbackHandler):\n    \"\"\"Custom callback for streaming with metrics\"\"\"\n    \n    def __init__(self):\n        self.tokens = []\n        self.start_time = None\n        self.token_count = 0\n    \n    def on_llm_start(self, serialized, prompts, **kwargs):\n        self.start_time = time.time()\n        self.tokens = []\n        self.token_count = 0\n        print(\"\\n\ud83d\ude80 Starting generation...\")\n    \n    def on_llm_new_token(self, token, **kwargs):\n        self.tokens.append(token)\n        self.token_count += 1\n        print(token, end=\"\", flush=True)\n        \n        # Show progress every 10 tokens\n        if self.token_count % 10 == 0:\n            elapsed = time.time() - self.start_time\n            tps = self.token_count / elapsed\n            print(f\" [{tps:.1f} tokens/sec]\", end=\"\", flush=True)\n    \n    def on_llm_end(self, response, **kwargs):\n        elapsed = time.time() - self.start_time\n        print(f\"\\n\\n\u2705 Completed in {elapsed:.2f}s\")\n        print(f\"\ud83d\udcca Generated {self.token_count} tokens\")\n        print(f\"\u26a1 Average: {self.token_count/elapsed:.1f} tokens/sec\")\n    \n    def on_llm_error(self, error, **kwargs):\n        print(f\"\\n\u274c Error: {error}\")\n\n# Use streaming with custom callback\nstreaming_llm = ChatOpenAI(\n    temperature=0,\n    streaming=True,\n    callbacks=[CustomStreamingCallback()]\n)\n\n# Stream response\nresponse = streaming_llm.invoke(\n    \"Explain quantum computing in simple terms.\"\n)\n\n# Async streaming for better performance\nimport asyncio\nfrom langchain.callbacks.manager import AsyncCallbackManager\n\nasync def async_generate():\n    async_llm = ChatOpenAI(\n        temperature=0,\n        streaming=True,\n        callback_manager=AsyncCallbackManager([CustomStreamingCallback()])\n    )\n    \n    response = await async_llm.agenerate([\n        \"Write a short story about AI.\"\n    ])\n    return response\n\n# Run async\n# asyncio.run(async_generate())"
            }
          ],
          "checklist": [
            {
              "id": "check-4-1-1",
              "text": "Master complex chain composition patterns",
              "completed": false
            },
            {
              "id": "check-4-1-2",
              "text": "Implement various memory systems for context retention",
              "completed": false
            },
            {
              "id": "check-4-1-3",
              "text": "Create custom callback handlers for monitoring",
              "completed": false
            },
            {
              "id": "check-4-1-4",
              "text": "Build streaming response systems",
              "completed": false
            },
            {
              "id": "check-4-1-5",
              "text": "Optimize LangChain performance with caching",
              "completed": false
            },
            {
              "id": "check-4-1-6",
              "text": "Handle errors gracefully in complex chains",
              "completed": false
            },
            {
              "id": "check-4-1-7",
              "text": "Implement parallel processing for efficiency",
              "completed": false
            },
            {
              "id": "check-4-1-8",
              "text": "Profile and identify bottlenecks",
              "completed": false
            }
          ],
          "timestamps": [
            {
              "time": "0:03:18",
              "label": "Course Introduction",
              "description": "Introduction to Week 4, moving beyond basic prompts to chains"
            },
            {
              "time": "0:03:42",
              "label": "What are Chains?",
              "description": "Understanding LangChain's chain concept and ecosystem"
            },
            {
              "time": "0:04:18",
              "label": "Learning Objectives",
              "description": "Goals: Understanding chains, implementing LCEL, data passing, and concurrency"
            },
            {
              "time": "0:05:46",
              "label": "DAGs and Chain Architecture",
              "description": "Chains as directed acyclic graphs, series of components"
            },
            {
              "time": "0:08:30",
              "label": "LangChain Expression Language (LCEL)",
              "description": "Introduction to LCEL syntax and benefits"
            },
            {
              "time": "0:12:15",
              "label": "Simple Chain Example",
              "description": "Building your first chain with prompt templates"
            },
            {
              "time": "0:18:42",
              "label": "Data Flow in Chains",
              "description": "How data passes through chain components"
            },
            {
              "time": "0:24:30",
              "label": "Sequential vs Parallel Chains",
              "description": "Learn context and production implementation techniques"
            },
            {
              "time": "0:31:20",
              "label": "Memory in Chains",
              "description": "Adding memory and context to chain workflows"
            },
            {
              "time": "0:38:15",
              "label": "Advanced LCEL Syntax",
              "description": "Pipe operators, RunnablePassthrough, and RunnableLambda"
            },
            {
              "time": "0:45:30",
              "label": "Error Handling in Chains",
              "description": "Handling failures and implementing fallbacks"
            },
            {
              "time": "0:52:45",
              "label": "RAG Chain Example",
              "description": "Building a retrieval-augmented generation chain"
            },
            {
              "time": "0:59:20",
              "label": "Performance Optimization",
              "description": "Caching, batching, and streaming in chains"
            },
            {
              "time": "1:06:15",
              "label": "Debugging Chain Execution",
              "description": "Using verbose mode and callbacks for debugging"
            },
            {
              "time": "1:11:30",
              "label": "Q&A and Wrap-up",
              "description": "Student questions and summary of key concepts"
            }
          ],
          "videoPath": "cohort_2/week_4_class_1_2024-06-10.mp4"
        },
        {
          "id": "lesson-4-2",
          "title": "AI Agent Architecture",
          "duration": "1:30:00",
          "videoUrl": "/videos/cohort_2/week_04/week_4_class_2_2024-06-11.mp4",
          "videoId": "week_4_class_2_2024-06-11",
          "content": "# AI Agent Architecture\n\n## Overview\n\nA comprehensive exploration of AI agent architecture, covering planning, memory, tool integration, and multi-agent coordination. This office hours session provides hands-on experience building sophisticated agent systems.\n\n## Agent Architecture Fundamentals\n\nUnderstanding the core components of AI agents: perception, planning, action, and learning. We explore different architectural patterns and their trade-offs for various use cases.\n\n## Planning and Reasoning\n\nAgents need to plan sequences of actions to achieve goals. We cover planning algorithms, reasoning strategies, and techniques for handling uncertainty and incomplete information.\n\n## Memory Systems\n\nDifferent memory architectures for agents including short-term working memory, long-term episodic memory, and semantic memory. Learn to implement memory systems that enable agents to learn from experience.\n\n## Tool Integration\n\nAgents become powerful when equipped with tools. We explore patterns for tool discovery, selection, and execution, along with strategies for handling tool failures and retries.\n\n## Multi-Agent Coordination\n\nComplex tasks often require multiple agents working together. Learn coordination strategies, communication protocols, and techniques for managing multi-agent workflows.",
          "codeExamples": [
            {
              "id": "code-4-2-1",
              "title": "Agent with Planning System",
              "language": "python",
              "code": "from typing import List, Dict, Any\nfrom langchain.agents import Tool, AgentExecutor\nfrom langchain_openai import ChatOpenAI\nfrom pydantic import BaseModel, Field\n\nclass TaskPlan(BaseModel):\n    \"\"\"Structured plan for task execution\"\"\"\n    goal: str = Field(description=\"The main goal to achieve\")\n    steps: List[str] = Field(description=\"Ordered list of steps\")\n    dependencies: Dict[str, List[str]] = Field(\n        default={}, \n        description=\"Step dependencies\"\n    )\n    \nclass PlanningAgent:\n    def __init__(self, llm=None):\n        self.llm = llm or ChatOpenAI(temperature=0)\n        self.current_plan = None\n        self.completed_steps = set()\n    \n    def create_plan(self, goal: str) -> TaskPlan:\n        \"\"\"Create a structured plan for achieving a goal\"\"\"\n        \n        prompt = f\"\"\"\nCreate a detailed plan to achieve this goal: {goal}\n\nReturn a structured plan with:\n1. Clear, actionable steps\n2. Dependencies between steps\n3. Success criteria\n\nFormat as JSON with keys: goal, steps, dependencies\n\"\"\"\n        \n        # Get structured output\n        response = self.llm.invoke(prompt)\n        \n        # Parse into TaskPlan (simplified for example)\n        plan = TaskPlan(\n            goal=goal,\n            steps=[\n                \"Gather requirements\",\n                \"Research solutions\",\n                \"Design architecture\", \n                \"Implement core features\",\n                \"Test and validate\",\n                \"Deploy to production\"\n            ],\n            dependencies={\n                \"Design architecture\": [\"Gather requirements\", \"Research solutions\"],\n                \"Implement core features\": [\"Design architecture\"],\n                \"Test and validate\": [\"Implement core features\"],\n                \"Deploy to production\": [\"Test and validate\"]\n            }\n        )\n        \n        self.current_plan = plan\n        return plan\n    \n    def get_next_steps(self) -> List[str]:\n        \"\"\"Get next executable steps based on dependencies\"\"\"\n        if not self.current_plan:\n            return []\n        \n        available_steps = []\n        for step in self.current_plan.steps:\n            if step in self.completed_steps:\n                continue\n                \n            # Check if dependencies are met\n            deps = self.current_plan.dependencies.get(step, [])\n            if all(dep in self.completed_steps for dep in deps):\n                available_steps.append(step)\n        \n        return available_steps\n    \n    def execute_step(self, step: str) -> str:\n        \"\"\"Execute a single step of the plan\"\"\"\n        print(f\"\ud83d\udd04 Executing: {step}\")\n        \n        # Simulate step execution\n        result = f\"Completed: {step}\"\n        \n        self.completed_steps.add(step)\n        \n        # Check if plan is complete\n        if len(self.completed_steps) == len(self.current_plan.steps):\n            result += \"\\n\u2705 Plan completed successfully!\"\n        \n        return result\n\n# Usage example\nagent = PlanningAgent()\nplan = agent.create_plan(\"Build a web scraping system\")\n\nprint(f\"\ud83d\udccb Plan created with {len(plan.steps)} steps\")\nprint(f\"\ud83c\udfaf Goal: {plan.goal}\")\n\n# Execute plan\nwhile True:\n    next_steps = agent.get_next_steps()\n    if not next_steps:\n        break\n    \n    for step in next_steps:\n        result = agent.execute_step(step)\n        print(f\"  \u2192 {result}\")"
            },
            {
              "id": "code-4-2-2",
              "title": "Multi-Agent Coordination System",
              "language": "python",
              "code": "from typing import Dict, List, Any, Optional\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport asyncio\nfrom queue import Queue\n\nclass AgentRole(Enum):\n    RESEARCHER = \"researcher\"\n    ANALYST = \"analyst\"\n    WRITER = \"writer\"\n    REVIEWER = \"reviewer\"\n\n@dataclass\nclass Message:\n    sender: str\n    recipient: str\n    content: Any\n    msg_type: str = \"task\"\n    \nclass Agent:\n    def __init__(self, name: str, role: AgentRole):\n        self.name = name\n        self.role = role\n        self.inbox = Queue()\n        self.knowledge = {}\n        \n    def receive_message(self, message: Message):\n        self.inbox.put(message)\n    \n    def process_messages(self):\n        results = []\n        while not self.inbox.empty():\n            msg = self.inbox.get()\n            result = self.handle_message(msg)\n            results.append(result)\n        return results\n    \n    def handle_message(self, message: Message) -> Dict:\n        \"\"\"Process message based on role\"\"\"\n        if self.role == AgentRole.RESEARCHER:\n            return self.research(message.content)\n        elif self.role == AgentRole.ANALYST:\n            return self.analyze(message.content)\n        elif self.role == AgentRole.WRITER:\n            return self.write(message.content)\n        elif self.role == AgentRole.REVIEWER:\n            return self.review(message.content)\n    \n    def research(self, query: str) -> Dict:\n        return {\n            \"agent\": self.name,\n            \"action\": \"research\",\n            \"result\": f\"Research findings on: {query}\"\n        }\n    \n    def analyze(self, data: Any) -> Dict:\n        return {\n            \"agent\": self.name,\n            \"action\": \"analyze\",\n            \"result\": f\"Analysis complete: {data}\"\n        }\n    \n    def write(self, content: Any) -> Dict:\n        return {\n            \"agent\": self.name,\n            \"action\": \"write\",\n            \"result\": f\"Document written: {content}\"\n        }\n    \n    def review(self, document: Any) -> Dict:\n        return {\n            \"agent\": self.name,\n            \"action\": \"review\",\n            \"result\": f\"Review complete: {document}\"\n        }\n\nclass MultiAgentOrchestrator:\n    def __init__(self):\n        self.agents: Dict[str, Agent] = {}\n        self.workflow_history = []\n        \n    def register_agent(self, agent: Agent):\n        self.agents[agent.name] = agent\n        print(f\"\u2705 Registered agent: {agent.name} ({agent.role.value})\")\n    \n    def send_message(self, sender: str, recipient: str, content: Any):\n        msg = Message(sender, recipient, content)\n        if recipient in self.agents:\n            self.agents[recipient].receive_message(msg)\n            self.workflow_history.append(msg)\n    \n    def broadcast(self, sender: str, content: Any, role_filter: Optional[AgentRole] = None):\n        \"\"\"Broadcast message to multiple agents\"\"\"\n        for name, agent in self.agents.items():\n            if role_filter and agent.role != role_filter:\n                continue\n            if name != sender:\n                self.send_message(sender, name, content)\n    \n    def execute_workflow(self, task: str) -> List[Dict]:\n        \"\"\"Execute a coordinated workflow\"\"\"\n        results = []\n        \n        # Step 1: Research phase\n        self.broadcast(\"orchestrator\", f\"Research: {task}\", AgentRole.RESEARCHER)\n        for agent in self.agents.values():\n            if agent.role == AgentRole.RESEARCHER:\n                results.extend(agent.process_messages())\n        \n        # Step 2: Analysis phase\n        research_data = [r[\"result\"] for r in results]\n        self.broadcast(\"orchestrator\", research_data, AgentRole.ANALYST)\n        for agent in self.agents.values():\n            if agent.role == AgentRole.ANALYST:\n                results.extend(agent.process_messages())\n        \n        # Step 3: Writing phase\n        analysis_data = [r[\"result\"] for r in results if r[\"action\"] == \"analyze\"]\n        self.broadcast(\"orchestrator\", analysis_data, AgentRole.WRITER)\n        for agent in self.agents.values():\n            if agent.role == AgentRole.WRITER:\n                results.extend(agent.process_messages())\n        \n        # Step 4: Review phase\n        written_content = [r[\"result\"] for r in results if r[\"action\"] == \"write\"]\n        self.broadcast(\"orchestrator\", written_content, AgentRole.REVIEWER)\n        for agent in self.agents.values():\n            if agent.role == AgentRole.REVIEWER:\n                results.extend(agent.process_messages())\n        \n        return results\n\n# Usage\norchestrator = MultiAgentOrchestrator()\n\n# Create specialized agents\norchestrator.register_agent(Agent(\"researcher_1\", AgentRole.RESEARCHER))\norchestrator.register_agent(Agent(\"analyst_1\", AgentRole.ANALYST))\norchestrator.register_agent(Agent(\"writer_1\", AgentRole.WRITER))\norchestrator.register_agent(Agent(\"reviewer_1\", AgentRole.REVIEWER))\n\n# Execute coordinated workflow\nresults = orchestrator.execute_workflow(\"Create report on AI trends\")\n\nprint(\"\\n\ud83d\udcca Workflow Results:\")\nfor result in results:\n    print(f\"  {result['agent']}: {result['action']} \u2192 {result['result'][:50]}...\")"
            },
            {
              "id": "code-4-2-3",
              "title": "Agent Memory and Learning System",
              "language": "python",
              "code": "import json\nfrom typing import Dict, List, Any, Optional\nfrom datetime import datetime\nfrom collections import defaultdict\n\nclass AgentMemory:\n    \"\"\"Sophisticated memory system for AI agents\"\"\"\n    \n    def __init__(self, capacity: int = 1000):\n        self.capacity = capacity\n        self.short_term = []  # Recent interactions\n        self.long_term = {}   # Persistent knowledge\n        self.episodic = []    # Specific experiences\n        self.semantic = defaultdict(list)  # Concept relationships\n        \n    def add_short_term(self, memory: Dict):\n        \"\"\"Add to short-term memory with recency management\"\"\"\n        memory['timestamp'] = datetime.now().isoformat()\n        self.short_term.append(memory)\n        \n        # Manage capacity\n        if len(self.short_term) > self.capacity:\n            # Move oldest to long-term if important\n            old_memory = self.short_term.pop(0)\n            if self._is_important(old_memory):\n                self._consolidate_to_long_term(old_memory)\n    \n    def _is_important(self, memory: Dict) -> bool:\n        \"\"\"Determine if memory should be consolidated\"\"\"\n        # Simple heuristic: memories with high confidence or multiple references\n        importance_score = memory.get('confidence', 0.5)\n        importance_score += len(memory.get('entities', [])) * 0.1\n        importance_score += memory.get('user_feedback', 0)\n        return importance_score > 0.7\n    \n    def _consolidate_to_long_term(self, memory: Dict):\n        \"\"\"Move important memories to long-term storage\"\"\"\n        key = memory.get('topic', 'general')\n        if key not in self.long_term:\n            self.long_term[key] = []\n        self.long_term[key].append(memory)\n        \n        # Extract semantic relationships\n        for entity in memory.get('entities', []):\n            self.semantic[entity].append(memory)\n    \n    def recall(self, query: str, context: Optional[Dict] = None) -> List[Dict]:\n        \"\"\"Retrieve relevant memories based on query and context\"\"\"\n        relevant_memories = []\n        \n        # Search short-term memory\n        for memory in self.short_term[-10:]:  # Recent 10\n            if query.lower() in str(memory).lower():\n                relevant_memories.append(memory)\n        \n        # Search long-term memory\n        for topic, memories in self.long_term.items():\n            if query.lower() in topic.lower():\n                relevant_memories.extend(memories[-5:])  # Top 5 from each topic\n        \n        # Search semantic memory\n        for word in query.split():\n            if word in self.semantic:\n                relevant_memories.extend(self.semantic[word][-3:])\n        \n        # Remove duplicates and sort by relevance\n        seen = set()\n        unique_memories = []\n        for mem in relevant_memories:\n            mem_str = str(mem)\n            if mem_str not in seen:\n                seen.add(mem_str)\n                unique_memories.append(mem)\n        \n        return unique_memories[:10]  # Return top 10 most relevant\n\nclass LearningAgent:\n    \"\"\"Agent that learns from interactions\"\"\"\n    \n    def __init__(self, name: str):\n        self.name = name\n        self.memory = AgentMemory()\n        self.skills = {}\n        self.performance_history = []\n        \n    def learn_from_feedback(self, task: str, feedback: Dict):\n        \"\"\"Learn from task execution feedback\"\"\"\n        memory_entry = {\n            'task': task,\n            'feedback': feedback,\n            'success': feedback.get('success', False),\n            'confidence': feedback.get('confidence', 0.5),\n            'entities': self._extract_entities(task)\n        }\n        \n        self.memory.add_short_term(memory_entry)\n        \n        # Update skills based on feedback\n        skill = feedback.get('skill_used', 'general')\n        if skill not in self.skills:\n            self.skills[skill] = {'successes': 0, 'failures': 0}\n        \n        if feedback.get('success'):\n            self.skills[skill]['successes'] += 1\n        else:\n            self.skills[skill]['failures'] += 1\n        \n        # Track performance\n        self.performance_history.append({\n            'timestamp': datetime.now().isoformat(),\n            'task': task,\n            'success': feedback.get('success', False)\n        })\n    \n    def _extract_entities(self, text: str) -> List[str]:\n        \"\"\"Simple entity extraction (would use NER in production)\"\"\"\n        # Mock entity extraction\n        important_words = [w for w in text.split() if len(w) > 4 and w[0].isupper()]\n        return important_words\n    \n    def get_confidence_for_task(self, task: str) -> float:\n        \"\"\"Calculate confidence for a given task based on history\"\"\"\n        relevant_memories = self.memory.recall(task)\n        \n        if not relevant_memories:\n            return 0.5  # Neutral confidence for unknown tasks\n        \n        # Calculate based on past performance\n        successes = sum(1 for m in relevant_memories if m.get('success'))\n        total = len(relevant_memories)\n        \n        confidence = successes / total if total > 0 else 0.5\n        \n        # Adjust based on recency\n        recent_memories = relevant_memories[-3:]\n        recent_success = sum(1 for m in recent_memories if m.get('success'))\n        recency_factor = recent_success / len(recent_memories) if recent_memories else 0.5\n        \n        # Weighted average\n        final_confidence = 0.7 * confidence + 0.3 * recency_factor\n        \n        return min(max(final_confidence, 0.0), 1.0)\n    \n    def suggest_approach(self, task: str) -> Dict:\n        \"\"\"Suggest approach based on learned experience\"\"\"\n        confidence = self.get_confidence_for_task(task)\n        relevant_memories = self.memory.recall(task)\n        \n        # Find successful approaches\n        successful_approaches = [\n            m for m in relevant_memories \n            if m.get('success') and 'approach' in m\n        ]\n        \n        if successful_approaches:\n            best_approach = successful_approaches[-1]['approach']\n            return {\n                'suggested_approach': best_approach,\n                'confidence': confidence,\n                'based_on': len(successful_approaches)\n            }\n        \n        return {\n            'suggested_approach': 'standard',\n            'confidence': confidence,\n            'based_on': 0\n        }\n\n# Usage example\nagent = LearningAgent(\"Learner-1\")\n\n# Simulate learning from multiple tasks\ntasks_and_feedback = [\n    (\"Analyze customer data\", {\"success\": True, \"confidence\": 0.9, \"skill_used\": \"analysis\"}),\n    (\"Generate report\", {\"success\": True, \"confidence\": 0.8, \"skill_used\": \"writing\"}),\n    (\"Analyze sales trends\", {\"success\": False, \"confidence\": 0.4, \"skill_used\": \"analysis\"}),\n    (\"Analyze customer data\", {\"success\": True, \"confidence\": 0.95, \"skill_used\": \"analysis\"}),\n]\n\nfor task, feedback in tasks_and_feedback:\n    agent.learn_from_feedback(task, feedback)\n    print(f\"Learned from: {task} (success: {feedback['success']})\")\n\n# Test learned knowledge\ntest_task = \"Analyze market data\"\nconfidence = agent.get_confidence_for_task(test_task)\napproach = agent.suggest_approach(test_task)\n\nprint(f\"\\n\ud83e\udde0 For task '{test_task}':\")\nprint(f\"  Confidence: {confidence:.2%}\")\nprint(f\"  Suggested approach: {approach}\")"
            }
          ],
          "checklist": [
            {
              "id": "check-4-2-1",
              "text": "Design AI agent architectures",
              "completed": false
            },
            {
              "id": "check-4-2-2",
              "text": "Implement planning and reasoning systems",
              "completed": false
            },
            {
              "id": "check-4-2-3",
              "text": "Build sophisticated memory systems for agents",
              "completed": false
            },
            {
              "id": "check-4-2-4",
              "text": "Create multi-agent coordination systems",
              "completed": false
            },
            {
              "id": "check-4-2-5",
              "text": "Implement inter-agent communication protocols",
              "completed": false
            },
            {
              "id": "check-4-2-6",
              "text": "Design agent learning mechanisms",
              "completed": false
            },
            {
              "id": "check-4-2-7",
              "text": "Build tool discovery and selection systems",
              "completed": false
            },
            {
              "id": "check-4-2-8",
              "text": "Debug complex multi-agent workflows",
              "completed": false
            },
            {
              "id": "check-4-2-9",
              "text": "Implement agent performance tracking",
              "completed": false
            },
            {
              "id": "check-4-2-10",
              "text": "Create agent orchestration strategies",
              "completed": false
            }
          ],
          "timestamps": [
            {
              "time": "0:00:12",
              "label": "Office Hours Welcome",
              "description": "Welcome to AI track office hours"
            },
            {
              "time": "0:02:23",
              "label": "Database Agent Implementation",
              "description": "Building an agent that interacts with databases"
            },
            {
              "time": "0:05:30",
              "label": "CSV Data Processing",
              "description": "Agent for processing CSV files with data"
            },
            {
              "time": "0:08:45",
              "label": "Tool Creation for Agents",
              "description": "Creating custom tools for agent capabilities"
            },
            {
              "time": "0:13:09",
              "label": "SQL Query Generation",
              "description": "Agent generating and executing SQL queries"
            },
            {
              "time": "0:18:25",
              "label": "Error Handling in Agents",
              "description": "Managing agent failures and recovery"
            },
            {
              "time": "0:23:40",
              "label": "Multi-Tool Agent Setup",
              "description": "Configuring agents with multiple tools"
            },
            {
              "time": "0:28:15",
              "label": "Agent Memory Systems",
              "description": "Implementing short and long-term memory"
            },
            {
              "time": "0:33:50",
              "label": "Agent Planning Logic",
              "description": "How agents plan and execute tasks"
            },
            {
              "time": "0:39:20",
              "label": "Debugging Agent Behavior",
              "description": "Learn architecture and sequence implementation techniques"
            },
            {
              "time": "0:44:35",
              "label": "Student Project Support",
              "description": "Helping students with their agent implementations"
            },
            {
              "time": "0:49:10",
              "label": "Agent Best Practices",
              "description": "Tips for building robust agents"
            },
            {
              "time": "0:54:25",
              "label": "Advanced Agent Patterns",
              "description": "Complex agent architectures and patterns"
            },
            {
              "time": "0:58:45",
              "label": "Performance Considerations",
              "description": "Learn architecture and sequence implementation techniques"
            },
            {
              "time": "1:02:30",
              "label": "Agent Tools",
              "description": "Final questions and next steps"
            }
          ],
          "videoPath": "cohort_2/week_4_class_2_2024-06-11.mp4"
        },
        {
          "id": "lesson-4-3",
          "title": "Complex Workflow Implementation",
          "duration": "1:25:00",
          "videoUrl": "/videos/cohort_2/week_04/week_4_class_3_2024-06-12.mp4",
          "videoId": "week_4_class_3_2024-06-12",
          "content": "# Complex Workflow Implementation\n\n## Overview\n\nThe culmination of our agent and workflow learning, this lesson focuses on implementing complex, production-ready workflows. We cover state machines, orchestration, error recovery, and deployment strategies.\n\n## Workflow Design Principles\n\nDesigning robust workflows requires careful consideration of state management, error handling, and scalability. Learn principles for creating maintainable and reliable workflow systems.\n\n## State Machines and Transitions\n\nState machines provide a formal model for workflow execution. We implement state machines with clear transitions, guards, and actions, ensuring predictable workflow behavior.\n\n## Orchestration Strategies\n\nOrchestrating complex workflows involves coordinating multiple components, managing dependencies, and handling concurrent execution. Learn patterns for effective workflow orchestration.\n\n## Error Recovery and Resilience\n\nProduction workflows must handle failures gracefully. We cover retry strategies, compensation logic, circuit breakers, and techniques for building resilient systems.\n\n## Monitoring and Observability\n\nVisibility into workflow execution is crucial for debugging and optimization. Learn to implement comprehensive monitoring, logging, and tracing for complex workflows.\n\n## Production Deployment\n\nDeploying workflows to production requires consideration of scaling, versioning, and maintenance. We cover deployment strategies, blue-green deployments, and techniques for zero-downtime updates.",
          "codeExamples": [
            {
              "id": "code-4-3-1",
              "title": "State Machine Workflow",
              "language": "python",
              "code": "from enum import Enum\nfrom typing import Dict, Any, Optional, Callable\nfrom dataclasses import dataclass\nimport json\n\nclass WorkflowState(Enum):\n    IDLE = \"idle\"\n    PROCESSING = \"processing\"\n    WAITING_APPROVAL = \"waiting_approval\"\n    APPROVED = \"approved\"\n    REJECTED = \"rejected\"\n    COMPLETED = \"completed\"\n    ERROR = \"error\"\n\n@dataclass\nclass StateTransition:\n    from_state: WorkflowState\n    to_state: WorkflowState\n    condition: Optional[Callable] = None\n    action: Optional[Callable] = None\n\nclass WorkflowStateMachine:\n    def __init__(self, initial_state: WorkflowState = WorkflowState.IDLE):\n        self.current_state = initial_state\n        self.context = {}\n        self.transitions: Dict[WorkflowState, List[StateTransition]] = {}\n        self.state_history = [(initial_state, None)]\n        \n    def add_transition(self, transition: StateTransition):\n        \"\"\"Add a state transition rule\"\"\"\n        if transition.from_state not in self.transitions:\n            self.transitions[transition.from_state] = []\n        self.transitions[transition.from_state].append(transition)\n    \n    def can_transition_to(self, target_state: WorkflowState) -> bool:\n        \"\"\"Check if transition to target state is valid\"\"\"\n        if self.current_state not in self.transitions:\n            return False\n        \n        for transition in self.transitions[self.current_state]:\n            if transition.to_state == target_state:\n                if transition.condition:\n                    return transition.condition(self.context)\n                return True\n        return False\n    \n    def transition_to(self, target_state: WorkflowState, data: Optional[Dict] = None):\n        \"\"\"Execute state transition\"\"\"\n        if not self.can_transition_to(target_state):\n            raise ValueError(f\"Invalid transition from {self.current_state} to {target_state}\")\n        \n        # Find and execute transition\n        for transition in self.transitions[self.current_state]:\n            if transition.to_state == target_state:\n                # Execute transition action if defined\n                if transition.action:\n                    transition.action(self.context, data)\n                \n                # Update state\n                old_state = self.current_state\n                self.current_state = target_state\n                self.state_history.append((target_state, data))\n                \n                print(f\"\ud83d\udd04 Transitioned: {old_state.value} \u2192 {target_state.value}\")\n                return\n    \n    def get_available_transitions(self) -> List[WorkflowState]:\n        \"\"\"Get list of valid next states\"\"\"\n        if self.current_state not in self.transitions:\n            return []\n        \n        available = []\n        for transition in self.transitions[self.current_state]:\n            if transition.condition is None or transition.condition(self.context):\n                available.append(transition.to_state)\n        \n        return available\n\n# Example: Document Approval Workflow\nclass DocumentWorkflow(WorkflowStateMachine):\n    def __init__(self):\n        super().__init__(WorkflowState.IDLE)\n        self._setup_transitions()\n    \n    def _setup_transitions(self):\n        # Define transition rules\n        self.add_transition(StateTransition(\n            WorkflowState.IDLE,\n            WorkflowState.PROCESSING,\n            condition=lambda ctx: 'document' in ctx,\n            action=lambda ctx, data: ctx.update({'start_time': 'now'})\n        ))\n        \n        self.add_transition(StateTransition(\n            WorkflowState.PROCESSING,\n            WorkflowState.WAITING_APPROVAL,\n            condition=lambda ctx: ctx.get('processed', False),\n            action=lambda ctx, data: ctx.update({'awaiting_user': data.get('approver')}\n        ))\n        \n        self.add_transition(StateTransition(\n            WorkflowState.WAITING_APPROVAL,\n            WorkflowState.APPROVED,\n            condition=lambda ctx: ctx.get('approval_received', False)\n        ))\n        \n        self.add_transition(StateTransition(\n            WorkflowState.WAITING_APPROVAL,\n            WorkflowState.REJECTED,\n            condition=lambda ctx: ctx.get('rejection_received', False)\n        ))\n        \n        self.add_transition(StateTransition(\n            WorkflowState.APPROVED,\n            WorkflowState.COMPLETED,\n            action=lambda ctx, data: ctx.update({'completion_time': 'now'})\n        ))\n        \n        self.add_transition(StateTransition(\n            WorkflowState.REJECTED,\n            WorkflowState.PROCESSING,\n            action=lambda ctx, data: ctx.update({'revision_count': ctx.get('revision_count', 0) + 1})\n        ))\n        \n        # Error transitions from any state\n        for state in WorkflowState:\n            if state != WorkflowState.ERROR:\n                self.add_transition(StateTransition(\n                    state,\n                    WorkflowState.ERROR,\n                    condition=lambda ctx: ctx.get('error', False)\n                ))\n    \n    def process_document(self, document: Dict):\n        \"\"\"Process a document through the workflow\"\"\"\n        self.context['document'] = document\n        self.transition_to(WorkflowState.PROCESSING)\n        \n        # Simulate processing\n        self.context['processed'] = True\n        self.transition_to(WorkflowState.WAITING_APPROVAL, {'approver': 'manager'})\n        \n        return self.current_state\n    \n    def approve(self):\n        \"\"\"Approve the document\"\"\"\n        self.context['approval_received'] = True\n        self.transition_to(WorkflowState.APPROVED)\n        self.transition_to(WorkflowState.COMPLETED)\n    \n    def reject(self, reason: str):\n        \"\"\"Reject the document\"\"\"\n        self.context['rejection_received'] = True\n        self.context['rejection_reason'] = reason\n        self.transition_to(WorkflowState.REJECTED)\n\n# Usage\nworkflow = DocumentWorkflow()\nprint(f\"Initial state: {workflow.current_state.value}\")\nprint(f\"Available transitions: {[s.value for s in workflow.get_available_transitions()]}\")\n\n# Process document\nworkflow.process_document({'title': 'Report', 'content': '...'})\nprint(f\"\\nCurrent state: {workflow.current_state.value}\")\n\n# Approve document\nworkflow.approve()\nprint(f\"Final state: {workflow.current_state.value}\")\nprint(f\"\\nWorkflow history: {[(s.value, d) for s, d in workflow.state_history]}\")"
            },
            {
              "id": "code-4-3-2",
              "title": "Error Recovery and Retry Logic",
              "language": "python",
              "code": "import time\nimport random\nfrom typing import Callable, Any, Optional, Dict\nfrom functools import wraps\nfrom dataclasses import dataclass\nfrom enum import Enum\n\nclass RetryStrategy(Enum):\n    EXPONENTIAL = \"exponential\"\n    LINEAR = \"linear\"\n    FIXED = \"fixed\"\n\n@dataclass\nclass RetryConfig:\n    max_attempts: int = 3\n    initial_delay: float = 1.0\n    max_delay: float = 60.0\n    strategy: RetryStrategy = RetryStrategy.EXPONENTIAL\n    jitter: bool = True\n    exceptions: tuple = (Exception,)\n\ndef calculate_delay(attempt: int, config: RetryConfig) -> float:\n    \"\"\"Calculate delay based on retry strategy\"\"\"\n    if config.strategy == RetryStrategy.EXPONENTIAL:\n        delay = config.initial_delay * (2 ** attempt)\n    elif config.strategy == RetryStrategy.LINEAR:\n        delay = config.initial_delay * attempt\n    else:  # FIXED\n        delay = config.initial_delay\n    \n    # Apply jitter to avoid thundering herd\n    if config.jitter:\n        delay = delay * (0.5 + random.random())\n    \n    return min(delay, config.max_delay)\n\ndef retry_with_backoff(config: Optional[RetryConfig] = None):\n    \"\"\"Decorator for automatic retry with backoff\"\"\"\n    if config is None:\n        config = RetryConfig()\n    \n    def decorator(func: Callable) -> Callable:\n        @wraps(func)\n        def wrapper(*args, **kwargs) -> Any:\n            last_exception = None\n            \n            for attempt in range(config.max_attempts):\n                try:\n                    return func(*args, **kwargs)\n                except config.exceptions as e:\n                    last_exception = e\n                    \n                    if attempt < config.max_attempts - 1:\n                        delay = calculate_delay(attempt, config)\n                        print(f\"\u26a0\ufe0f Attempt {attempt + 1} failed: {str(e)}\")\n                        print(f\"\u23f0 Retrying in {delay:.2f} seconds...\")\n                        time.sleep(delay)\n                    else:\n                        print(f\"\u274c All {config.max_attempts} attempts failed\")\n            \n            raise last_exception\n        \n        return wrapper\n    return decorator\n\nclass CircuitBreaker:\n    \"\"\"Circuit breaker pattern for fault tolerance\"\"\"\n    \n    def __init__(self, failure_threshold: int = 5, recovery_timeout: float = 60.0):\n        self.failure_threshold = failure_threshold\n        self.recovery_timeout = recovery_timeout\n        self.failure_count = 0\n        self.last_failure_time = None\n        self.state = \"closed\"  # closed, open, half-open\n    \n    def __call__(self, func: Callable) -> Callable:\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            if self.state == \"open\":\n                if time.time() - self.last_failure_time > self.recovery_timeout:\n                    self.state = \"half-open\"\n                    print(\"\ud83d\udd04 Circuit breaker: Attempting recovery (half-open)\")\n                else:\n                    raise Exception(\"Circuit breaker is open\")\n            \n            try:\n                result = func(*args, **kwargs)\n                \n                if self.state == \"half-open\":\n                    self.state = \"closed\"\n                    self.failure_count = 0\n                    print(\"\u2705 Circuit breaker: Recovered (closed)\")\n                \n                return result\n                \n            except Exception as e:\n                self.failure_count += 1\n                self.last_failure_time = time.time()\n                \n                if self.failure_count >= self.failure_threshold:\n                    self.state = \"open\"\n                    print(f\"\ud83d\udd34 Circuit breaker: Open (failures: {self.failure_count})\")\n                \n                raise e\n        \n        return wrapper\n\nclass CompensationManager:\n    \"\"\"Manage compensation actions for failed transactions\"\"\"\n    \n    def __init__(self):\n        self.compensation_stack = []\n    \n    def add_compensation(self, action: Callable, *args, **kwargs):\n        \"\"\"Add a compensation action to the stack\"\"\"\n        self.compensation_stack.append((action, args, kwargs))\n    \n    def compensate(self):\n        \"\"\"Execute all compensation actions in reverse order\"\"\"\n        print(\"\ud83d\udd04 Starting compensation...\")\n        \n        while self.compensation_stack:\n            action, args, kwargs = self.compensation_stack.pop()\n            try:\n                action(*args, **kwargs)\n                print(f\"\u2705 Compensated: {action.__name__}\")\n            except Exception as e:\n                print(f\"\u26a0\ufe0f Compensation failed: {action.__name__} - {str(e)}\")\n\n# Example usage\n@retry_with_backoff(RetryConfig(max_attempts=3, strategy=RetryStrategy.EXPONENTIAL))\ndef unreliable_api_call(success_rate: float = 0.3):\n    \"\"\"Simulated unreliable API call\"\"\"\n    if random.random() > success_rate:\n        raise Exception(\"API call failed\")\n    return \"Success!\"\n\n@CircuitBreaker(failure_threshold=3, recovery_timeout=10)\ndef protected_service_call():\n    \"\"\"Service call protected by circuit breaker\"\"\"\n    if random.random() > 0.5:\n        raise Exception(\"Service unavailable\")\n    return \"Service response\"\n\ndef complex_workflow_with_compensation():\n    \"\"\"Workflow with compensation for failures\"\"\"\n    comp_manager = CompensationManager()\n    \n    try:\n        # Step 1: Create record\n        record_id = \"123\"\n        print(f\"Creating record {record_id}\")\n        comp_manager.add_compensation(\n            lambda rid: print(f\"Deleting record {rid}\"),\n            record_id\n        )\n        \n        # Step 2: Process payment\n        payment_id = \"pay_456\"\n        print(f\"Processing payment {payment_id}\")\n        comp_manager.add_compensation(\n            lambda pid: print(f\"Refunding payment {pid}\"),\n            payment_id\n        )\n        \n        # Step 3: Send notification (this might fail)\n        if random.random() > 0.5:\n            raise Exception(\"Notification service down\")\n        print(\"Notification sent\")\n        \n        print(\"\u2705 Workflow completed successfully\")\n        \n    except Exception as e:\n        print(f\"\u274c Workflow failed: {str(e)}\")\n        comp_manager.compensate()\n\n# Test the patterns\nprint(\"=== Testing Retry with Backoff ===\")\ntry:\n    result = unreliable_api_call(success_rate=0.7)\n    print(f\"Result: {result}\")\nexcept Exception as e:\n    print(f\"Failed after retries: {e}\")\n\nprint(\"\\n=== Testing Circuit Breaker ===\")\nfor i in range(10):\n    try:\n        result = protected_service_call()\n        print(f\"Call {i+1}: {result}\")\n    except Exception as e:\n        print(f\"Call {i+1}: {e}\")\n    time.sleep(1)\n\nprint(\"\\n=== Testing Compensation ===\")\ncomplex_workflow_with_compensation()"
            },
            {
              "id": "code-4-3-3",
              "title": "Production Monitoring and Observability",
              "language": "python",
              "code": "import time\nimport json\nimport logging\nfrom typing import Dict, Any, Optional, List\nfrom dataclasses import dataclass, asdict\nfrom datetime import datetime\nfrom contextlib import contextmanager\nimport traceback\n\n# Configure structured logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\n\n@dataclass\nclass MetricEvent:\n    name: str\n    value: float\n    unit: str\n    timestamp: str\n    tags: Dict[str, str]\n\n@dataclass  \nclass TraceSpan:\n    span_id: str\n    parent_id: Optional[str]\n    operation: str\n    start_time: float\n    end_time: Optional[float] = None\n    tags: Dict[str, Any] = None\n    events: List[Dict] = None\n    status: str = \"in_progress\"\n\nclass WorkflowMonitor:\n    \"\"\"Comprehensive monitoring for workflows\"\"\"\n    \n    def __init__(self, workflow_name: str):\n        self.workflow_name = workflow_name\n        self.logger = logging.getLogger(workflow_name)\n        self.metrics: List[MetricEvent] = []\n        self.traces: Dict[str, TraceSpan] = {}\n        self.current_span_id = None\n        \n    def emit_metric(self, name: str, value: float, unit: str = \"count\", **tags):\n        \"\"\"Emit a metric event\"\"\"\n        metric = MetricEvent(\n            name=f\"{self.workflow_name}.{name}\",\n            value=value,\n            unit=unit,\n            timestamp=datetime.now().isoformat(),\n            tags=tags\n        )\n        self.metrics.append(metric)\n        self.logger.info(f\"\ud83d\udcca Metric: {json.dumps(asdict(metric))}\")\n    \n    @contextmanager\n    def trace_span(self, operation: str, **tags):\n        \"\"\"Create a trace span for an operation\"\"\"\n        span_id = f\"{operation}_{int(time.time() * 1000)}\"\n        parent_id = self.current_span_id\n        \n        span = TraceSpan(\n            span_id=span_id,\n            parent_id=parent_id,\n            operation=operation,\n            start_time=time.time(),\n            tags=tags,\n            events=[]\n        )\n        \n        self.traces[span_id] = span\n        old_span_id = self.current_span_id\n        self.current_span_id = span_id\n        \n        try:\n            self.logger.info(f\"\ud83d\udd0d Span started: {operation}\")\n            yield span\n            span.status = \"success\"\n            span.end_time = time.time()\n            duration = span.end_time - span.start_time\n            self.emit_metric(f\"{operation}.duration\", duration, \"seconds\")\n            \n        except Exception as e:\n            span.status = \"error\"\n            span.end_time = time.time()\n            span.events.append({\n                \"type\": \"error\",\n                \"message\": str(e),\n                \"traceback\": traceback.format_exc()\n            })\n            self.emit_metric(f\"{operation}.error\", 1, \"count\", error_type=type(e).__name__)\n            raise\n            \n        finally:\n            self.current_span_id = old_span_id\n            self.logger.info(f\"\ud83d\udd0d Span ended: {operation} ({span.status}, {span.end_time - span.start_time:.2f}s)\")\n    \n    def add_event(self, event_type: str, message: str, **metadata):\n        \"\"\"Add an event to the current span\"\"\"\n        if self.current_span_id and self.current_span_id in self.traces:\n            event = {\n                \"type\": event_type,\n                \"message\": message,\n                \"timestamp\": datetime.now().isoformat(),\n                **metadata\n            }\n            self.traces[self.current_span_id].events.append(event)\n            self.logger.info(f\"\ud83d\udcdd Event: {event_type} - {message}\")\n\nclass HealthChecker:\n    \"\"\"Health checking for workflow components\"\"\"\n    \n    def __init__(self):\n        self.checks: Dict[str, Callable] = {}\n        self.last_check_results: Dict[str, Dict] = {}\n        \n    def register_check(self, name: str, check_func: Callable):\n        \"\"\"Register a health check\"\"\"\n        self.checks[name] = check_func\n    \n    def run_checks(self) -> Dict[str, Dict]:\n        \"\"\"Run all registered health checks\"\"\"\n        results = {}\n        \n        for name, check_func in self.checks.items():\n            try:\n                start_time = time.time()\n                check_result = check_func()\n                duration = time.time() - start_time\n                \n                results[name] = {\n                    \"status\": \"healthy\" if check_result else \"unhealthy\",\n                    \"duration\": duration,\n                    \"timestamp\": datetime.now().isoformat(),\n                    \"details\": check_result\n                }\n                \n            except Exception as e:\n                results[name] = {\n                    \"status\": \"unhealthy\",\n                    \"error\": str(e),\n                    \"timestamp\": datetime.now().isoformat()\n                }\n        \n        self.last_check_results = results\n        return results\n    \n    def get_overall_health(self) -> str:\n        \"\"\"Get overall system health\"\"\"\n        if not self.last_check_results:\n            return \"unknown\"\n        \n        unhealthy = [k for k, v in self.last_check_results.items() \n                    if v[\"status\"] == \"unhealthy\"]\n        \n        if not unhealthy:\n            return \"healthy\"\n        elif len(unhealthy) < len(self.last_check_results) / 2:\n            return \"degraded\"\n        else:\n            return \"unhealthy\"\n\n# Example monitored workflow\ndef monitored_data_pipeline():\n    \"\"\"Example of a fully monitored data pipeline\"\"\"\n    monitor = WorkflowMonitor(\"data_pipeline\")\n    \n    try:\n        # Stage 1: Data extraction\n        with monitor.trace_span(\"extract_data\", source=\"database\"):\n            monitor.add_event(\"info\", \"Connecting to database\")\n            time.sleep(0.5)  # Simulate work\n            \n            records_extracted = 1000\n            monitor.emit_metric(\"records.extracted\", records_extracted)\n            monitor.add_event(\"info\", f\"Extracted {records_extracted} records\")\n        \n        # Stage 2: Data transformation  \n        with monitor.trace_span(\"transform_data\"):\n            monitor.add_event(\"info\", \"Starting transformation\")\n            time.sleep(0.3)  # Simulate work\n            \n            records_transformed = 950\n            monitor.emit_metric(\"records.transformed\", records_transformed)\n            monitor.add_event(\"info\", f\"Transformed {records_transformed} records\")\n            \n            # Simulate some records being filtered\n            records_filtered = 50\n            monitor.emit_metric(\"records.filtered\", records_filtered)\n        \n        # Stage 3: Data loading\n        with monitor.trace_span(\"load_data\", destination=\"warehouse\"):\n            monitor.add_event(\"info\", \"Loading to data warehouse\")\n            time.sleep(0.4)  # Simulate work\n            \n            records_loaded = 950\n            monitor.emit_metric(\"records.loaded\", records_loaded)\n            monitor.add_event(\"info\", f\"Loaded {records_loaded} records\")\n        \n        # Overall metrics\n        monitor.emit_metric(\"pipeline.success\", 1)\n        print(\"\u2705 Pipeline completed successfully\")\n        \n    except Exception as e:\n        monitor.emit_metric(\"pipeline.failure\", 1)\n        print(f\"\u274c Pipeline failed: {str(e)}\")\n        raise\n    \n    finally:\n        # Generate summary report\n        print(\"\\n\ud83d\udcc8 Pipeline Metrics Summary:\")\n        for metric in monitor.metrics:\n            print(f\"  {metric.name}: {metric.value} {metric.unit}\")\n        \n        print(\"\\n\ud83d\udd0d Trace Summary:\")\n        for span_id, span in monitor.traces.items():\n            if span.end_time:\n                duration = span.end_time - span.start_time\n                print(f\"  {span.operation}: {duration:.2f}s ({span.status})\")\n\n# Setup health checks\nhealth_checker = HealthChecker()\n\nhealth_checker.register_check(\n    \"database\",\n    lambda: {\"connected\": True, \"latency_ms\": random.randint(10, 50)}\n)\n\nhealth_checker.register_check(\n    \"api\",\n    lambda: {\"status\": \"online\", \"response_time_ms\": random.randint(50, 200)}\n)\n\nhealth_checker.register_check(\n    \"cache\",\n    lambda: {\"hit_rate\": 0.85, \"memory_used_mb\": 256}\n)\n\n# Run monitored workflow\nmonitored_data_pipeline()\n\n# Check system health\nprint(\"\\n\ud83c\udfe5 System Health Check:\")\nhealth_results = health_checker.run_checks()\nfor component, result in health_results.items():\n    print(f\"  {component}: {result['status']}\")\nprint(f\"\\nOverall health: {health_checker.get_overall_health()}\")"
            }
          ],
          "checklist": [
            {
              "id": "check-4-3-1",
              "text": "Design complex workflow state machines",
              "completed": false
            },
            {
              "id": "check-4-3-2",
              "text": "Implement error recovery and retry logic",
              "completed": false
            },
            {
              "id": "check-4-3-3",
              "text": "Build circuit breakers for fault tolerance",
              "completed": false
            },
            {
              "id": "check-4-3-4",
              "text": "Create compensation mechanisms for failures",
              "completed": false
            },
            {
              "id": "check-4-3-5",
              "text": "Implement comprehensive monitoring systems",
              "completed": false
            },
            {
              "id": "check-4-3-6",
              "text": "Build distributed tracing for workflows",
              "completed": false
            },
            {
              "id": "check-4-3-7",
              "text": "Design health checking systems",
              "completed": false
            },
            {
              "id": "check-4-3-8",
              "text": "Create production deployment strategies",
              "completed": false
            },
            {
              "id": "check-4-3-9",
              "text": "Implement zero-downtime updates",
              "completed": false
            },
            {
              "id": "check-4-3-10",
              "text": "Test workflows under various failure scenarios",
              "completed": false
            }
          ],
          "timestamps": [
            {
              "time": "0:00:07",
              "label": "Course Introduction",
              "description": "Introduction to complex workflow implementation"
            },
            {
              "time": "0:00:39",
              "label": "Project Guidance",
              "description": "Supporting students with coursework projects"
            },
            {
              "time": "0:02:29",
              "label": "AI Application Architecture",
              "description": "Learn implementation and strategies implementation techniques"
            },
            {
              "time": "0:06:31",
              "label": "End-to-End Workflows",
              "description": "Complete workflow from input to output"
            },
            {
              "time": "0:11:15",
              "label": "Design to Implementation",
              "description": "Converting design files to working code"
            },
            {
              "time": "0:15:21",
              "label": "State Management",
              "description": "Managing state in complex workflows"
            },
            {
              "time": "0:20:45",
              "label": "Workflow Orchestration",
              "description": "Learn implementation and strategies implementation techniques"
            },
            {
              "time": "0:26:30",
              "label": "Error Recovery Patterns",
              "description": "Learn implementation and strategies implementation techniques"
            },
            {
              "time": "0:32:15",
              "label": "Retry and Compensation",
              "description": "Retry logic and compensation strategies"
            },
            {
              "time": "0:38:40",
              "label": "Monitoring Workflows",
              "description": "Learn implementation and strategies implementation techniques"
            },
            {
              "time": "0:44:25",
              "label": "Production Deployment",
              "description": "Deploy implementation and strategies to cloud infrastructure"
            },
            {
              "time": "0:50:10",
              "label": "Scaling Considerations",
              "description": "Scaling workflows for high load"
            },
            {
              "time": "0:55:19",
              "label": "Performance Optimization",
              "description": "Learn implementation and strategies implementation techniques"
            },
            {
              "time": "1:01:45",
              "label": "Real-world Examples",
              "description": "Case studies and practical examples"
            },
            {
              "time": "1:08:20",
              "label": "Final Q&A",
              "description": "Addressing remaining questions and wrap-up"
            }
          ],
          "videoPath": "cohort_2/week_4_class_3_2024-06-12.mp4"
        }
      ]
    },
    {
      "id": "week-5",
      "title": "Week 5: Fine-Tuning and Model Customization",
      "description": "Master fine-tuning techniques for customizing LLMs, from OpenAI's API to open-source models like LLAMA.",
      "lessons": [
        {
          "id": "lesson-5-1",
          "title": "Fine-Tuning Fundamentals",
          "duration": "1:23:14",
          "videoUrl": "/videos/cohort_2/week_05/week_5_class_1_2024-06-17.mp4",
          "videoId": "week_5_class_1_2024-06-17",
          "content": "# Fine-Tuning Fundamentals\n\n## Overview\n\nThis lesson introduces fine-tuning as a powerful technique for customizing large language models to specific tasks. We explore when and why to use fine-tuning versus prompt engineering, with a practical focus on generating unit tests for code.\n\n## What is Fine-Tuning?\n\nFine-tuning adjusts the weights of a pre-trained large language model to improve its performance on specific tasks. Unlike prompt engineering, which works with the model as-is, fine-tuning creates a specialized version of the model optimized for your use case.\n\n## The Fine-Tuning Process\n\n1. **Model Selection**: Choose an appropriate base model (GPT-3.5, GPT-4, or open-source alternatives)\n2. **Data Preparation**: Gather and format training examples in JSONL format\n3. **Training**: Upload data and run the fine-tuning process\n4. **Validation**: Test the fine-tuned model against the base model\n5. **Deployment**: Use the customized model in production\n\n## When to Use Fine-Tuning\n\nFine-tuning is beneficial when:\n- You need consistent structured outputs\n- Cost reduction through shorter prompts\n- Domain-specific knowledge is required\n- Response latency needs improvement\n- Privacy or compliance requires on-premise models\n\n## Practical Application: Unit Test Generation\n\nThe lesson demonstrates fine-tuning a model to generate unit tests from function signatures and docstrings. This showcases how fine-tuning can create specialized tools for developers that understand coding patterns and testing best practices.\n\n## Cost-Benefit Analysis\n\nFine-tuning involves upfront costs but can reduce per-request costs significantly by eliminating the need for lengthy prompts and few-shot examples. The break-even point depends on usage volume and prompt complexity.",
          "codeExamples": [
            {
              "id": "code-5-1-1",
              "title": "Preparing Fine-Tuning Data",
              "language": "python",
              "code": "import json\nimport openai\n\n# Format training data for fine-tuning\ndef prepare_training_data(examples):\n    \"\"\"Convert examples to JSONL format for OpenAI fine-tuning\"\"\"\n    \n    training_data = []\n    \n    for example in examples:\n        # Format for chat completion fine-tuning\n        training_example = {\n            \"messages\": [\n                {\n                    \"role\": \"system\",\n                    \"content\": \"You are a unit test generator for Python functions.\"\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Generate unit tests for:\\n{example['function']}\"\n                },\n                {\n                    \"role\": \"assistant\",\n                    \"content\": example['unit_tests']\n                }\n            ]\n        }\n        training_data.append(training_example)\n    \n    # Save to JSONL file\n    with open('training_data.jsonl', 'w') as f:\n        for item in training_data:\n            f.write(json.dumps(item) + '\\n')\n    \n    return len(training_data)\n\n# Example usage\nexamples = [\n    {\n        \"function\": \"def add(a, b):\\n    return a + b\",\n        \"unit_tests\": \"import unittest\\n\\nclass TestAdd(unittest.TestCase):\\n    def test_positive_numbers(self):\\n        self.assertEqual(add(2, 3), 5)\\n    \\n    def test_negative_numbers(self):\\n        self.assertEqual(add(-1, -1), -2)\\n    \\n    def test_zero(self):\\n        self.assertEqual(add(0, 5), 5)\"\n    }\n]\n\ncount = prepare_training_data(examples)\nprint(f\"Prepared {count} training examples\")"
            },
            {
              "id": "code-5-1-2",
              "title": "Fine-Tuning with OpenAI API",
              "language": "python",
              "code": "import openai\nimport time\n\nclass FineTuningManager:\n    def __init__(self, api_key):\n        openai.api_key = api_key\n        \n    def upload_training_file(self, file_path):\n        \"\"\"Upload training data file\"\"\"\n        with open(file_path, 'rb') as f:\n            response = openai.File.create(\n                file=f,\n                purpose='fine-tune'\n            )\n        return response['id']\n    \n    def create_fine_tuning_job(self, training_file_id, model='gpt-3.5-turbo'):\n        \"\"\"Create a fine-tuning job\"\"\"\n        response = openai.FineTuningJob.create(\n            training_file=training_file_id,\n            model=model,\n            hyperparameters={\n                \"n_epochs\": 3,\n                \"batch_size\": 4,\n                \"learning_rate_multiplier\": 0.1\n            }\n        )\n        return response['id']\n    \n    def monitor_job(self, job_id):\n        \"\"\"Monitor fine-tuning job progress\"\"\"\n        while True:\n            job = openai.FineTuningJob.retrieve(job_id)\n            print(f\"Status: {job['status']}\")\n            \n            if job['status'] in ['succeeded', 'failed']:\n                return job\n            \n            # List recent events\n            events = openai.FineTuningJob.list_events(\n                id=job_id,\n                limit=5\n            )\n            \n            for event in events['data']:\n                print(f\"  {event['created_at']}: {event['message']}\")\n            \n            time.sleep(30)\n    \n    def test_fine_tuned_model(self, model_id, prompt):\n        \"\"\"Test the fine-tuned model\"\"\"\n        response = openai.ChatCompletion.create(\n            model=model_id,\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a unit test generator.\"},\n                {\"role\": \"user\", \"content\": prompt}\n            ],\n            temperature=0.3\n        )\n        return response.choices[0].message['content']\n\n# Usage example\nmanager = FineTuningManager('your-api-key')\n\n# Upload and fine-tune\nfile_id = manager.upload_training_file('training_data.jsonl')\njob_id = manager.create_fine_tuning_job(file_id)\n\n# Monitor progress\nresult = manager.monitor_job(job_id)\n\nif result['status'] == 'succeeded':\n    model_id = result['fine_tuned_model']\n    \n    # Test the model\n    test_function = \"\"\"\ndef multiply(x, y):\n    '''Multiply two numbers'''\n    return x * y\n\"\"\"\n    \n    tests = manager.test_fine_tuned_model(model_id, test_function)\n    print(f\"Generated tests:\\n{tests}\")"
            },
            {
              "id": "code-5-1-3",
              "title": "Validating Fine-Tuned Models",
              "language": "python",
              "code": "import openai\nfrom typing import List, Dict\nimport statistics\n\nclass ModelValidator:\n    def __init__(self, base_model: str, fine_tuned_model: str):\n        self.base_model = base_model\n        self.fine_tuned_model = fine_tuned_model\n        self.results = []\n    \n    def compare_models(self, test_cases: List[Dict]):\n        \"\"\"Compare base and fine-tuned model performance\"\"\"\n        \n        for test in test_cases:\n            prompt = test['prompt']\n            expected = test['expected_output']\n            \n            # Test base model\n            base_response = self._get_completion(\n                self.base_model, \n                prompt\n            )\n            \n            # Test fine-tuned model\n            ft_response = self._get_completion(\n                self.fine_tuned_model,\n                prompt\n            )\n            \n            # Calculate scores\n            base_score = self._calculate_score(base_response, expected)\n            ft_score = self._calculate_score(ft_response, expected)\n            \n            self.results.append({\n                'prompt': prompt,\n                'base_score': base_score,\n                'ft_score': ft_score,\n                'improvement': ft_score - base_score\n            })\n        \n        return self._generate_report()\n    \n    def _get_completion(self, model: str, prompt: str) -> str:\n        \"\"\"Get completion from model\"\"\"\n        response = openai.ChatCompletion.create(\n            model=model,\n            messages=[\n                {\"role\": \"user\", \"content\": prompt}\n            ],\n            temperature=0.3,\n            max_tokens=500\n        )\n        return response.choices[0].message['content']\n    \n    def _calculate_score(self, response: str, expected: str) -> float:\n        \"\"\"Calculate similarity score (0-1)\"\"\"\n        # Simple scoring based on keyword matching\n        # In production, use more sophisticated metrics\n        expected_keywords = set(expected.lower().split())\n        response_keywords = set(response.lower().split())\n        \n        if not expected_keywords:\n            return 0.0\n        \n        matches = expected_keywords & response_keywords\n        return len(matches) / len(expected_keywords)\n    \n    def _generate_report(self) -> Dict:\n        \"\"\"Generate comparison report\"\"\"\n        base_scores = [r['base_score'] for r in self.results]\n        ft_scores = [r['ft_score'] for r in self.results]\n        improvements = [r['improvement'] for r in self.results]\n        \n        return {\n            'num_tests': len(self.results),\n            'base_model_avg': statistics.mean(base_scores),\n            'fine_tuned_avg': statistics.mean(ft_scores),\n            'avg_improvement': statistics.mean(improvements),\n            'improvement_rate': sum(1 for i in improvements if i > 0) / len(improvements),\n            'details': self.results\n        }\n\n# Example validation\ntest_cases = [\n    {\n        'prompt': 'Generate tests for: def square(n): return n**2',\n        'expected_output': 'test_positive test_negative test_zero assertEqual'\n    },\n    {\n        'prompt': 'Generate tests for: def is_even(n): return n % 2 == 0',\n        'expected_output': 'test_even_numbers test_odd_numbers assertTrue assertFalse'\n    }\n]\n\nvalidator = ModelValidator('gpt-3.5-turbo', 'ft:gpt-3.5-turbo:org:model-id')\nreport = validator.compare_models(test_cases)\n\nprint(f\"Base Model Average: {report['base_model_avg']:.2%}\")\nprint(f\"Fine-Tuned Average: {report['fine_tuned_avg']:.2%}\")\nprint(f\"Average Improvement: {report['avg_improvement']:.2%}\")\nprint(f\"Tests Improved: {report['improvement_rate']:.0%}\")"
            }
          ],
          "checklist": [
            {
              "id": "check-5-1-1",
              "text": "Understand when to use fine-tuning vs prompt engineering",
              "completed": false
            },
            {
              "id": "check-5-1-2",
              "text": "Learn the fine-tuning process from data preparation to deployment",
              "completed": false
            },
            {
              "id": "check-5-1-3",
              "text": "Prepare training data in JSONL format",
              "completed": false
            },
            {
              "id": "check-5-1-4",
              "text": "Use OpenAI's fine-tuning API",
              "completed": false
            },
            {
              "id": "check-5-1-5",
              "text": "Monitor fine-tuning job progress",
              "completed": false
            },
            {
              "id": "check-5-1-6",
              "text": "Validate fine-tuned model performance",
              "completed": false
            },
            {
              "id": "check-5-1-7",
              "text": "Calculate cost-benefit analysis for fine-tuning",
              "completed": false
            },
            {
              "id": "check-5-1-8",
              "text": "Implement unit test generation with fine-tuned models",
              "completed": false
            }
          ],
          "timestamps": [
            {
              "time": "0:00:10",
              "label": "Welcome to Week 5",
              "description": "Introduction to fine-tuning concepts and objectives; Adjusting weights of LLMs for specific tasks"
            },
            {
              "time": "0:00:46",
              "label": "Instruction-Tuned Models",
              "description": "Evolution from raw to instruction-tuned models"
            },
            {
              "time": "0:01:04",
              "label": "Fine-Tuning for Developers",
              "description": "Practical applications for software development"
            },
            {
              "time": "0:02:21",
              "label": "Learning Objectives",
              "description": "Six key competencies for fine-tuning; Decision criteria and use cases"
            },
            {
              "time": "0:02:52",
              "label": "Model Selection Criteria",
              "description": "Choosing the right base model"
            },
            {
              "time": "0:03:36",
              "label": "Production Deployment",
              "description": "Steps from model selection to deployment"
            },
            {
              "time": "0:04:16",
              "label": "Data Preparation",
              "description": "Gathering and formatting training data"
            },
            {
              "time": "0:06:52",
              "label": "Cost Considerations",
              "description": "When fine-tuning saves money vs prompting"
            },
            {
              "time": "0:10:00",
              "label": "Domain-Specific Applications",
              "description": "Learn language model and production implementation techniques"
            },
            {
              "time": "0:15:30",
              "label": "Unit Test Generation Demo",
              "description": "Practical example generating test code"
            },
            {
              "time": "0:25:45",
              "label": "Training Data Format",
              "description": "JSONL format and structure requirements"
            },
            {
              "time": "0:35:20",
              "label": "Validation and Testing",
              "description": "Comparing base vs fine-tuned model performance"
            },
            {
              "time": "0:45:00",
              "label": "Best Practices",
              "description": "Industry standards for language model and production development"
            }
          ],
          "videoPath": "cohort_2/week_5_class_1_2024-06-17.mp4"
        },
        {
          "id": "lesson-5-2",
          "title": "API Development and Deployment",
          "duration": "1:07:29",
          "videoUrl": "/videos/cohort_2/week_05/week_5_class_2_2024-06-18.mp4",
          "videoId": "week_5_class_2_2024-06-18",
          "content": "# Flask API and Agent Implementation\n\n## Overview\n\nThis office hours session focuses on practical implementation, converting a prompt-based code agent into a production-ready Flask API. We also explore Lightning AI as a platform for GPU-accelerated model training.\n\n## Code Agent to API Transformation\n\nThe session demonstrates refactoring an existing LangChain agent from a command-line interface to a RESTful API, making it accessible for web applications and service integration.\n\n## Key Components\n\n### Flask API Structure\n- RESTful endpoint design\n- Request/response handling\n- Authentication and security considerations\n- Error handling and validation\n\n### Agent Capabilities\n- File manipulation (create, read, update, delete)\n- Directory operations\n- Shell command execution\n- Support for multiple file formats (CSV, XML, YAML, JSON)\n\n### Web Frontend\nBuilding a simple chat interface that communicates with the Flask API, providing users with an intuitive way to interact with the code agent.\n\n## Lightning AI Platform\n\nIntroduction to Lightning AI for:\n- GPU-accelerated training\n- Distributed computing\n- Model experimentation\n- Cost-effective training solutions\n\n## Production Considerations\n\n- API rate limiting\n- Caching strategies\n- Load balancing\n- Monitoring and logging\n- Security best practices\n\n## Hands-on Implementation\n\nStudents work on converting their own agents to APIs, with real-time troubleshooting and guidance on common challenges.",
          "codeExamples": [
            {
              "id": "code-5-2-1",
              "title": "Flask API for LangChain Agent",
              "language": "python",
              "code": "from flask import Flask, request, jsonify\nfrom flask_cors import CORS\nfrom langchain.agents import create_openai_tools_agent, AgentExecutor\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.tools import tool\nimport logging\n\napp = Flask(__name__)\nCORS(app)\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Define agent tools\n@tool\ndef create_file(filename: str, content: str) -> str:\n    \"\"\"Create a new file with specified content\"\"\"\n    try:\n        with open(filename, 'w') as f:\n            f.write(content)\n        return f\"File {filename} created successfully\"\n    except Exception as e:\n        return f\"Error creating file: {str(e)}\"\n\n@tool\ndef read_file(filename: str) -> str:\n    \"\"\"Read contents of a file\"\"\"\n    try:\n        with open(filename, 'r') as f:\n            return f.read()\n    except Exception as e:\n        return f\"Error reading file: {str(e)}\"\n\n@tool\ndef list_files(directory: str = '.') -> str:\n    \"\"\"List files in a directory\"\"\"\n    import os\n    try:\n        files = os.listdir(directory)\n        return '\\n'.join(files)\n    except Exception as e:\n        return f\"Error listing files: {str(e)}\"\n\n# Initialize agent\ndef create_agent():\n    llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n    tools = [create_file, read_file, list_files]\n    \n    from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n    \n    prompt = ChatPromptTemplate.from_messages([\n        (\"system\", \"You are a helpful coding assistant with file system access.\"),\n        (\"user\", \"{input}\"),\n        MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n    ])\n    \n    agent = create_openai_tools_agent(llm, tools, prompt)\n    return AgentExecutor(agent=agent, tools=tools, verbose=True)\n\nagent_executor = create_agent()\n\n# API Routes\n@app.route('/health', methods=['GET'])\ndef health_check():\n    return jsonify({\"status\": \"healthy\"}), 200\n\n@app.route('/execute', methods=['POST'])\ndef execute_command():\n    try:\n        data = request.json\n        command = data.get('command', '')\n        \n        if not command:\n            return jsonify({\"error\": \"No command provided\"}), 400\n        \n        logger.info(f\"Executing command: {command}\")\n        \n        # Execute agent\n        result = agent_executor.invoke({\"input\": command})\n        \n        return jsonify({\n            \"success\": True,\n            \"result\": result['output']\n        }), 200\n        \n    except Exception as e:\n        logger.error(f\"Error executing command: {str(e)}\")\n        return jsonify({\n            \"success\": False,\n            \"error\": str(e)\n        }), 500\n\n@app.route('/tools', methods=['GET'])\ndef list_tools():\n    \"\"\"List available tools\"\"\"\n    tools_info = [\n        {\n            \"name\": tool.name,\n            \"description\": tool.description\n        }\n        for tool in agent_executor.tools\n    ]\n    return jsonify({\"tools\": tools_info}), 200\n\nif __name__ == '__main__':\n    app.run(debug=True, port=5000)"
            },
            {
              "id": "code-5-2-2",
              "title": "Web Frontend for Agent API",
              "language": "html",
              "code": "<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Code Agent Interface</title>\n    <style>\n        body {\n            font-family: 'Segoe UI', system-ui, sans-serif;\n            max-width: 800px;\n            margin: 0 auto;\n            padding: 20px;\n            background: #f5f5f5;\n        }\n        .container {\n            background: white;\n            border-radius: 10px;\n            padding: 30px;\n            box-shadow: 0 2px 10px rgba(0,0,0,0.1);\n        }\n        h1 {\n            color: #333;\n            border-bottom: 2px solid #4CAF50;\n            padding-bottom: 10px;\n        }\n        .input-group {\n            display: flex;\n            gap: 10px;\n            margin: 20px 0;\n        }\n        input[type=\"text\"] {\n            flex: 1;\n            padding: 12px;\n            border: 2px solid #ddd;\n            border-radius: 5px;\n            font-size: 16px;\n        }\n        button {\n            padding: 12px 24px;\n            background: #4CAF50;\n            color: white;\n            border: none;\n            border-radius: 5px;\n            cursor: pointer;\n            font-size: 16px;\n        }\n        button:hover {\n            background: #45a049;\n        }\n        button:disabled {\n            background: #ccc;\n            cursor: not-allowed;\n        }\n        .output {\n            margin-top: 20px;\n            padding: 15px;\n            background: #f9f9f9;\n            border-left: 4px solid #4CAF50;\n            border-radius: 5px;\n            white-space: pre-wrap;\n            font-family: 'Courier New', monospace;\n        }\n        .error {\n            border-left-color: #f44336;\n            background: #ffebee;\n        }\n        .loading {\n            display: none;\n            color: #666;\n            font-style: italic;\n        }\n        .tools {\n            margin: 20px 0;\n            padding: 15px;\n            background: #e8f5e9;\n            border-radius: 5px;\n        }\n    </style>\n</head>\n<body>\n    <div class=\"container\">\n        <h1>\ud83e\udd16 Code Agent Interface</h1>\n        \n        <div class=\"tools\" id=\"toolsList\">\n            <h3>Available Tools:</h3>\n            <ul id=\"tools\"></ul>\n        </div>\n        \n        <div class=\"input-group\">\n            <input type=\"text\" \n                   id=\"commandInput\" \n                   placeholder=\"Enter your command (e.g., 'Create a Python file with a hello world function')\"\n                   onkeypress=\"if(event.key==='Enter') executeCommand()\">\n            <button onclick=\"executeCommand()\" id=\"executeBtn\">Execute</button>\n        </div>\n        \n        <div class=\"loading\" id=\"loading\">Processing...</div>\n        \n        <div class=\"output\" id=\"output\" style=\"display: none;\"></div>\n    </div>\n\n    <script>\n        const API_URL = 'http://localhost:5000';\n        \n        // Load available tools on page load\n        async function loadTools() {\n            try {\n                const response = await fetch(`${API_URL}/tools`);\n                const data = await response.json();\n                \n                const toolsList = document.getElementById('tools');\n                toolsList.innerHTML = data.tools.map(tool => \n                    `<li><strong>${tool.name}</strong>: ${tool.description}</li>`\n                ).join('');\n            } catch (error) {\n                console.error('Error loading tools:', error);\n            }\n        }\n        \n        async function executeCommand() {\n            const input = document.getElementById('commandInput');\n            const output = document.getElementById('output');\n            const loading = document.getElementById('loading');\n            const executeBtn = document.getElementById('executeBtn');\n            \n            const command = input.value.trim();\n            if (!command) return;\n            \n            // Show loading state\n            loading.style.display = 'block';\n            output.style.display = 'none';\n            executeBtn.disabled = true;\n            \n            try {\n                const response = await fetch(`${API_URL}/execute`, {\n                    method: 'POST',\n                    headers: {\n                        'Content-Type': 'application/json',\n                    },\n                    body: JSON.stringify({ command })\n                });\n                \n                const data = await response.json();\n                \n                // Display result\n                output.style.display = 'block';\n                output.className = 'output';\n                \n                if (data.success) {\n                    output.textContent = data.result;\n                } else {\n                    output.className = 'output error';\n                    output.textContent = `Error: ${data.error}`;\n                }\n                \n            } catch (error) {\n                output.style.display = 'block';\n                output.className = 'output error';\n                output.textContent = `Connection error: ${error.message}`;\n            } finally {\n                loading.style.display = 'none';\n                executeBtn.disabled = false;\n            }\n        }\n        \n        // Load tools when page loads\n        loadTools();\n    </script>\n</body>\n</html>"
            },
            {
              "id": "code-5-2-3",
              "title": "Lightning AI Training Setup",
              "language": "python",
              "code": "import lightning as L\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport json\n\nclass FineTuningDataset(Dataset):\n    \"\"\"Custom dataset for fine-tuning\"\"\"\n    \n    def __init__(self, data_path, tokenizer, max_length=512):\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n        # Load training data\n        with open(data_path, 'r') as f:\n            self.data = [json.loads(line) for line in f]\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        item = self.data[idx]\n        \n        # Format the conversation\n        text = \"\"\n        for message in item['messages']:\n            text += f\"{message['role']}: {message['content']}\\n\"\n        \n        # Tokenize\n        encoding = self.tokenizer(\n            text,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n        \n        return {\n            'input_ids': encoding['input_ids'].squeeze(),\n            'attention_mask': encoding['attention_mask'].squeeze(),\n            'labels': encoding['input_ids'].squeeze()\n        }\n\nclass FineTuningModule(L.LightningModule):\n    \"\"\"Lightning module for fine-tuning\"\"\"\n    \n    def __init__(self, model_name='gpt2', learning_rate=5e-5):\n        super().__init__()\n        self.save_hyperparameters()\n        \n        # Load model and tokenizer\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        \n        # Add padding token if needed\n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n    \n    def forward(self, input_ids, attention_mask, labels=None):\n        return self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n    \n    def training_step(self, batch, batch_idx):\n        outputs = self(\n            input_ids=batch['input_ids'],\n            attention_mask=batch['attention_mask'],\n            labels=batch['labels']\n        )\n        \n        loss = outputs.loss\n        self.log('train_loss', loss, prog_bar=True)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        outputs = self(\n            input_ids=batch['input_ids'],\n            attention_mask=batch['attention_mask'],\n            labels=batch['labels']\n        )\n        \n        loss = outputs.loss\n        self.log('val_loss', loss, prog_bar=True)\n        return loss\n    \n    def configure_optimizers(self):\n        return torch.optim.AdamW(\n            self.parameters(),\n            lr=self.hparams.learning_rate\n        )\n    \n    def generate_text(self, prompt, max_length=100):\n        \"\"\"Generate text using the fine-tuned model\"\"\"\n        inputs = self.tokenizer(prompt, return_tensors='pt')\n        \n        with torch.no_grad():\n            outputs = self.model.generate(\n                inputs['input_ids'],\n                max_length=max_length,\n                temperature=0.7,\n                do_sample=True\n            )\n        \n        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Training script\ndef train_model(data_path, model_name='gpt2', epochs=3):\n    # Initialize model\n    model = FineTuningModule(model_name=model_name)\n    \n    # Create dataset\n    train_dataset = FineTuningDataset(\n        data_path,\n        model.tokenizer\n    )\n    \n    # Create dataloader\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=4,\n        shuffle=True,\n        num_workers=2\n    )\n    \n    # Initialize trainer\n    trainer = L.Trainer(\n        max_epochs=epochs,\n        accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n        devices=1,\n        precision='16-mixed' if torch.cuda.is_available() else 32,\n        gradient_clip_val=1.0,\n        accumulate_grad_batches=4,\n        log_every_n_steps=10\n    )\n    \n    # Train\n    trainer.fit(model, train_loader)\n    \n    # Save model\n    trainer.save_checkpoint('fine_tuned_model.ckpt')\n    \n    return model\n\n# Usage\nif __name__ == '__main__':\n    model = train_model('training_data.jsonl', epochs=3)\n    \n    # Test generation\n    prompt = \"Generate unit tests for: def factorial(n): return 1 if n <= 1 else n * factorial(n-1)\"\n    result = model.generate_text(prompt)\n    print(f\"Generated: {result}\")"
            }
          ],
          "checklist": [
            {
              "id": "check-5-2-1",
              "text": "Convert LangChain agents to Flask APIs",
              "completed": false
            },
            {
              "id": "check-5-2-2",
              "text": "Build RESTful endpoints for agent interaction",
              "completed": false
            },
            {
              "id": "check-5-2-3",
              "text": "Create web frontends for API interaction",
              "completed": false
            },
            {
              "id": "check-5-2-4",
              "text": "Implement error handling and validation",
              "completed": false
            },
            {
              "id": "check-5-2-5",
              "text": "Set up Lightning AI for GPU training",
              "completed": false
            },
            {
              "id": "check-5-2-6",
              "text": "Deploy APIs to production environments",
              "completed": false
            },
            {
              "id": "check-5-2-7",
              "text": "Implement API security and rate limiting",
              "completed": false
            },
            {
              "id": "check-5-2-8",
              "text": "Monitor API performance and usage",
              "completed": false
            },
            {
              "id": "check-5-2-9",
              "text": "Test API endpoints thoroughly",
              "completed": false
            },
            {
              "id": "check-5-2-10",
              "text": "Document API specifications",
              "completed": false
            }
          ],
          "timestamps": [
            {
              "time": "0:00:06",
              "label": "Course Introduction",
              "description": "Overview of Flask API implementation; Converting prompt-based agent to API"
            },
            {
              "time": "0:00:30",
              "label": "API Configuration",
              "description": "Introduction to GPU training platform"
            },
            {
              "time": "0:01:07",
              "label": "Code Agent Architecture",
              "description": "Understanding the existing agent structure"
            },
            {
              "time": "0:01:52",
              "label": "Flask API Setup",
              "description": "Configure production and integration parameters and dependencies"
            },
            {
              "time": "0:02:18",
              "label": "File Type Support",
              "description": "Handling CSV, XML, YAML files; Available tools and capabilities"
            },
            {
              "time": "0:02:47",
              "label": "Model Selection",
              "description": "Choosing between GPT-4 and fine-tuned models"
            },
            {
              "time": "0:05:00",
              "label": "API Implementation",
              "description": "Learn production and integration implementation techniques"
            },
            {
              "time": "0:10:30",
              "label": "Web Frontend Development",
              "description": "Building a simple chat interface"
            },
            {
              "time": "0:18:45",
              "label": "Error Handling",
              "description": "Managing API errors and edge cases"
            },
            {
              "time": "0:25:20",
              "label": "Testing the API",
              "description": "Validate production and integration functionality and edge cases"
            },
            {
              "time": "0:35:00",
              "label": "Lightning AI Demo",
              "description": "Learn production and integration implementation techniques"
            },
            {
              "time": "0:45:30",
              "label": "Deployment Considerations",
              "description": "Deploy production and integration to production environment"
            },
            {
              "time": "0:55:00",
              "label": "Student Projects Support",
              "description": "Learn production and integration implementation techniques"
            }
          ],
          "videoPath": "cohort_2/week_5_class_2_2024-06-18.mp4"
        },
        {
          "id": "lesson-5-3",
          "title": "Open-Source Model Fine-Tuning",
          "duration": "1:23:44",
          "videoUrl": "/videos/cohort_2/week_05/week_5_class_3_2024-06-20.mp4",
          "videoId": "week_5_class_3_2024-06-20",
          "content": "# Advanced Fine-Tuning with Open-Source Models\n\n## Overview\n\nThis lesson ventures beyond the OpenAI ecosystem to explore fine-tuning open-source models, specifically Meta's LLAMA. We demonstrate practical techniques for fine-tuning with limited computational resources using LoRA and QLoRA.\n\n## Open-Source Model Landscape\n\n### Why Open-Source Models?\n- Full control over the model\n- No API costs for inference\n- Privacy and data security\n- Customization flexibility\n- On-premise deployment options\n\n## LLAMA Model Family\n\nMeta's LLAMA (Large Language Model Meta AI) provides powerful open-source alternatives to commercial models. Available in various sizes (7B, 13B, 70B parameters), these models can be fine-tuned for specific tasks.\n\n## Efficient Fine-Tuning Techniques\n\n### LoRA (Low-Rank Adaptation)\nInstead of updating all model weights, LoRA introduces trainable rank decomposition matrices, dramatically reducing memory requirements and training time.\n\n### QLoRA (Quantized LoRA)\nCombines quantization with LoRA, enabling fine-tuning of large models on consumer GPUs by reducing precision while maintaining performance.\n\n## Practical Implementation\n\nThe lesson demonstrates:\n1. Setting up Google Colab with free GPU\n2. Loading LLAMA model with quantization\n3. Preparing dataset in the correct format\n4. Configuring LoRA parameters\n5. Training and monitoring progress\n6. Evaluating and saving the model\n\n## Deployment Considerations\n\n### Self-Hosting Options\n- Local deployment with GGML/llama.cpp\n- Cloud deployment on AWS/GCP/Azure\n- Edge deployment for offline applications\n\n### Cost Analysis\nComparing total cost of ownership between API-based services and self-hosted models, considering:\n- Infrastructure costs\n- Maintenance overhead\n- Scaling requirements\n- Performance needs\n\n## Advanced Topics\n\n- Direct Preference Optimization (DPO)\n- Reinforcement Learning from Human Feedback (RLHF)\n- Multi-task fine-tuning\n- Continuous learning strategies",
          "codeExamples": [
            {
              "id": "code-5-3-1",
              "title": "LLAMA Fine-Tuning with LoRA",
              "language": "python",
              "code": "# Install required packages (run in notebook)\n# !pip install transformers peft datasets accelerate bitsandbytes\n\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments\n)\nfrom peft import LoraConfig, get_peft_model, TaskType\nfrom datasets import load_dataset\nimport torch\n\nclass LLAMAFineTuner:\n    def __init__(self, model_name=\"meta-llama/Llama-2-7b-hf\"):\n        self.model_name = model_name\n        self.setup_quantization()\n        self.load_model()\n        self.setup_lora()\n    \n    def setup_quantization(self):\n        \"\"\"Configure 4-bit quantization for memory efficiency\"\"\"\n        self.bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.bfloat16\n        )\n    \n    def load_model(self):\n        \"\"\"Load LLAMA model with quantization\"\"\"\n        self.model = AutoModelForCausalLM.from_pretrained(\n            self.model_name,\n            quantization_config=self.bnb_config,\n            device_map=\"auto\",\n            trust_remote_code=True\n        )\n        \n        self.tokenizer = AutoTokenizer.from_pretrained(\n            self.model_name,\n            trust_remote_code=True\n        )\n        \n        # Add padding token\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n        self.tokenizer.padding_side = \"right\"\n    \n    def setup_lora(self):\n        \"\"\"Configure LoRA for efficient fine-tuning\"\"\"\n        lora_config = LoraConfig(\n            r=16,  # Rank\n            lora_alpha=32,\n            target_modules=[\n                \"q_proj\",\n                \"k_proj\", \n                \"v_proj\",\n                \"o_proj\",\n                \"gate_proj\",\n                \"up_proj\",\n                \"down_proj\"\n            ],\n            lora_dropout=0.1,\n            bias=\"none\",\n            task_type=TaskType.CAUSAL_LM\n        )\n        \n        # Apply LoRA to model\n        self.model = get_peft_model(self.model, lora_config)\n        \n        # Print trainable parameters\n        self.print_trainable_parameters()\n    \n    def print_trainable_parameters(self):\n        \"\"\"Display number of trainable parameters\"\"\"\n        trainable_params = 0\n        all_param = 0\n        \n        for _, param in self.model.named_parameters():\n            all_param += param.numel()\n            if param.requires_grad:\n                trainable_params += param.numel()\n        \n        print(f\"Trainable params: {trainable_params:,}\")\n        print(f\"All params: {all_param:,}\")\n        print(f\"Trainable %: {100 * trainable_params / all_param:.2f}%\")\n    \n    def prepare_dataset(self, dataset_name):\n        \"\"\"Load and prepare dataset for training\"\"\"\n        dataset = load_dataset(dataset_name, split=\"train\")\n        \n        def formatting_func(examples):\n            texts = []\n            for instruction, output in zip(examples['instruction'], examples['output']):\n                text = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{output}\"\n                texts.append(text)\n            return texts\n        \n        # Tokenize dataset\n        def tokenize_function(examples):\n            return self.tokenizer(\n                formatting_func(examples),\n                truncation=True,\n                padding=True,\n                max_length=512\n            )\n        \n        tokenized_dataset = dataset.map(\n            tokenize_function,\n            batched=True,\n            remove_columns=dataset.column_names\n        )\n        \n        return tokenized_dataset\n    \n    def train(self, dataset, output_dir=\"./llama-fine-tuned\"):\n        \"\"\"Train the model with LoRA\"\"\"\n        training_args = TrainingArguments(\n            output_dir=output_dir,\n            num_train_epochs=3,\n            per_device_train_batch_size=4,\n            gradient_accumulation_steps=4,\n            warmup_steps=100,\n            learning_rate=2e-4,\n            fp16=True,\n            logging_steps=10,\n            save_strategy=\"epoch\",\n            evaluation_strategy=\"no\",\n            save_total_limit=2,\n            load_best_model_at_end=False,\n            report_to=\"none\",  # Disable wandb\n            remove_unused_columns=False,\n            gradient_checkpointing=True\n        )\n        \n        from transformers import Trainer\n        \n        trainer = Trainer(\n            model=self.model,\n            args=training_args,\n            train_dataset=dataset,\n            tokenizer=self.tokenizer\n        )\n        \n        # Start training\n        trainer.train()\n        \n        # Save LoRA weights\n        self.model.save_pretrained(output_dir)\n        self.tokenizer.save_pretrained(output_dir)\n        \n        print(f\"Model saved to {output_dir}\")\n    \n    def generate(self, prompt, max_length=200):\n        \"\"\"Generate text using fine-tuned model\"\"\"\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n        \n        with torch.no_grad():\n            outputs = self.model.generate(\n                **inputs,\n                max_length=max_length,\n                temperature=0.7,\n                do_sample=True,\n                top_p=0.95\n            )\n        \n        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Usage example\nfine_tuner = LLAMAFineTuner()\n# dataset = fine_tuner.prepare_dataset(\"your-dataset\")\n# fine_tuner.train(dataset)\n\n# Test generation\nprompt = \"### Instruction:\\nWrite a Python function to calculate fibonacci\\n\\n### Response:\\n\"\n# response = fine_tuner.generate(prompt)\n# print(response)"
            },
            {
              "id": "code-5-3-2",
              "title": "QLoRA Implementation",
              "language": "python",
              "code": "import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\nfrom peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\nfrom trl import SFTTrainer\nfrom datasets import Dataset\nimport pandas as pd\n\ndef setup_qlora_training(model_name=\"meta-llama/Llama-2-7b-hf\"):\n    \"\"\"Setup QLoRA for memory-efficient fine-tuning\"\"\"\n    \n    # Quantization config for 4-bit\n    from transformers import BitsAndBytesConfig\n    \n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.float16,\n        bnb_4bit_use_double_quant=True,\n    )\n    \n    # Load model with quantization\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        quantization_config=bnb_config,\n        device_map=\"auto\",\n        trust_remote_code=True,\n    )\n    \n    # Prepare model for k-bit training\n    model = prepare_model_for_kbit_training(model)\n    \n    # LoRA configuration\n    peft_config = LoraConfig(\n        lora_alpha=16,\n        lora_dropout=0.1,\n        r=64,  # Rank\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n        target_modules=[\n            \"q_proj\",\n            \"k_proj\",\n            \"v_proj\",\n            \"o_proj\",\n            \"gate_proj\",\n            \"up_proj\",\n            \"down_proj\",\n        ],\n    )\n    \n    # Get PEFT model\n    model = get_peft_model(model, peft_config)\n    \n    # Load tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.padding_side = \"right\"\n    \n    return model, tokenizer, peft_config\n\ndef create_training_dataset(data_path):\n    \"\"\"Create dataset for training\"\"\"\n    \n    # Load your data\n    df = pd.read_csv(data_path)\n    \n    # Format for training\n    def format_instruction(row):\n        return f\"\"\"### Human: {row['instruction']}\n### Assistant: {row['response']}\n### Human: Thank you!\n### Assistant: You're welcome!\"\"\"\n    \n    df['text'] = df.apply(format_instruction, axis=1)\n    \n    # Convert to Dataset\n    dataset = Dataset.from_pandas(df[['text']])\n    \n    return dataset\n\ndef train_with_qlora(\n    model,\n    tokenizer,\n    peft_config,\n    dataset,\n    output_dir=\"./qlora-output\"\n):\n    \"\"\"Train model using QLoRA\"\"\"\n    \n    # Training arguments\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        num_train_epochs=3,\n        per_device_train_batch_size=4,\n        gradient_accumulation_steps=4,\n        optim=\"paged_adamw_32bit\",\n        save_steps=50,\n        logging_steps=10,\n        learning_rate=2e-4,\n        weight_decay=0.001,\n        fp16=False,\n        bf16=False,\n        max_grad_norm=0.3,\n        max_steps=-1,\n        warmup_ratio=0.03,\n        group_by_length=True,\n        lr_scheduler_type=\"cosine\",\n        report_to=\"none\",\n        gradient_checkpointing=True,\n    )\n    \n    # Create trainer\n    trainer = SFTTrainer(\n        model=model,\n        train_dataset=dataset,\n        peft_config=peft_config,\n        dataset_text_field=\"text\",\n        max_seq_length=512,\n        tokenizer=tokenizer,\n        args=training_args,\n        packing=False,\n    )\n    \n    # Train\n    trainer.train()\n    \n    # Save model\n    trainer.model.save_pretrained(output_dir)\n    trainer.tokenizer.save_pretrained(output_dir)\n    \n    return trainer\n\n# Memory monitoring utility\ndef print_gpu_memory():\n    \"\"\"Print current GPU memory usage\"\"\"\n    if torch.cuda.is_available():\n        print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n        print(f\"GPU Memory Reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n    else:\n        print(\"No GPU available\")\n\n# Complete training pipeline\ndef run_qlora_training(data_path, model_name=\"meta-llama/Llama-2-7b-hf\"):\n    \"\"\"Complete QLoRA training pipeline\"\"\"\n    \n    print(\"Setting up QLoRA...\")\n    model, tokenizer, peft_config = setup_qlora_training(model_name)\n    \n    print(\"\\nModel setup complete!\")\n    print_gpu_memory()\n    \n    print(\"\\nPreparing dataset...\")\n    dataset = create_training_dataset(data_path)\n    \n    print(f\"\\nDataset size: {len(dataset)}\")\n    \n    print(\"\\nStarting training...\")\n    trainer = train_with_qlora(\n        model,\n        tokenizer,\n        peft_config,\n        dataset\n    )\n    \n    print(\"\\nTraining complete!\")\n    print_gpu_memory()\n    \n    return trainer\n\n# Example usage\n# trainer = run_qlora_training(\"training_data.csv\")"
            },
            {
              "id": "code-5-3-3",
              "title": "Model Deployment and Inference",
              "language": "python",
              "code": "import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\nimport time\nfrom typing import Optional\n\nclass DeployedModel:\n    \"\"\"Class for deploying and serving fine-tuned models\"\"\"\n    \n    def __init__(\n        self,\n        base_model_name: str,\n        adapter_path: Optional[str] = None,\n        load_in_8bit: bool = True\n    ):\n        self.base_model_name = base_model_name\n        self.adapter_path = adapter_path\n        self.load_in_8bit = load_in_8bit\n        \n        self.load_model()\n    \n    def load_model(self):\n        \"\"\"Load base model and LoRA adapter if provided\"\"\"\n        \n        print(f\"Loading base model: {self.base_model_name}\")\n        \n        # Load base model\n        if self.load_in_8bit:\n            from transformers import BitsAndBytesConfig\n            \n            quantization_config = BitsAndBytesConfig(\n                load_in_8bit=True,\n                bnb_8bit_compute_dtype=torch.float16\n            )\n            \n            self.model = AutoModelForCausalLM.from_pretrained(\n                self.base_model_name,\n                quantization_config=quantization_config,\n                device_map=\"auto\"\n            )\n        else:\n            self.model = AutoModelForCausalLM.from_pretrained(\n                self.base_model_name,\n                device_map=\"auto\",\n                torch_dtype=torch.float16\n            )\n        \n        # Load tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_name)\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n        \n        # Load LoRA adapter if provided\n        if self.adapter_path:\n            print(f\"Loading LoRA adapter: {self.adapter_path}\")\n            self.model = PeftModel.from_pretrained(\n                self.model,\n                self.adapter_path\n            )\n            # Merge adapter weights for faster inference\n            self.model = self.model.merge_and_unload()\n        \n        print(\"Model loaded successfully!\")\n    \n    def generate(\n        self,\n        prompt: str,\n        max_new_tokens: int = 200,\n        temperature: float = 0.7,\n        top_p: float = 0.95,\n        do_sample: bool = True\n    ) -> str:\n        \"\"\"Generate text from prompt\"\"\"\n        \n        # Tokenize input\n        inputs = self.tokenizer(\n            prompt,\n            return_tensors=\"pt\",\n            truncation=True,\n            max_length=512\n        ).to(self.model.device)\n        \n        # Generate\n        start_time = time.time()\n        \n        with torch.no_grad():\n            outputs = self.model.generate(\n                **inputs,\n                max_new_tokens=max_new_tokens,\n                temperature=temperature,\n                top_p=top_p,\n                do_sample=do_sample,\n                pad_token_id=self.tokenizer.eos_token_id\n            )\n        \n        generation_time = time.time() - start_time\n        \n        # Decode output\n        response = self.tokenizer.decode(\n            outputs[0][inputs['input_ids'].shape[1]:],\n            skip_special_tokens=True\n        )\n        \n        return {\n            'response': response,\n            'generation_time': generation_time,\n            'tokens_generated': outputs.shape[1] - inputs['input_ids'].shape[1]\n        }\n    \n    def benchmark(self, test_prompts: list) -> dict:\n        \"\"\"Benchmark model performance\"\"\"\n        \n        results = []\n        \n        for prompt in test_prompts:\n            result = self.generate(prompt, max_new_tokens=100)\n            results.append({\n                'prompt': prompt[:50] + '...',\n                'response_length': len(result['response']),\n                'time': result['generation_time'],\n                'tokens_per_second': result['tokens_generated'] / result['generation_time']\n            })\n        \n        # Calculate averages\n        avg_time = sum(r['time'] for r in results) / len(results)\n        avg_tps = sum(r['tokens_per_second'] for r in results) / len(results)\n        \n        return {\n            'num_tests': len(test_prompts),\n            'avg_generation_time': avg_time,\n            'avg_tokens_per_second': avg_tps,\n            'details': results\n        }\n\n# Deployment configurations\nclass ModelServer:\n    \"\"\"Simple model server for inference\"\"\"\n    \n    def __init__(self, model: DeployedModel):\n        self.model = model\n        self.request_count = 0\n        self.total_generation_time = 0\n    \n    def handle_request(self, request: dict) -> dict:\n        \"\"\"Handle inference request\"\"\"\n        \n        prompt = request.get('prompt', '')\n        params = request.get('params', {})\n        \n        # Validate input\n        if not prompt:\n            return {'error': 'No prompt provided'}\n        \n        try:\n            # Generate response\n            result = self.model.generate(\n                prompt,\n                max_new_tokens=params.get('max_tokens', 200),\n                temperature=params.get('temperature', 0.7),\n                top_p=params.get('top_p', 0.95)\n            )\n            \n            # Update metrics\n            self.request_count += 1\n            self.total_generation_time += result['generation_time']\n            \n            return {\n                'success': True,\n                'response': result['response'],\n                'metrics': {\n                    'generation_time': result['generation_time'],\n                    'tokens_generated': result['tokens_generated']\n                }\n            }\n            \n        except Exception as e:\n            return {\n                'success': False,\n                'error': str(e)\n            }\n    \n    def get_stats(self) -> dict:\n        \"\"\"Get server statistics\"\"\"\n        return {\n            'total_requests': self.request_count,\n            'avg_generation_time': (\n                self.total_generation_time / self.request_count \n                if self.request_count > 0 else 0\n            )\n        }\n\n# Usage example\nif __name__ == \"__main__\":\n    # Deploy fine-tuned model\n    model = DeployedModel(\n        base_model_name=\"meta-llama/Llama-2-7b-hf\",\n        adapter_path=\"./qlora-output\",\n        load_in_8bit=True\n    )\n    \n    # Create server\n    server = ModelServer(model)\n    \n    # Handle requests\n    request = {\n        'prompt': 'Write a Python function to sort a list:',\n        'params': {\n            'max_tokens': 150,\n            'temperature': 0.5\n        }\n    }\n    \n    response = server.handle_request(request)\n    print(f\"Response: {response['response']}\")\n    print(f\"Stats: {server.get_stats()}\")"
            }
          ],
          "checklist": [
            {
              "id": "check-5-3-1",
              "text": "Understand LoRA and QLoRA techniques",
              "completed": false
            },
            {
              "id": "check-5-3-2",
              "text": "Set up environment for LLAMA fine-tuning",
              "completed": false
            },
            {
              "id": "check-5-3-3",
              "text": "Implement 4-bit quantization for memory efficiency",
              "completed": false
            },
            {
              "id": "check-5-3-4",
              "text": "Configure LoRA parameters for optimal training",
              "completed": false
            },
            {
              "id": "check-5-3-5",
              "text": "Train models using Google Colab free GPUs",
              "completed": false
            },
            {
              "id": "check-5-3-6",
              "text": "Deploy fine-tuned open-source models",
              "completed": false
            },
            {
              "id": "check-5-3-7",
              "text": "Compare costs: OpenAI vs self-hosted models",
              "completed": false
            },
            {
              "id": "check-5-3-8",
              "text": "Implement model serving infrastructure",
              "completed": false
            },
            {
              "id": "check-5-3-9",
              "text": "Monitor model performance in production",
              "completed": false
            },
            {
              "id": "check-5-3-10",
              "text": "Explore advanced techniques (DPO, RLHF)",
              "completed": false
            }
          ],
          "timestamps": [
            {
              "time": "0:07:45",
              "label": "Course Introduction",
              "description": "Introduction to advanced fine-tuning techniques; Exploring open-source model fine-tuning"
            },
            {
              "time": "0:08:18",
              "label": "Environment Setup",
              "description": "Working with Meta's LLAMA models; Fine-tuning with limited resources"
            },
            {
              "time": "0:10:00",
              "label": "LoRA Technique",
              "description": "Low-Rank Adaptation for efficient fine-tuning"
            },
            {
              "time": "0:15:30",
              "label": "API Configuration",
              "description": "Quantized LoRA for memory efficiency"
            },
            {
              "time": "0:22:00",
              "label": "Dataset Preparation",
              "description": "Preparing data for LLAMA fine-tuning"
            },
            {
              "time": "0:28:45",
              "label": "Training Configuration",
              "description": "Learn family and source implementation techniques"
            },
            {
              "time": "0:35:20",
              "label": "Google Colab Setup",
              "description": "Using free GPU resources for training"
            },
            {
              "time": "0:42:00",
              "label": "Training Process Demo",
              "description": "Learn family and source implementation techniques"
            },
            {
              "time": "0:48:30",
              "label": "Monitoring Training",
              "description": "Tracking loss and performance metrics"
            },
            {
              "time": "0:55:00",
              "label": "Model Evaluation",
              "description": "Testing the fine-tuned LLAMA model"
            },
            {
              "time": "1:02:00",
              "label": "Deployment Options",
              "description": "Deploy family and source to production environment"
            },
            {
              "time": "1:08:15",
              "label": "Cost Comparison",
              "description": "Learn family and source implementation techniques"
            },
            {
              "time": "1:14:00",
              "label": "Advanced Techniques",
              "description": "DPO, RLHF, and other methods"
            }
          ],
          "videoPath": "cohort_2/week_5_class_3_2024-06-20.mp4"
        }
      ]
    },
    {
      "id": "week-6",
      "title": "Week 6: Agents & Orchestration",
      "description": "Explore multi-agent systems, communication patterns, and workflow automation using LangGraph.",
      "lessons": [
        {
          "id": "lesson-6-1",
          "title": "Multi-Agent Systems Design",
          "duration": "1:32:00",
          "videoUrl": "/videos/cohort_2/week_06/week_6_class_1_2024-06-24.mp4",
          "videoId": "week_6_class_1_2024-06-24",
          "content": "# Multi-Agent Systems Design\n\n## Overview\n\nThis lesson introduces the fundamentals of multi-agent systems using LangGraph, exploring how to design, implement, and coordinate multiple AI agents working together to solve complex problems.\n\n## LangGraph Architecture\n\n### Core Components\n\n**Nodes and Edges**\n- Nodes represent functions or agents that process data\n- Edges define the flow and connections between nodes\n- State maintains information across the graph execution\n\n**State Management**\n- Shared state allows agents to coordinate and communicate\n- State persists throughout the workflow execution\n- Enables checkpoints and breakpoints for human-in-the-loop interactions\n\n### Graph vs DAG vs Chains\n\n**When to Use Each:**\n- **Tools**: Simple, single-purpose functions\n- **Chains**: Sequential execution, highly composable\n- **Agents**: Dynamic decision-making with environment interaction\n- **Graphs**: Complex multi-agent coordination with state management\n\n## Multi-Agent Collaboration Patterns\n\n### Collaborative Multi-Agent Systems\n- Multiple agents working together without hierarchy\n- Shared state and communication protocols\n- Example: Research report generation with researcher, chart generator, and router agents\n\n### Supervisor-Based Systems\n- Central supervisor agent coordinates sub-agents\n- Clear hierarchy with reporting structure\n- Example: Development team with supervisor managing coder and researcher agents\n\n### Hierarchical Team Structures\n- Multiple levels of supervision and coordination\n- Teams of agents with their own supervisors\n- Example: Research teams and writing teams coordinated by a main supervisor\n\n## Use Cases and Applications\n\n### Knowledge Representation\n- Complex workflows requiring multiple specialized agents\n- Systems where visualization of dependencies is important\n- Applications needing clear understanding of process flow\n\n### Developer Productivity\n- Code generation and review systems\n- Unit testing automation\n- Documentation generation workflows\n\n### Research and Analysis\n- Multi-step research processes\n- Data analysis and visualization\n- Report generation with multiple inputs\n\n## Safety and Security Considerations\n\n### Code Execution Risks\n- Arbitrary code execution requires careful sandboxing\n- Use Docker containers or isolated environments\n- Implement proper validation and error handling\n\n### Human-in-the-Loop\n- Checkpoints for critical decision points\n- Breakpoints for validation and approval\n- State persistence for long-running processes\n\n## Best Practices\n\n### When to Use Multi-Agent Systems\n- When you would need a team of humans for the same task\n- Complex problems requiring specialized expertise\n- Systems requiring coordination and collaboration\n\n### Design Principles\n- Clear role definition for each agent\n- Well-defined communication protocols\n- Proper error handling and recovery mechanisms\n- Monitoring and observability throughout the system",
          "codeExamples": [
            {
              "id": "code-6-1-1",
              "title": "Basic LangGraph Setup",
              "language": "python",
              "code": "from langgraph.graph import StateGraph, END\nfrom langchain_core.messages import HumanMessage\nfrom typing import TypedDict, List\n\nclass GraphState(TypedDict):\n    messages: List[HumanMessage]\n    \n# Create workflow\nworkflow = StateGraph(GraphState)\n\n# Add nodes\nworkflow.add_node(\"agent\", agent_node)\nworkflow.add_node(\"tools\", tool_node)\n\n# Set entry point\nworkflow.set_entry_point(\"agent\")\n\n# Add edges\nworkflow.add_conditional_edges(\n    \"agent\",\n    should_continue,\n    {\n        \"continue\": \"tools\",\n        \"end\": END\n    }\n)\n\nworkflow.add_edge(\"tools\", \"agent\")\n\n# Compile the graph\napp = workflow.compile()"
            },
            {
              "id": "code-6-1-2",
              "title": "Multi-Agent System Configuration",
              "language": "python",
              "code": "# Create agents with different roles\ndef create_agent(llm, tools, system_prompt):\n    prompt = ChatPromptTemplate.from_messages([\n        (\"system\", system_prompt),\n        MessagesPlaceholder(variable_name=\"messages\"),\n        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n    ])\n    \n    agent = create_openai_functions_agent(llm, tools, prompt)\n    executor = AgentExecutor(agent=agent, tools=tools)\n    return executor\n\n# Define team members\nmembers = [\"researcher\", \"coder\", \"chart_generator\"]\n\n# Create supervisor\nsupervisor_node = create_team_supervisor(llm, members)\n\n# Add all nodes to workflow\nfor member in members:\n    workflow.add_node(member, agent_node)\n\nworkflow.add_node(\"supervisor\", supervisor_node)"
            }
          ],
          "checklist": [
            {
              "id": "check-6-1-1",
              "text": "Understand the components of LangGraph (nodes, edges, state)",
              "completed": false
            },
            {
              "id": "check-6-1-2",
              "text": "Implement a basic multi-agent workflow",
              "completed": false
            },
            {
              "id": "check-6-1-3",
              "text": "Configure agent roles and responsibilities",
              "completed": false
            },
            {
              "id": "check-6-1-4",
              "text": "Set up conditional routing between agents",
              "completed": false
            },
            {
              "id": "check-6-1-5",
              "text": "Test agent communication and coordination",
              "completed": false
            },
            {
              "id": "check-6-1-6",
              "text": "Add error handling and validation to the system",
              "completed": false
            },
            {
              "id": "check-6-1-7",
              "text": "Monitor multi-agent system performance",
              "completed": false
            }
          ],
          "timestamps": [
            {
              "time": "0:00:16",
              "label": "Course Introduction",
              "description": "Welcome to Week 6 multi-agent systems with LangGraph"
            },
            {
              "time": "0:05:00",
              "label": "LangGraph Architecture",
              "description": "Understanding nodes, edges, and state management fundamentals"
            },
            {
              "time": "0:12:00",
              "label": "Core Components Deep Dive",
              "description": "Exploring nodes as agents and edges as connections"
            },
            {
              "time": "0:18:30",
              "label": "State Management Patterns",
              "description": "Shared state persistence and checkpoint mechanisms"
            },
            {
              "time": "0:25:00",
              "label": "Graph vs DAG vs Chains",
              "description": "Choosing the right architecture for your use case"
            },
            {
              "time": "0:32:00",
              "label": "Collaborative Multi-Agent Systems",
              "description": "Building agents that work together without hierarchy"
            },
            {
              "time": "0:39:00",
              "label": "Research Report Demo",
              "description": "Live example with researcher and chart generator agents"
            },
            {
              "time": "0:46:30",
              "label": "Supervisor-Based Architecture",
              "description": "Implementing central coordinator patterns for agent teams"
            },
            {
              "time": "0:53:00",
              "label": "Hierarchical Team Structures",
              "description": "Building multi-level supervision with nested teams"
            },
            {
              "time": "1:00:00",
              "label": "Developer Productivity Use Cases",
              "description": "Code generation, testing, and documentation workflows"
            },
            {
              "time": "1:07:30",
              "label": "Research and Analysis Applications",
              "description": "Multi-step research with data visualization agents"
            },
            {
              "time": "1:14:00",
              "label": "Safety and Security",
              "description": "Sandboxing code execution and validation strategies"
            },
            {
              "time": "1:20:00",
              "label": "Human-in-the-Loop Patterns",
              "description": "Implementing checkpoints and approval workflows"
            },
            {
              "time": "1:26:00",
              "label": "Best Practices",
              "description": "Design principles and monitoring strategies"
            },
            {
              "time": "1:30:00",
              "label": "Q&A and Wrap-up",
              "description": "Student questions and next week preview"
            }
          ],
          "videoPath": "cohort_2/week_6_class_1_2024-06-24.mp4"
        },
        {
          "id": "lesson-6-2",
          "title": "Agent Communication & Coordination",
          "duration": "1:22:07",
          "videoUrl": "/videos/cohort_2/week_06/week_6_class_2_2024-06-25.mp4",
          "videoId": "week_6_class_2_2024-06-25",
          "content": "# Agent Communication & Coordination\n\n## Overview\n\nThis lesson focuses on implementing effective communication patterns and coordination mechanisms between multiple AI agents in a system, ensuring smooth collaboration and information sharing.\n\n## Communication Patterns\n\n### Message Passing\n- Structured communication between agents\n- State updates and information sharing\n- Event-driven coordination mechanisms\n\n### Shared State Management\n- Centralized state for agent coordination\n- Consistent data access across agents\n- State synchronization and conflict resolution\n\n### Protocol Design\n- Standardized communication formats\n- Error handling and retry mechanisms\n- Timeout and failure management\n\n## Coordination Mechanisms\n\n### Workflow Orchestration\n- Sequential and parallel agent execution\n- Conditional routing based on results\n- Dynamic workflow adaptation\n\n### Resource Management\n- Agent availability and load balancing\n- Task queue management\n- Priority-based scheduling\n\n### Conflict Resolution\n- Handling competing agent decisions\n- Consensus mechanisms\n- Fallback strategies\n\n## Implementation Strategies\n\n### LangGraph Communication\n- Node-to-node message passing\n- State-based coordination\n- Event-driven workflows\n\n### API-Based Coordination\n- RESTful agent interfaces\n- Webhook notifications\n- Real-time communication channels\n\n### Database-Mediated Communication\n- Shared data stores\n- Transaction management\n- Consistency guarantees\n\n## Monitoring and Observability\n\n### Communication Tracking\n- Message flow visualization\n- Performance metrics\n- Error rate monitoring\n\n### Agent Health Monitoring\n- Availability tracking\n- Response time measurement\n- Resource utilization monitoring\n\n### System-Wide Observability\n- End-to-end workflow tracking\n- Bottleneck identification\n- Performance optimization\n\n## Advanced Patterns\n\n### Event-Driven Architecture\n- Asynchronous communication\n- Event sourcing patterns\n- Reactive agent behaviors\n\n### Publish-Subscribe Models\n- Decoupled agent communication\n- Topic-based message routing\n- Scalable notification systems\n\n### Circuit Breaker Patterns\n- Fault tolerance mechanisms\n- Graceful degradation\n- Recovery strategies",
          "codeExamples": [
            {
              "id": "code-6-2-1",
              "title": "Basic LangGraph Setup",
              "language": "python",
              "code": "from langgraph.graph import StateGraph, END\nfrom langchain_core.messages import HumanMessage\nfrom typing import TypedDict, List\n\nclass GraphState(TypedDict):\n    messages: List[HumanMessage]\n    \n# Create workflow\nworkflow = StateGraph(GraphState)\n\n# Add nodes\nworkflow.add_node(\"agent\", agent_node)\nworkflow.add_node(\"tools\", tool_node)\n\n# Set entry point\nworkflow.set_entry_point(\"agent\")\n\n# Add edges\nworkflow.add_conditional_edges(\n    \"agent\",\n    should_continue,\n    {\n        \"continue\": \"tools\",\n        \"end\": END\n    }\n)\n\nworkflow.add_edge(\"tools\", \"agent\")\n\n# Compile the graph\napp = workflow.compile()"
            },
            {
              "id": "code-6-2-2",
              "title": "Multi-Agent System Configuration",
              "language": "python",
              "code": "# Create agents with different roles\ndef create_agent(llm, tools, system_prompt):\n    prompt = ChatPromptTemplate.from_messages([\n        (\"system\", system_prompt),\n        MessagesPlaceholder(variable_name=\"messages\"),\n        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n    ])\n    \n    agent = create_openai_functions_agent(llm, tools, prompt)\n    executor = AgentExecutor(agent=agent, tools=tools)\n    return executor\n\n# Define team members\nmembers = [\"researcher\", \"coder\", \"chart_generator\"]\n\n# Create supervisor\nsupervisor_node = create_team_supervisor(llm, members)\n\n# Add all nodes to workflow\nfor member in members:\n    workflow.add_node(member, agent_node)\n\nworkflow.add_node(\"supervisor\", supervisor_node)"
            }
          ],
          "checklist": [
            {
              "id": "check-6-2-1",
              "text": "Understand the components of LangGraph (nodes, edges, state)",
              "completed": false
            },
            {
              "id": "check-6-2-2",
              "text": "Implement a basic multi-agent workflow",
              "completed": false
            },
            {
              "id": "check-6-2-3",
              "text": "Configure agent roles and responsibilities",
              "completed": false
            },
            {
              "id": "check-6-2-4",
              "text": "Set up conditional routing between agents",
              "completed": false
            },
            {
              "id": "check-6-2-5",
              "text": "Test agent communication and coordination",
              "completed": false
            },
            {
              "id": "check-6-2-6",
              "text": "Add error handling and validation to the system",
              "completed": false
            },
            {
              "id": "check-6-2-7",
              "text": "Monitor multi-agent system performance",
              "completed": false
            }
          ],
          "timestamps": [
            {
              "time": "0:00:19",
              "label": "LangGraph Code Development",
              "description": "Introduction to LangGraph code development using multi-agent flow"
            },
            {
              "time": "0:01:32",
              "label": "Code Agent Implementation",
              "description": "Creating a single code agent with role, task, and instructions"
            },
            {
              "time": "0:01:57",
              "label": "Tester Agent Creation",
              "description": "Building a testing agent to write test cases for generated code"
            },
            {
              "time": "0:03:59",
              "label": "Code Generation Demo",
              "description": "Generating Fibonacci sequence code with the programmer agent"
            },
            {
              "time": "0:05:00",
              "label": "Test Case Generation",
              "description": "Creating test cases with inputs and expected outputs"
            },
            {
              "time": "0:06:03",
              "label": "Executable Code Creator",
              "description": "Building an executor to run tests and check for errors"
            },
            {
              "time": "0:07:15",
              "label": "Error Detection",
              "description": "Test failure on negative input edge case"
            },
            {
              "time": "0:07:58",
              "label": "API Configuration",
              "description": "Creating debugger agent to fix code based on test failures"
            },
            {
              "time": "0:08:40",
              "label": "Code Refinement",
              "description": "Debugger fixes edge cases and validates input types"
            },
            {
              "time": "0:09:50",
              "label": "Test-Driven Development",
              "description": "Discussion of TDD approach with multiple agents"
            },
            {
              "time": "0:11:30",
              "label": "Prompt Engineering Insights",
              "description": "Importance of specific prompts for agent behavior"
            },
            {
              "time": "0:20:15",
              "label": "Graph State Management",
              "description": "Creating agent state dictionary for workflow coordination"
            },
            {
              "time": "0:21:47",
              "label": "Building the State Graph",
              "description": "Constructing LangGraph workflow with nodes and edges"
            },
            {
              "time": "0:22:45",
              "label": "Conditional Edges",
              "description": "Adding decision logic for workflow branching"
            },
            {
              "time": "0:28:47",
              "label": "Two-Sum Problem Demo",
              "description": "Running complete multi-agent workflow on LeetCode problem"
            },
            {
              "time": "0:36:50",
              "label": "Debugging Workflow Errors",
              "description": "Troubleshooting validation errors in the graph workflow"
            },
            {
              "time": "0:40:30",
              "label": "Recursion Limits",
              "description": "Managing iteration limits to prevent infinite loops"
            },
            {
              "time": "0:43:20",
              "label": "Performance Optimization",
              "description": "Discussion on GPU usage and execution speed"
            },
            {
              "time": "0:48:15",
              "label": "API Configuration",
              "description": "Set up endpoints and authentication for shared and agent"
            },
            {
              "time": "0:50:10",
              "label": "OpenDevin Agent Hub",
              "description": "Exploring available agents and their capabilities"
            },
            {
              "time": "0:52:30",
              "label": "Docker Setup",
              "description": "Running OpenDevin in containerized environment"
            },
            {
              "time": "0:55:40",
              "label": "NestJS API Creation",
              "description": "Building Hello World API with OpenDevin"
            },
            {
              "time": "0:58:20",
              "label": "Automatic Dependencies",
              "description": "OpenDevin installing Node.js and tools autonomously"
            },
            {
              "time": "1:01:45",
              "label": "Permission Handling",
              "description": "Agent using sudo to resolve permission issues"
            },
            {
              "time": "1:05:30",
              "label": "Component Generation",
              "description": "Creating controllers, services, and modules"
            },
            {
              "time": "1:10:00",
              "label": "Real-World Applications",
              "description": "Refactoring and migration use cases"
            },
            {
              "time": "1:15:20",
              "label": "Cost Analysis",
              "description": "Token usage and pricing considerations"
            },
            {
              "time": "1:18:30",
              "label": "Best Practices Q&A",
              "description": "When to use agent systems in production"
            },
            {
              "time": "1:21:00",
              "label": "Session Wrap-up",
              "description": "Summary and next steps for multi-agent development"
            }
          ],
          "videoPath": "cohort_2/week_6_class_2_2024-06-25.mp4"
        },
        {
          "id": "lesson-6-3",
          "title": "Workflow Automation Implementation",
          "duration": "1:29:00",
          "videoUrl": "/videos/cohort_2/week_06/week_6_class_3_2024-06-26.mp4",
          "videoId": "week_6_class_3_2024-06-26",
          "content": "# Workflow Automation Implementation\n\n## Overview\n\nThis lesson demonstrates how to implement and deploy multi-agent systems as production-ready APIs, focusing on workflow automation, API design, and integration patterns for real-world applications.\n\n## API Design for Multi-Agent Systems\n\n### RESTful Agent Endpoints\n- Clean API abstractions for complex agent workflows\n- Request/response patterns for agent interactions\n- Authentication and authorization mechanisms\n\n### Workflow Orchestration APIs\n- Start, pause, and resume workflow execution\n- Status monitoring and progress tracking\n- Result retrieval and error handling\n\n### Integration Patterns\n- Webhook notifications for workflow events\n- Batch processing capabilities\n- Real-time streaming interfaces\n\n## Production Deployment\n\n### Containerization\n- Docker-based agent deployment\n- Kubernetes orchestration\n- Scalability and load balancing\n\n### Security Considerations\n- API token management\n- Rate limiting and throttling\n- Input validation and sanitization\n\n### Monitoring and Logging\n- Comprehensive workflow logging\n- Performance metrics collection\n- Error tracking and alerting\n\n## Automation Patterns\n\n### Event-Driven Workflows\n- Trigger-based automation\n- Scheduled execution patterns\n- Reactive workflow adaptation\n\n### Code Generation Workflows\n- Automated development tasks\n- Testing and validation pipelines\n- Documentation generation\n\n### Data Processing Pipelines\n- ETL automation with agents\n- Real-time data processing\n- Batch processing workflows\n\n## Integration with Development Tools\n\n### Git Integration\n- Pull request automation\n- Code review assistance\n- Deployment pipeline integration\n\n### CI/CD Integration\n- Automated testing workflows\n- Build pipeline enhancement\n- Quality assurance automation\n\n### Development Environment Integration\n- IDE plugin development\n- Local development support\n- Team collaboration tools\n\n## Practical Applications\n\n### Developer Productivity Tools\n- Code generation assistants\n- Documentation automation\n- Testing and validation tools\n\n### Business Process Automation\n- Report generation\n- Data analysis workflows\n- Customer service automation\n\n### Research and Analysis Tools\n- Literature review automation\n- Data collection and analysis\n- Visualization and reporting\n\n## Best Practices\n\n### Workflow Design\n- Clear separation of concerns\n- Modular and reusable components\n- Proper error handling and recovery\n\n### Performance Optimization\n- Caching strategies\n- Parallel execution patterns\n- Resource management\n\n### Maintenance and Operations\n- Version control for workflows\n- A/B testing capabilities\n- Gradual rollout strategies",
          "codeExamples": [
            {
              "id": "code-6-3-1",
              "title": "Basic LangGraph Setup",
              "language": "python",
              "code": "from langgraph.graph import StateGraph, END\nfrom langchain_core.messages import HumanMessage\nfrom typing import TypedDict, List\n\nclass GraphState(TypedDict):\n    messages: List[HumanMessage]\n    \n# Create workflow\nworkflow = StateGraph(GraphState)\n\n# Add nodes\nworkflow.add_node(\"agent\", agent_node)\nworkflow.add_node(\"tools\", tool_node)\n\n# Set entry point\nworkflow.set_entry_point(\"agent\")\n\n# Add edges\nworkflow.add_conditional_edges(\n    \"agent\",\n    should_continue,\n    {\n        \"continue\": \"tools\",\n        \"end\": END\n    }\n)\n\nworkflow.add_edge(\"tools\", \"agent\")\n\n# Compile the graph\napp = workflow.compile()"
            },
            {
              "id": "code-6-3-2",
              "title": "Multi-Agent System Configuration",
              "language": "python",
              "code": "# Create agents with different roles\ndef create_agent(llm, tools, system_prompt):\n    prompt = ChatPromptTemplate.from_messages([\n        (\"system\", system_prompt),\n        MessagesPlaceholder(variable_name=\"messages\"),\n        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n    ])\n    \n    agent = create_openai_functions_agent(llm, tools, prompt)\n    executor = AgentExecutor(agent=agent, tools=tools)\n    return executor\n\n# Define team members\nmembers = [\"researcher\", \"coder\", \"chart_generator\"]\n\n# Create supervisor\nsupervisor_node = create_team_supervisor(llm, members)\n\n# Add all nodes to workflow\nfor member in members:\n    workflow.add_node(member, agent_node)\n\nworkflow.add_node(\"supervisor\", supervisor_node)"
            }
          ],
          "checklist": [
            {
              "id": "check-6-3-1",
              "text": "Understand the components of LangGraph (nodes, edges, state)",
              "completed": false
            },
            {
              "id": "check-6-3-2",
              "text": "Implement a basic multi-agent workflow",
              "completed": false
            },
            {
              "id": "check-6-3-3",
              "text": "Configure agent roles and responsibilities",
              "completed": false
            },
            {
              "id": "check-6-3-4",
              "text": "Set up conditional routing between agents",
              "completed": false
            },
            {
              "id": "check-6-3-5",
              "text": "Test agent communication and coordination",
              "completed": false
            },
            {
              "id": "check-6-3-6",
              "text": "Add error handling and validation to the system",
              "completed": false
            },
            {
              "id": "check-6-3-7",
              "text": "Monitor multi-agent system performance",
              "completed": false
            }
          ],
          "timestamps": [
            {
              "time": "0:00:10",
              "label": "Session Introduction",
              "description": "Welcome to workflow automation implementation class"
            },
            {
              "time": "0:04:00",
              "label": "CrewAI Framework Overview",
              "description": "Introduction to CrewAI for multi-agent orchestration"
            },
            {
              "time": "0:10:30",
              "label": "Agent Role Definition",
              "description": "Creating specialized agents with clear responsibilities"
            },
            {
              "time": "0:17:00",
              "label": "Task Management System",
              "description": "Defining and assigning tasks to agents effectively"
            },
            {
              "time": "0:24:00",
              "label": "Workflow Configuration",
              "description": "Setting up sequential and parallel execution patterns"
            },
            {
              "time": "0:31:00",
              "label": "Live Coding: Basic Workflow",
              "description": "Building a simple multi-agent workflow from scratch"
            },
            {
              "time": "0:38:30",
              "label": "Agent Communication",
              "description": "Implementing inter-agent messaging and data sharing"
            },
            {
              "time": "0:45:00",
              "label": "Context Management",
              "description": "Maintaining shared context across agent interactions"
            },
            {
              "time": "0:52:00",
              "label": "Error Handling Strategies",
              "description": "Implementing robust error recovery in workflows"
            },
            {
              "time": "0:59:00",
              "label": "Advanced Workflow Patterns",
              "description": "Conditional routing and dynamic agent selection"
            },
            {
              "time": "1:06:00",
              "label": "Production Deployment",
              "description": "Deploying multi-agent systems to production"
            },
            {
              "time": "1:13:00",
              "label": "Monitoring and Debugging",
              "description": "Tools and techniques for workflow observability"
            },
            {
              "time": "1:19:30",
              "label": "Performance Optimization",
              "description": "Scaling workflows and reducing latency"
            },
            {
              "time": "1:25:00",
              "label": "Course Recap",
              "description": "Key takeaways and homework assignment"
            }
          ],
          "videoPath": "cohort_2/week_6_class_3_2024-06-26.mp4"
        }
      ]
    },
    {
      "id": "week-7",
      "title": "Week 7: Production Systems Architecture",
      "description": "Deploy AI systems to production with enterprise-grade architecture and scaling strategies.",
      "lessons": [
        {
          "id": "lesson-7-1",
          "title": "Production Systems Architecture",
          "duration": "1:28:00",
          "videoUrl": "/videos/cohort_2/week_07/week_7_class_1_2024-07-08.mp4",
          "videoId": "week_7_class_1_2024-07-08",
          "content": "# Production Systems Architecture\n\n## Advanced Agent Workflows and Human-in-the-Loop Systems\n\nThis lesson explores sophisticated agent architectures, human-in-the-loop design patterns, and advanced RAG techniques for production deployments.\n\n## When to Use Multi-Agent Architectures\n\n### Multi-Agent Use Cases\n\nMulti-agent architectures are best suited for:\n- Complex tasks requiring multiple personas or expertise areas\n- Problems that would take a real-world team to solve\n- Parallel processing of multiple subtasks\n- Systems requiring diverse specialized knowledge\n\n### Architecture Complexity Spectrum\n\n**From Simple to Complex:**\n1. **Chains**: Simplest, structured output, defined task sequence\n2. **Single Agent**: Moderate complexity, single problem domain\n3. **Multi-Agent**: Most complex, multiple domains, parallel processing\n\n### Performance Considerations\n\nMulti-agent systems excel when:\n- Tasks can be parallelized across agents\n- Different expertise areas are required\n- Complex reasoning across domains is needed\n- The underlying use case justifies the implementation complexity\n\n## Human-in-the-Loop Design Patterns\n\n### Core Concepts\n\nHuman-in-the-loop (HITL) systems incorporate human feedback to improve agent performance over time. This creates a dynamic learning environment where the system becomes more confident and accurate through iterative feedback.\n\n### Feedback Loop Architecture\n\n**Process Flow:**\n1. User query enters the agent workflow\n2. Model generates initial response with confidence level\n3. Human reviews output and provides feedback\n4. Feedback updates the model's confidence for similar queries\n5. System becomes more accurate over time\n\n### Real-World Applications\n\n**Google Maps AI Integration:**\n- Interactive feedback for location recommendations\n- User preferences shape future suggestions\n- Dynamic adaptation to individual needs\n\n**Email Management (Her Movie Example):**\n- AI learns email filtering preferences\n- Gradual confidence building through user feedback\n- Boundary setting through minimal corrections\n\n### Implementation Challenges\n\n**Multi-Agent Feedback Conflicts:**\n- Agents may disagree based on different prompting\n- Human feedback resolves conflicts and sets priorities\n- Prevents agents from diverging down unintended paths\n\n**UI/UX Complexity:**\n- Custom interfaces required for feedback collection\n- Notification mechanisms for human intervention\n- Balance between automation and human control\n\n### Benefits of Human Oversight\n\n- Iterative improvement for complex problems\n- Early error correction when models lack confidence\n- Contextual refinement based on domain expertise\n- Elimination of faulty autonomous planning\n\n## DYLAN Framework: Dynamic LLM Agent Networks\n\n### Overview\n\nDYLAN creates networks of dynamic agents that collaborate, share knowledge, and evolve their strategies based on performance metrics.\n\n### Core Architecture\n\n**Key Components:**\n- Modular agent design for different domains\n- Dynamic strategy adjustment in real-time\n- Peer rating system for performance evaluation\n- Knowledge sharing without centralized control\n\n### DYLAN Process Steps\n\n1. **Agent Creation**: Initialize specialized agents for the task\n2. **Collaboration**: Agents contribute expertise to execution\n3. **Performance Assessment**: Evaluate agent effectiveness each round\n4. **Selection**: Choose top performers for next iteration\n5. **Knowledge Sharing**: Distribute insights across network\n6. **Iteration**: Continuous refinement and optimization\n\n### Performance Metrics\n\n- 13% improvement on MATH and HumanEval datasets vs GPT-3.5\n- Up to 25% increase in agent team optimization\n- Significant practical improvements in real-world applications\n\n### Healthcare Use Case Example\n\n**Medical Image Diagnostics:**\n- Specialized agents for different imaging types (MRI, X-ray, ultrasound)\n- Peer review of diagnostic assessments\n- Knowledge propagation improves network performance\n- Top performers' insights enhance overall accuracy\n\n## HyDE: Hypothetical Document Embeddings\n\n### Concept\n\nHyDE enhances query processing by generating hypothetical additional context, improving response quality for underspecified queries.\n\n### How HyDE Works\n\n**Process:**\n1. Analyze user's original query\n2. Generate hypothetical document embeddings\n3. Augment query with additional context\n4. Retrieve more relevant information\n5. Generate improved response\n\n### Examples\n\n**Original Query \u2192 HyDE Enhancement:**\n- \"Coffee shops\" \u2192 \"Find coffee shops within 5 miles suitable for working\"\n- \"Wisdom teeth removal time?\" \u2192 \"Write medical passage about wisdom teeth extraction duration\"\n- Engineering question \u2192 Add domain-specific technical context\n\n### Implementation Requirements\n\n- Fine-tuning required for context generation\n- Labeled dataset for training augmentation patterns\n- Gold standard additional context examples\n- Continuous improvement through usage\n\n### Benefits\n\n- Reduces inaccuracy from vague queries\n- Expands application domains without multiple models\n- Improves response relevance and quality\n- Handles hyper-specific technical queries better\n\n## Advanced RAG Techniques\n\n### Corrective RAG\n\n**Architecture:**\n1. Query \u2192 Retrieve documents\n2. Grade retrieval quality\n3. Check for irrelevant documents\n4. If irrelevant found, rewrite query\n5. Iterate until all documents relevant\n6. Generate final answer\n\n**Use Cases:**\n- Legal compliance (accurate case law retrieval)\n- Healthcare (medically accurate reports)\n- High-stakes decision support systems\n\n**Key Features:**\n- Self-reflexive quality assessment\n- Iterative refinement of retrieval\n- No fine-tuning required\n- Focus on retrieval accuracy\n\n### Adaptive RAG\n\n**Architecture:**\n1. Analyze query first (not retrieve)\n2. Determine if query matches index content\n3. If unrelated: Search web \u2192 Generate context \u2192 Answer\n4. If related: Use Corrective RAG flow\n5. Check for hallucinations\n6. Verify answer completeness\n7. Deliver final response\n\n**Advanced Features:**\n- Query routing based on content analysis\n- Web search fallback for unknown topics\n- Hallucination detection and correction\n- Answer validation before delivery\n\n**Benefits:**\n- Handles queries outside knowledge base\n- Reduces hallucinations significantly\n- Provides comprehensive answer validation\n- Adapts to different query types dynamically\n\n### Comparison of RAG Approaches\n\n**Vanilla RAG:**\n- Simplest implementation\n- Direct retrieve \u2192 generate flow\n- Limited accuracy improvements\n\n**Corrective RAG:**\n- Moderate complexity\n- Iterative refinement\n- Higher contextual accuracy\n- Good for single-domain applications\n\n**Adaptive RAG:**\n- High complexity\n- Query analysis and routing\n- Multiple fallback strategies\n- Best accuracy across domains\n\n**HyDE:**\n- Requires fine-tuning\n- Query augmentation focus\n- Excellent for vague queries\n- Complements other RAG approaches\n\n## Production Deployment Considerations\n\n### Scaling Strategies\n\n- Use frameworks like DYLAN for complex multi-agent systems\n- Implement gradual rollout with feedback loops\n- Monitor performance metrics continuously\n- Design for modularity and extensibility\n\n### Cost Management\n\n- Balance model complexity with operational costs\n- Use adaptive techniques to minimize unnecessary processing\n- Implement caching for common query patterns\n- Monitor token usage across agent networks\n\n### Monitoring and Observability\n\n- Track agent performance individually\n- Monitor feedback loop effectiveness\n- Measure query routing accuracy\n- Analyze hallucination rates and corrections\n\n### Best Practices\n\n1. Start with simple architectures, evolve based on needs\n2. Implement comprehensive error handling\n3. Design clear human intervention points\n4. Maintain audit trails for compliance\n5. Use A/B testing for architecture improvements\n6. Document agent behaviors and decision paths\n7. Implement graceful degradation strategies",
          "codeExamples": [
            {
              "id": "code-7-1-1",
              "title": "Human-in-the-Loop Feedback System",
              "language": "python",
              "code": "from typing import Dict, List, Optional\nfrom dataclasses import dataclass\nfrom datetime import datetime\n\n@dataclass\nclass FeedbackEntry:\n    query: str\n    response: str\n    confidence: float\n    human_feedback: Optional[str] = None\n    rating: Optional[int] = None\n    timestamp: datetime = None\n    \n    def __post_init__(self):\n        if self.timestamp is None:\n            self.timestamp = datetime.now()\n\nclass HumanInLoopSystem:\n    def __init__(self):\n        self.feedback_history: List[FeedbackEntry] = []\n        self.confidence_thresholds = {\n            'low': 0.3,\n            'medium': 0.7,\n            'high': 0.9\n        }\n    \n    def generate_response(self, query: str, llm_response: str, confidence: float):\n        \"\"\"Generate response with confidence tracking\"\"\"\n        entry = FeedbackEntry(\n            query=query,\n            response=llm_response,\n            confidence=confidence\n        )\n        \n        # Determine if human review needed\n        if confidence < self.confidence_thresholds['medium']:\n            return self.request_human_review(entry)\n        \n        self.feedback_history.append(entry)\n        return llm_response\n    \n    def request_human_review(self, entry: FeedbackEntry):\n        \"\"\"Simulate human review request\"\"\"\n        print(f\"\u26a0\ufe0f Low confidence ({entry.confidence:.2f}) - Requesting human review\")\n        print(f\"Query: {entry.query}\")\n        print(f\"Proposed response: {entry.response}\")\n        \n        # In production, this would trigger UI notification\n        # For demo, we'll simulate approval\n        entry.human_feedback = \"Approved with modifications\"\n        entry.rating = 4\n        \n        self.feedback_history.append(entry)\n        self.update_confidence_model(entry)\n        \n        return entry.response\n    \n    def update_confidence_model(self, entry: FeedbackEntry):\n        \"\"\"Update confidence based on human feedback\"\"\"\n        # In production, this would update model weights or fine-tuning dataset\n        similar_queries = self.find_similar_queries(entry.query)\n        \n        for similar in similar_queries:\n            if entry.rating >= 4:\n                similar.confidence = min(1.0, similar.confidence * 1.1)\n            else:\n                similar.confidence = max(0.0, similar.confidence * 0.9)\n    \n    def find_similar_queries(self, query: str) -> List[FeedbackEntry]:\n        \"\"\"Find similar queries in history\"\"\"\n        # Simplified similarity check - in production use embeddings\n        similar = []\n        query_words = set(query.lower().split())\n        \n        for entry in self.feedback_history:\n            entry_words = set(entry.query.lower().split())\n            if len(query_words & entry_words) > len(query_words) * 0.5:\n                similar.append(entry)\n        \n        return similar\n    \n    def get_feedback_stats(self) -> Dict:\n        \"\"\"Get statistics on feedback and confidence\"\"\"\n        if not self.feedback_history:\n            return {}\n        \n        total = len(self.feedback_history)\n        reviewed = sum(1 for e in self.feedback_history if e.human_feedback)\n        avg_confidence = sum(e.confidence for e in self.feedback_history) / total\n        \n        return {\n            'total_queries': total,\n            'human_reviewed': reviewed,\n            'review_rate': reviewed / total,\n            'average_confidence': avg_confidence,\n            'confidence_trend': self.calculate_confidence_trend()\n        }\n    \n    def calculate_confidence_trend(self) -> str:\n        \"\"\"Calculate if confidence is improving over time\"\"\"\n        if len(self.feedback_history) < 10:\n            return \"insufficient_data\"\n        \n        recent = self.feedback_history[-5:]\n        older = self.feedback_history[-10:-5]\n        \n        recent_avg = sum(e.confidence for e in recent) / len(recent)\n        older_avg = sum(e.confidence for e in older) / len(older)\n        \n        if recent_avg > older_avg * 1.05:\n            return \"improving\"\n        elif recent_avg < older_avg * 0.95:\n            return \"declining\"\n        else:\n            return \"stable\""
            },
            {
              "id": "code-7-1-2",
              "title": "DYLAN Agent Network Implementation",
              "language": "python",
              "code": "import random\nfrom typing import List, Dict, Any\nfrom abc import ABC, abstractmethod\n\nclass DYLANAgent(ABC):\n    \"\"\"Base class for DYLAN network agents\"\"\"\n    \n    def __init__(self, agent_id: str, expertise: str):\n        self.agent_id = agent_id\n        self.expertise = expertise\n        self.performance_score = 0.5\n        self.knowledge_base = {}\n    \n    @abstractmethod\n    def process_task(self, task: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Process task based on agent expertise\"\"\"\n        pass\n    \n    def share_knowledge(self, other_agent: 'DYLANAgent'):\n        \"\"\"Share knowledge with another agent\"\"\"\n        shared_knowledge = {\n            k: v for k, v in self.knowledge_base.items() \n            if random.random() > 0.3  # Share 70% of knowledge\n        }\n        other_agent.receive_knowledge(shared_knowledge)\n    \n    def receive_knowledge(self, knowledge: Dict[str, Any]):\n        \"\"\"Receive and integrate knowledge from other agents\"\"\"\n        self.knowledge_base.update(knowledge)\n    \n    def rate_peer(self, other_agent: 'DYLANAgent', task_result: Dict) -> float:\n        \"\"\"Rate another agent's performance\"\"\"\n        # Simplified rating based on result quality\n        if 'accuracy' in task_result:\n            return task_result['accuracy']\n        return 0.5\n\nclass MedicalImageAgent(DYLANAgent):\n    \"\"\"Specialized agent for medical image analysis\"\"\"\n    \n    def __init__(self, agent_id: str, image_type: str):\n        super().__init__(agent_id, f\"medical_imaging_{image_type}\")\n        self.image_type = image_type\n        self.diagnostic_accuracy = {\n            'xray': 0.85,\n            'mri': 0.90,\n            'ultrasound': 0.80\n        }.get(image_type, 0.75)\n    \n    def process_task(self, task: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Process medical image analysis task\"\"\"\n        if task.get('image_type') != self.image_type:\n            accuracy = self.diagnostic_accuracy * 0.7  # Lower accuracy outside expertise\n        else:\n            accuracy = self.diagnostic_accuracy\n        \n        # Add noise to simulate real performance variation\n        accuracy += random.uniform(-0.1, 0.1)\n        accuracy = max(0, min(1, accuracy))\n        \n        return {\n            'agent_id': self.agent_id,\n            'diagnosis': f\"Analysis by {self.expertise}\",\n            'accuracy': accuracy,\n            'confidence': accuracy * self.performance_score\n        }\n\nclass DYLANNetwork:\n    \"\"\"Dynamic LLM Agent Network coordinator\"\"\"\n    \n    def __init__(self):\n        self.agents: List[DYLANAgent] = []\n        self.iteration_count = 0\n        self.performance_history = []\n    \n    def add_agent(self, agent: DYLANAgent):\n        \"\"\"Add agent to network\"\"\"\n        self.agents.append(agent)\n    \n    def execute_task(self, task: Dict[str, Any], iterations: int = 3):\n        \"\"\"Execute task through network iterations\"\"\"\n        results = []\n        \n        for iteration in range(iterations):\n            iteration_results = self.run_iteration(task)\n            results.append(iteration_results)\n            \n            # Select top performers\n            top_agents = self.select_top_performers(iteration_results)\n            \n            # Knowledge sharing among top performers\n            self.share_knowledge_phase(top_agents)\n            \n            # Update performance scores\n            self.update_performance_scores(iteration_results)\n            \n            self.iteration_count += 1\n        \n        return self.aggregate_results(results)\n    \n    def run_iteration(self, task: Dict[str, Any]) -> List[Dict]:\n        \"\"\"Run single iteration with all agents\"\"\"\n        results = []\n        for agent in self.agents:\n            result = agent.process_task(task)\n            results.append(result)\n        return results\n    \n    def select_top_performers(self, results: List[Dict], top_n: int = None) -> List[DYLANAgent]:\n        \"\"\"Select top performing agents based on results\"\"\"\n        if top_n is None:\n            top_n = max(2, len(self.agents) // 2)\n        \n        # Sort agents by accuracy\n        agent_scores = []\n        for result in results:\n            agent_id = result['agent_id']\n            agent = next(a for a in self.agents if a.agent_id == agent_id)\n            agent_scores.append((agent, result.get('accuracy', 0)))\n        \n        agent_scores.sort(key=lambda x: x[1], reverse=True)\n        return [agent for agent, _ in agent_scores[:top_n]]\n    \n    def share_knowledge_phase(self, top_agents: List[DYLANAgent]):\n        \"\"\"Top agents share knowledge with network\"\"\"\n        for top_agent in top_agents:\n            for other_agent in self.agents:\n                if top_agent != other_agent:\n                    top_agent.share_knowledge(other_agent)\n    \n    def update_performance_scores(self, results: List[Dict]):\n        \"\"\"Update agent performance scores based on peer ratings\"\"\"\n        for agent in self.agents:\n            agent_result = next(\n                (r for r in results if r['agent_id'] == agent.agent_id), \n                None\n            )\n            if agent_result:\n                # Peer rating\n                peer_ratings = []\n                for other_agent in self.agents:\n                    if other_agent != agent:\n                        rating = other_agent.rate_peer(agent, agent_result)\n                        peer_ratings.append(rating)\n                \n                if peer_ratings:\n                    avg_rating = sum(peer_ratings) / len(peer_ratings)\n                    # Update performance score with momentum\n                    agent.performance_score = (\n                        0.7 * agent.performance_score + 0.3 * avg_rating\n                    )\n    \n    def aggregate_results(self, all_results: List[List[Dict]]) -> Dict:\n        \"\"\"Aggregate results across iterations\"\"\"\n        final_accuracy = 0\n        confidence_scores = []\n        \n        for iteration_results in all_results:\n            iteration_accuracy = sum(r.get('accuracy', 0) for r in iteration_results)\n            iteration_accuracy /= len(iteration_results)\n            final_accuracy += iteration_accuracy\n            \n            iteration_confidence = sum(r.get('confidence', 0) for r in iteration_results)\n            confidence_scores.append(iteration_confidence / len(iteration_results))\n        \n        final_accuracy /= len(all_results)\n        \n        return {\n            'final_accuracy': final_accuracy,\n            'confidence_trend': confidence_scores,\n            'iterations': len(all_results),\n            'agent_count': len(self.agents),\n            'top_performer': max(self.agents, key=lambda a: a.performance_score).agent_id\n        }"
            },
            {
              "id": "code-7-1-3",
              "title": "HyDE Implementation",
              "language": "python",
              "code": "from typing import List, Dict, Optional\nimport hashlib\n\nclass HyDEQueryEnhancer:\n    \"\"\"Hypothetical Document Embeddings for query enhancement\"\"\"\n    \n    def __init__(self, llm_client):\n        self.llm = llm_client\n        self.enhancement_cache = {}\n        self.enhancement_templates = {\n            'vague': \"Write a detailed passage that would answer the question: {query}\",\n            'technical': \"Write a technical documentation excerpt explaining: {query}\",\n            'medical': \"Write a medical reference passage about: {query}\",\n            'legal': \"Write a legal document excerpt regarding: {query}\",\n            'general': \"Write an informative passage to answer: {query}\"\n        }\n    \n    def enhance_query(self, \n                      original_query: str, \n                      domain: str = 'general',\n                      use_cache: bool = True) -> Dict[str, any]:\n        \"\"\"Enhance query with hypothetical document context\"\"\"\n        \n        # Check cache\n        query_hash = hashlib.md5(original_query.encode()).hexdigest()\n        if use_cache and query_hash in self.enhancement_cache:\n            return self.enhancement_cache[query_hash]\n        \n        # Analyze query complexity\n        complexity = self.analyze_query_complexity(original_query)\n        \n        # Generate hypothetical document\n        if complexity['needs_enhancement']:\n            enhanced_context = self.generate_hypothetical_doc(\n                original_query, \n                domain,\n                complexity\n            )\n        else:\n            enhanced_context = \"\"\n        \n        result = {\n            'original_query': original_query,\n            'enhanced_context': enhanced_context,\n            'domain': domain,\n            'complexity': complexity,\n            'final_query': self.construct_final_query(original_query, enhanced_context)\n        }\n        \n        # Cache result\n        if use_cache:\n            self.enhancement_cache[query_hash] = result\n        \n        return result\n    \n    def analyze_query_complexity(self, query: str) -> Dict:\n        \"\"\"Analyze if query needs enhancement\"\"\"\n        word_count = len(query.split())\n        has_question_word = any(\n            word in query.lower() \n            for word in ['what', 'how', 'why', 'when', 'where', 'who']\n        )\n        \n        # Simple heuristics for demo\n        needs_enhancement = word_count < 5 or not has_question_word\n        \n        return {\n            'word_count': word_count,\n            'has_question_word': has_question_word,\n            'needs_enhancement': needs_enhancement,\n            'enhancement_level': 'high' if word_count < 3 else 'medium' if word_count < 7 else 'low'\n        }\n    \n    def generate_hypothetical_doc(self, \n                                  query: str, \n                                  domain: str,\n                                  complexity: Dict) -> str:\n        \"\"\"Generate hypothetical document for context\"\"\"\n        \n        # Select template based on domain\n        template = self.enhancement_templates.get(domain, self.enhancement_templates['general'])\n        prompt = template.format(query=query)\n        \n        # Add complexity-based instructions\n        if complexity['enhancement_level'] == 'high':\n            prompt += \" Include comprehensive details and examples.\"\n        elif complexity['enhancement_level'] == 'medium':\n            prompt += \" Provide clear, focused information.\"\n        else:\n            prompt += \" Be concise and direct.\"\n        \n        # Generate hypothetical document using LLM\n        response = self.llm.generate(prompt, max_tokens=200)\n        \n        return response\n    \n    def construct_final_query(self, original: str, enhanced: str) -> str:\n        \"\"\"Construct final enhanced query\"\"\"\n        if not enhanced:\n            return original\n        \n        return f\"\"\"Context: {enhanced}\n        \nQuestion: {original}\n\nPlease provide a detailed answer based on the context above.\"\"\"\n    \n    def batch_enhance(self, queries: List[str], domain: str = 'general') -> List[Dict]:\n        \"\"\"Enhance multiple queries in batch\"\"\"\n        results = []\n        for query in queries:\n            result = self.enhance_query(query, domain)\n            results.append(result)\n        return results\n\n# Example usage demonstration\nclass MockLLM:\n    def generate(self, prompt: str, max_tokens: int = 100) -> str:\n        # Mock LLM response for demonstration\n        if \"coffee\" in prompt.lower():\n            return \"Coffee shops are establishments that primarily serve coffee and related beverages. They often provide WiFi, comfortable seating, and a quiet atmosphere suitable for work or socializing.\"\n        elif \"wisdom teeth\" in prompt.lower():\n            return \"Wisdom teeth removal is a common dental procedure that typically takes 45-60 minutes per tooth. Recovery usually requires 3-4 days for initial healing and up to 2 weeks for complete recovery.\"\n        else:\n            return \"This topic requires detailed explanation with specific context and examples relevant to the query.\"\n\n# Demonstration\ndef demonstrate_hyde():\n    llm = MockLLM()\n    hyde = HyDEQueryEnhancer(llm)\n    \n    # Test cases\n    test_queries = [\n        (\"coffee shops\", \"general\"),\n        (\"How long does it take to remove wisdom teeth?\", \"medical\"),\n        (\"Python async\", \"technical\")\n    ]\n    \n    for query, domain in test_queries:\n        result = hyde.enhance_query(query, domain)\n        print(f\"\\nOriginal: {query}\")\n        print(f\"Domain: {domain}\")\n        print(f\"Needs Enhancement: {result['complexity']['needs_enhancement']}\")\n        if result['enhanced_context']:\n            print(f\"Enhanced Context: {result['enhanced_context'][:100]}...\")"
            },
            {
              "id": "code-7-1-4",
              "title": "Corrective RAG Implementation",
              "language": "python",
              "code": "from typing import List, Dict, Tuple, Optional\nfrom dataclasses import dataclass\nimport time\n\n@dataclass\nclass Document:\n    content: str\n    relevance_score: float = 0.0\n    source: str = \"\"\n    metadata: Dict = None\n\nclass CorrectiveRAG:\n    \"\"\"Corrective RAG with iterative refinement\"\"\"\n    \n    def __init__(self, retriever, llm, max_iterations: int = 3):\n        self.retriever = retriever\n        self.llm = llm\n        self.max_iterations = max_iterations\n        self.relevance_threshold = 0.7\n    \n    def query(self, user_query: str) -> Dict:\n        \"\"\"Process query with corrective RAG\"\"\"\n        iteration = 0\n        query = user_query\n        all_documents = []\n        query_history = [user_query]\n        \n        while iteration < self.max_iterations:\n            # Retrieve documents\n            documents = self.retrieve_documents(query)\n            \n            # Grade document relevance\n            graded_docs = self.grade_documents(documents, query)\n            \n            # Check for irrelevant documents\n            irrelevant_docs = [\n                doc for doc in graded_docs \n                if doc.relevance_score < self.relevance_threshold\n            ]\n            \n            if not irrelevant_docs:\n                # All documents relevant, proceed to generation\n                all_documents.extend(graded_docs)\n                break\n            \n            # Rewrite query based on irrelevant documents\n            query = self.rewrite_query(query, irrelevant_docs, graded_docs)\n            query_history.append(query)\n            \n            # Keep relevant documents\n            relevant_docs = [\n                doc for doc in graded_docs \n                if doc.relevance_score >= self.relevance_threshold\n            ]\n            all_documents.extend(relevant_docs)\n            \n            iteration += 1\n        \n        # Generate answer from all collected documents\n        answer = self.generate_answer(user_query, all_documents)\n        \n        return {\n            'answer': answer,\n            'documents_used': len(all_documents),\n            'iterations': iteration + 1,\n            'query_history': query_history,\n            'relevance_scores': [doc.relevance_score for doc in all_documents]\n        }\n    \n    def retrieve_documents(self, query: str) -> List[Document]:\n        \"\"\"Retrieve documents for query\"\"\"\n        # Simulate retrieval\n        raw_docs = self.retriever.search(query, top_k=5)\n        \n        documents = []\n        for doc in raw_docs:\n            documents.append(Document(\n                content=doc['content'],\n                source=doc.get('source', 'unknown'),\n                metadata=doc.get('metadata', {})\n            ))\n        \n        return documents\n    \n    def grade_documents(self, documents: List[Document], query: str) -> List[Document]:\n        \"\"\"Grade document relevance to query\"\"\"\n        for doc in documents:\n            # Use LLM to grade relevance\n            grading_prompt = f\"\"\"\n            Query: {query}\n            Document: {doc.content[:500]}\n            \n            Rate the relevance of this document to the query on a scale of 0-1.\n            Consider: \n            - Direct answer to query\n            - Related information\n            - Context usefulness\n            \n            Provide only a number between 0 and 1.\n            \"\"\"\n            \n            response = self.llm.generate(grading_prompt)\n            \n            try:\n                score = float(response.strip())\n                doc.relevance_score = min(1.0, max(0.0, score))\n            except:\n                doc.relevance_score = 0.5  # Default middle score\n        \n        return documents\n    \n    def rewrite_query(self, \n                     original_query: str, \n                     irrelevant_docs: List[Document],\n                     all_docs: List[Document]) -> str:\n        \"\"\"Rewrite query based on irrelevant documents\"\"\"\n        \n        # Analyze why documents were irrelevant\n        irrelevant_summary = self.summarize_irrelevance(irrelevant_docs, original_query)\n        \n        rewrite_prompt = f\"\"\"\n        Original Query: {original_query}\n        \n        Retrieved documents were partially irrelevant because: {irrelevant_summary}\n        \n        Relevant document topics: {self.get_relevant_topics(all_docs)}\n        \n        Rewrite the query to be more specific and avoid irrelevant results.\n        Focus on the core intent while adding clarifying details.\n        \n        Rewritten query:\n        \"\"\"\n        \n        rewritten = self.llm.generate(rewrite_prompt)\n        return rewritten.strip()\n    \n    def summarize_irrelevance(self, docs: List[Document], query: str) -> str:\n        \"\"\"Analyze why documents were irrelevant\"\"\"\n        if not docs:\n            return \"No irrelevant documents\"\n        \n        # Simple heuristic - in production, use LLM analysis\n        reasons = []\n        for doc in docs[:2]:  # Analyze top 2 irrelevant\n            if doc.relevance_score < 0.3:\n                reasons.append(\"completely off-topic\")\n            elif doc.relevance_score < 0.5:\n                reasons.append(\"tangentially related\")\n            else:\n                reasons.append(\"missing key details\")\n        \n        return \", \".join(set(reasons))\n    \n    def get_relevant_topics(self, docs: List[Document]) -> str:\n        \"\"\"Extract topics from relevant documents\"\"\"\n        relevant_docs = [doc for doc in docs if doc.relevance_score >= self.relevance_threshold]\n        if not relevant_docs:\n            return \"none identified\"\n        \n        # In production, use topic extraction\n        topics = []\n        for doc in relevant_docs[:3]:\n            # Extract first sentence as topic proxy\n            first_sentence = doc.content.split('.')[0]\n            if len(first_sentence) < 100:\n                topics.append(first_sentence)\n        \n        return \"; \".join(topics) if topics else \"various topics\"\n    \n    def generate_answer(self, query: str, documents: List[Document]) -> str:\n        \"\"\"Generate final answer from documents\"\"\"\n        # Sort documents by relevance\n        documents.sort(key=lambda x: x.relevance_score, reverse=True)\n        \n        # Combine top documents\n        context = \"\\n\\n\".join([\n            f\"[Source: {doc.source}]\\n{doc.content}\"\n            for doc in documents[:3]\n        ])\n        \n        generation_prompt = f\"\"\"\n        Query: {query}\n        \n        Context from retrieved documents:\n        {context}\n        \n        Provide a comprehensive answer to the query based on the context above.\n        Cite sources where appropriate.\n        \"\"\"\n        \n        answer = self.llm.generate(generation_prompt)\n        return answer\n\n# Mock implementations for demonstration\nclass MockRetriever:\n    def search(self, query: str, top_k: int = 5) -> List[Dict]:\n        # Simulate document retrieval\n        if \"coffee\" in query.lower():\n            return [\n                {\"content\": \"Coffee shops provide workspace and beverages.\", \"source\": \"doc1\"},\n                {\"content\": \"Best coffee shops have WiFi and quiet atmosphere.\", \"source\": \"doc2\"},\n                {\"content\": \"Tea shops are similar to coffee shops.\", \"source\": \"doc3\"},\n                {\"content\": \"Restaurant guide for local dining.\", \"source\": \"doc4\"},\n                {\"content\": \"Coffee brewing techniques and equipment.\", \"source\": \"doc5\"}\n            ]\n        return [{\"content\": f\"Document about {query}\", \"source\": f\"doc{i}\"} for i in range(top_k)]\n\nclass MockLLM:\n    def generate(self, prompt: str) -> str:\n        if \"Rate the relevance\" in prompt:\n            if \"coffee\" in prompt.lower() and \"coffee\" in prompt.split(\"Document:\")[1].lower():\n                return \"0.9\"\n            elif \"coffee\" in prompt.lower() and \"tea\" in prompt.split(\"Document:\")[1].lower():\n                return \"0.6\"\n            elif \"coffee\" in prompt.lower() and \"restaurant\" in prompt.split(\"Document:\")[1].lower():\n                return \"0.3\"\n            return \"0.5\"\n        elif \"Rewrite the query\" in prompt:\n            return \"Find coffee shops with WiFi suitable for remote work\"\n        else:\n            return \"Based on the context, here is a comprehensive answer to your query.\""
            }
          ],
          "checklist": [
            {
              "id": "check-7-1-1",
              "text": "Understand when to use multi-agent architectures vs simpler approaches",
              "completed": false
            },
            {
              "id": "check-7-1-2",
              "text": "Implement human-in-the-loop feedback systems for agent improvement",
              "completed": false
            },
            {
              "id": "check-7-1-3",
              "text": "Master the DYLAN framework for dynamic agent networks",
              "completed": false
            },
            {
              "id": "check-7-1-4",
              "text": "Implement HyDE for query enhancement and context augmentation",
              "completed": false
            },
            {
              "id": "check-7-1-5",
              "text": "Build Corrective RAG systems with iterative refinement",
              "completed": false
            },
            {
              "id": "check-7-1-6",
              "text": "Design Adaptive RAG with query routing and web fallback",
              "completed": false
            },
            {
              "id": "check-7-1-7",
              "text": "Compare different RAG approaches and choose appropriate techniques",
              "completed": false
            },
            {
              "id": "check-7-1-8",
              "text": "Plan production deployment strategies for agent systems",
              "completed": false
            }
          ],
          "timestamps": [
            {
              "time": "0:00:13",
              "label": "Course Introduction",
              "description": "Welcome and class overview after July 4th break"
            },
            {
              "time": "0:00:32",
              "label": "Advanced Agent Workflows",
              "description": "Introduction to sophisticated agent workflows without code focus"
            },
            {
              "time": "0:01:55",
              "label": "When to Use Graphs",
              "description": "Discussion on when to use graphs in agent workflows"
            },
            {
              "time": "0:02:56",
              "label": "RAG Pipeline",
              "description": "Review of multi-agent architectures for complex tasks"
            },
            {
              "time": "0:04:36",
              "label": "Human-in-the-Loop Intro",
              "description": "Introduction to human-in-the-loop design patterns"
            },
            {
              "time": "0:04:51",
              "label": "Google Maps Example",
              "description": "Real-world HITL example with Google Maps AI integration"
            },
            {
              "time": "0:05:38",
              "label": "Feedback Loop Benefits",
              "description": "How feedback loops improve agent confidence over time"
            },
            {
              "time": "0:09:37",
              "label": "Email Cleaning Example",
              "description": "Discussion of AI email management from 'Her' movie"
            },
            {
              "time": "0:11:04",
              "label": "Multi-Agent Conflicts",
              "description": "Handling mismatched feedback in multi-agent systems"
            },
            {
              "time": "0:14:44",
              "label": "Q&A: Human Oversight",
              "description": "Discussion on entropy and code complexity checking"
            },
            {
              "time": "0:19:39",
              "label": "DYLAN Framework",
              "description": "Introduction to Dynamic LLM Agent Networks"
            },
            {
              "time": "0:22:05",
              "label": "DYLAN Process Steps",
              "description": "Agent creation, collaboration, and performance assessment"
            },
            {
              "time": "0:24:28",
              "label": "DYLAN Architecture",
              "description": "Graph-based agent networks and peer rating systems"
            },
            {
              "time": "0:26:53",
              "label": "Healthcare Use Case",
              "description": "Medical image diagnostics with specialized agents"
            },
            {
              "time": "0:31:29",
              "label": "RAG Pipeline",
              "description": "Hypothetical Document Embeddings for query enhancement"
            },
            {
              "time": "0:32:35",
              "label": "HyDE Process",
              "description": "How HyDE refines prompts with additional context"
            },
            {
              "time": "0:34:27",
              "label": "HyDE Benefits",
              "description": "Enhancing reliability and reducing inaccuracy"
            },
            {
              "time": "0:37:57",
              "label": "HyDE Architecture",
              "description": "Technical architecture and implementation details"
            },
            {
              "time": "0:38:33",
              "label": "Corrective RAG",
              "description": "Introduction to corrective feedback loops in RAG"
            },
            {
              "time": "0:39:11",
              "label": "Corrective RAG Flow",
              "description": "Iterative document grading and query rewriting"
            },
            {
              "time": "0:41:08",
              "label": "Legal Use Case",
              "description": "Corrective RAG for accurate case law retrieval"
            },
            {
              "time": "0:43:50",
              "label": "Adaptive RAG",
              "description": "Query analysis and dynamic routing strategies"
            },
            {
              "time": "0:45:09",
              "label": "Adaptive RAG Flow",
              "description": "Web search fallback and hallucination detection"
            },
            {
              "time": "0:48:31",
              "label": "RAG Comparison",
              "description": "Comparing vanilla, corrective, adaptive RAG and HyDE"
            },
            {
              "time": "0:50:20",
              "label": "Capstone Projects",
              "description": "Introduction to capstone project requirements"
            },
            {
              "time": "0:51:35",
              "label": "Project Expectations",
              "description": "MVP scope, multi-agent systems, and performance metrics"
            },
            {
              "time": "0:53:55",
              "label": "Tom's Advice",
              "description": "Lessons learned from first cohort capstones"
            },
            {
              "time": "0:56:30",
              "label": "Q&A Session",
              "description": "Questions about capstone projects and implementations"
            },
            {
              "time": "1:03:51",
              "label": "Closing Remarks",
              "description": "Final thoughts and next class preview"
            }
          ],
          "videoPath": "cohort_2/week_7_class_1_2024-07-08.mp4"
        },
        {
          "id": "lesson-7-2",
          "title": "Scaling & Enterprise Integration",
          "duration": "1:31:00",
          "videoUrl": "/videos/cohort_2/week_07/week_7_class_2_2024-07-10.mp4",
          "videoId": "week_7_class_2_2024-07-10",
          "content": "# Scaling & Enterprise Integration\n\n## LangServe for Production Deployment\n\nThis lesson covers LangServe architecture, deployment strategies, feedback loops, and enterprise-scale agent systems.\n\n## LangServe Overview\n\n### What is LangServe?\n\nLangServe is part of the LangChain ecosystem designed to deploy LLM chains as scalable web services. It provides a framework that accelerates deploying chains, graphs, and multi-agent systems without writing extensive boilerplate code.\n\n### Key Benefits\n\n- **Abstraction Layer**: Eliminates need for custom deployment code\n- **Built-in Scaling**: Handles web service scaling automatically\n- **FastAPI Integration**: Leverages FastAPI for high-performance APIs\n- **Streaming Support**: Built-in support for streaming responses\n- **Playground Interface**: Interactive testing environment included\n\n### Architecture Components\n\n**Core Stack:**\n- FastAPI for web framework\n- Pydantic for data validation\n- Async support for concurrent requests\n- WebSocket support for real-time communication\n- Automatic API documentation generation\n\n## Setting Up LangServe\n\n### Installation and Dependencies\n\n```bash\npip install langserve[all]\npip install langchain langchain-openai\npip install fastapi uvicorn\n```\n\n### Basic Server Structure\n\n**Project Layout:**\n```\nlangserve-app/\n\u251c\u2500\u2500 app/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 server.py\n\u2502   \u251c\u2500\u2500 chains.py\n\u2502   \u2514\u2500\u2500 agents.py\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 .env\n\u2514\u2500\u2500 README.md\n```\n\n### Creating Your First LangServe Application\n\n1. **Define chains or agents** in separate modules\n2. **Set up FastAPI app** with LangServe routes\n3. **Add middleware** for CORS, authentication\n4. **Configure endpoints** for different functionalities\n5. **Enable playground** for testing\n\n## Deployment to Render\n\n### Why Render?\n\nRender provides a lightweight, developer-friendly platform for deploying web applications:\n- Automatic builds from Git repositories\n- Built-in SSL certificates\n- Easy environment variable management\n- Automatic scaling capabilities\n- Free tier for testing\n\n### Deployment Process\n\n**Prerequisites:**\n- GitHub repository with LangServe app\n- Render account\n- Environment variables configured\n\n**Steps:**\n1. Connect GitHub repository to Render\n2. Configure build and start commands\n3. Set environment variables\n4. Deploy and monitor application\n5. Set up custom domains (optional)\n\n### Render Configuration\n\n**render.yaml example:**\n```yaml\nservices:\n  - type: web\n    name: langserve-app\n    env: python\n    buildCommand: pip install -r requirements.txt\n    startCommand: uvicorn app.server:app --host 0.0.0.0 --port $PORT\n    envVars:\n      - key: OPENAI_API_KEY\n        sync: false\n      - key: LANGCHAIN_API_KEY\n        sync: false\n```\n\n## Feedback Loops and Data Collection\n\n### Implementing Feedback Systems\n\n**Core Components:**\n1. **Response Scoring**: Collect user ratings on responses\n2. **Data Storage**: Persist feedback for analysis\n3. **Dataset Creation**: Build training datasets from feedback\n4. **Model Improvement**: Use feedback for fine-tuning\n\n### Feedback Collection Architecture\n\n```python\nclass FeedbackCollector:\n    def __init__(self):\n        self.feedback_store = []\n        self.datasets = {}\n    \n    def collect_feedback(self, query, response, rating, metadata):\n        feedback_entry = {\n            'timestamp': datetime.now(),\n            'query': query,\n            'response': response,\n            'rating': rating,\n            'metadata': metadata\n        }\n        self.feedback_store.append(feedback_entry)\n        self.update_datasets(feedback_entry)\n    \n    def update_datasets(self, entry):\n        # Categorize and store for training\n        if entry['rating'] >= 4:\n            self.add_to_dataset('high_quality', entry)\n        elif entry['rating'] <= 2:\n            self.add_to_dataset('needs_improvement', entry)\n```\n\n### Creating Training Datasets\n\n**Dataset Structure:**\n- Positive examples (high-rated responses)\n- Negative examples (low-rated responses)\n- Edge cases and corrections\n- Domain-specific categorization\n\n**Export Formats:**\n- JSONL for fine-tuning\n- CSV for analysis\n- Parquet for large-scale storage\n\n## Agent Graphs on LangServe\n\n### Graph-Based Agent Systems\n\nLangServe supports complex agent graphs with:\n- Multiple agent nodes\n- Conditional routing\n- Parallel execution\n- State management\n- Error handling\n\n### Implementation Patterns\n\n**Sequential Agents:**\n```python\ngraph = StateGraph(AgentState)\ngraph.add_node(\"researcher\", research_agent)\ngraph.add_node(\"writer\", writing_agent)\ngraph.add_node(\"reviewer\", review_agent)\n\ngraph.add_edge(\"researcher\", \"writer\")\ngraph.add_edge(\"writer\", \"reviewer\")\n```\n\n**Conditional Routing:**\n```python\ndef route_decision(state):\n    if state['complexity'] > 0.7:\n        return \"complex_handler\"\n    return \"simple_handler\"\n\ngraph.add_conditional_edges(\n    \"classifier\",\n    route_decision,\n    {\n        \"complex_handler\": \"complex_agent\",\n        \"simple_handler\": \"simple_agent\"\n    }\n)\n```\n\n### State Management\n\n**Persistent State:**\n- Session management across requests\n- State checkpointing for recovery\n- Distributed state for scaling\n\n**State Patterns:**\n1. **Accumulator**: Collect results across agents\n2. **Pipeline**: Transform data through stages\n3. **Supervisor**: Coordinate multiple workers\n4. **Consensus**: Aggregate multiple opinions\n\n## Enterprise Integration Patterns\n\n### Authentication and Security\n\n**Security Layers:**\n1. API key authentication\n2. OAuth2 integration\n3. Rate limiting per user/organization\n4. Request signing and validation\n5. End-to-end encryption for sensitive data\n\n### Monitoring and Observability\n\n**Key Metrics:**\n- Request latency percentiles\n- Token usage and costs\n- Error rates by endpoint\n- Agent performance metrics\n- User satisfaction scores\n\n**Integration Options:**\n- Datadog for comprehensive monitoring\n- Prometheus + Grafana for open-source stack\n- CloudWatch for AWS deployments\n- Custom dashboards with business metrics\n\n### Scaling Strategies\n\n**Horizontal Scaling:**\n- Load balancing across instances\n- Auto-scaling based on metrics\n- Geographic distribution\n- Cache layers for common queries\n\n**Vertical Scaling:**\n- Optimize model selection\n- Batch processing for efficiency\n- Async processing for long tasks\n- Resource allocation tuning\n\n## Production Best Practices\n\n### Error Handling\n\n1. **Graceful Degradation**: Fallback responses when services fail\n2. **Retry Logic**: Automatic retries with exponential backoff\n3. **Circuit Breakers**: Prevent cascade failures\n4. **Error Tracking**: Comprehensive logging and alerting\n\n### Performance Optimization\n\n**Caching Strategies:**\n- Response caching for common queries\n- Embedding caching for RAG systems\n- Model output caching\n- Session state caching\n\n**Optimization Techniques:**\n- Lazy loading of models\n- Connection pooling\n- Request batching\n- Async processing\n\n### Cost Management\n\n**Cost Optimization:**\n- Model selection based on complexity\n- Token usage monitoring\n- Caching to reduce API calls\n- Batch processing for efficiency\n\n**Budget Controls:**\n- Rate limiting by cost\n- Alerts for unusual usage\n- Automatic scaling limits\n- Usage quotas per user\n\n## Advanced Features\n\n### Streaming Responses\n\nLangServe supports streaming for real-time responses:\n- Server-sent events (SSE)\n- WebSocket connections\n- Chunked transfer encoding\n- Progressive rendering\n\n### Multi-Model Orchestration\n\n**Patterns:**\n- Model routing based on task type\n- Ensemble methods for accuracy\n- Fallback chains for reliability\n- A/B testing for optimization\n\n### Custom Middleware\n\n**Examples:**\n- Request validation\n- Response transformation\n- Logging and analytics\n- Custom authentication\n\n## Testing and Quality Assurance\n\n### Testing Strategies\n\n1. **Unit Tests**: Test individual chains and agents\n2. **Integration Tests**: Test complete workflows\n3. **Load Tests**: Verify scaling capabilities\n4. **End-to-End Tests**: Test full user journeys\n\n### Quality Metrics\n\n- Response accuracy\n- Latency requirements\n- Throughput capacity\n- Error rates\n- User satisfaction\n\n### Continuous Improvement\n\n**Feedback Loop:**\n1. Collect user feedback\n2. Analyze performance metrics\n3. Identify improvement areas\n4. Update models/prompts\n5. A/B test changes\n6. Deploy improvements\n7. Monitor impact\n\n## Deployment Checklist\n\n### Pre-Deployment\n\n- [ ] Environment variables configured\n- [ ] Security measures implemented\n- [ ] Monitoring setup complete\n- [ ] Error handling tested\n- [ ] Performance benchmarks met\n- [ ] Documentation updated\n\n### Post-Deployment\n\n- [ ] Health checks passing\n- [ ] Metrics flowing correctly\n- [ ] Alerts configured\n- [ ] Backup procedures verified\n- [ ] Rollback plan ready\n- [ ] User feedback collection active",
          "codeExamples": [
            {
              "id": "code-7-2-1",
              "title": "Basic LangServe Application",
              "language": "python",
              "code": "from fastapi import FastAPI\nfrom langchain.schema.runnable import Runnable\nfrom langchain_openai import ChatOpenAI\nfrom langserve import add_routes\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.schema import StrOutputParser\nimport uvicorn\n\n# Create FastAPI app\napp = FastAPI(\n    title=\"LangServe API\",\n    description=\"Production LLM API with LangServe\",\n    version=\"1.0.0\"\n)\n\n# Create a simple chain\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful AI assistant.\"),\n    (\"user\", \"{input}\")\n])\n\nllm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\noutput_parser = StrOutputParser()\n\n# Create the chain\nchain = prompt | llm | output_parser\n\n# Add CORS middleware\nfrom fastapi.middleware.cors import CORSMiddleware\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# Add routes\nadd_routes(\n    app,\n    chain,\n    path=\"/chat\",\n    enable_feedback_endpoint=True,\n    enable_public_trace_link_endpoint=True,\n    playground_type=\"default\",\n)\n\n# Add health check endpoint\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"healthy\", \"service\": \"langserve-api\"}\n\n# Add custom endpoint for complex logic\n@app.post(\"/analyze\")\nasync def analyze_text(text: str, analysis_type: str = \"sentiment\"):\n    \"\"\"Custom endpoint for text analysis\"\"\"\n    \n    analysis_prompt = ChatPromptTemplate.from_messages([\n        (\"system\", f\"You are an expert at {analysis_type} analysis.\"),\n        (\"user\", f\"Analyze this text for {analysis_type}: {text}\")\n    ])\n    \n    analysis_chain = analysis_prompt | llm | output_parser\n    result = await analysis_chain.ainvoke({\"text\": text})\n    \n    return {\n        \"input\": text,\n        \"analysis_type\": analysis_type,\n        \"result\": result\n    }\n\nif __name__ == \"__main__\":\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
            },
            {
              "id": "code-7-2-2",
              "title": "Feedback Collection System",
              "language": "python",
              "code": "from typing import Dict, List, Optional\nfrom datetime import datetime\nfrom pydantic import BaseModel, Field\nimport json\nfrom pathlib import Path\nimport pandas as pd\n\nclass FeedbackEntry(BaseModel):\n    \"\"\"Model for feedback entries\"\"\"\n    query: str\n    response: str\n    rating: int = Field(..., ge=1, le=5)\n    user_id: Optional[str] = None\n    session_id: str\n    metadata: Dict = Field(default_factory=dict)\n    timestamp: datetime = Field(default_factory=datetime.now)\n    \nclass FeedbackCollector:\n    \"\"\"Collect and manage user feedback for model improvement\"\"\"\n    \n    def __init__(self, storage_path: str = \"./feedback_data\"):\n        self.storage_path = Path(storage_path)\n        self.storage_path.mkdir(exist_ok=True)\n        self.current_session_feedback = []\n        self.datasets = {\n            'training': [],\n            'validation': [],\n            'test': []\n        }\n    \n    async def collect_feedback(self, feedback: FeedbackEntry) -> Dict:\n        \"\"\"Collect and store feedback\"\"\"\n        # Add to current session\n        self.current_session_feedback.append(feedback)\n        \n        # Persist to disk\n        await self._persist_feedback(feedback)\n        \n        # Update datasets if high quality\n        if feedback.rating >= 4:\n            await self._add_to_training_set(feedback)\n        \n        # Calculate session statistics\n        stats = self._calculate_session_stats()\n        \n        return {\n            \"feedback_id\": f\"{feedback.session_id}_{len(self.current_session_feedback)}\",\n            \"status\": \"collected\",\n            \"session_stats\": stats\n        }\n    \n    async def _persist_feedback(self, feedback: FeedbackEntry):\n        \"\"\"Save feedback to disk\"\"\"\n        filename = self.storage_path / f\"feedback_{datetime.now().strftime('%Y%m%d')}.jsonl\"\n        \n        with open(filename, 'a') as f:\n            f.write(feedback.json() + '\\n')\n    \n    async def _add_to_training_set(self, feedback: FeedbackEntry):\n        \"\"\"Add high-quality examples to training set\"\"\"\n        training_entry = {\n            \"messages\": [\n                {\"role\": \"user\", \"content\": feedback.query},\n                {\"role\": \"assistant\", \"content\": feedback.response}\n            ],\n            \"rating\": feedback.rating,\n            \"metadata\": feedback.metadata\n        }\n        \n        self.datasets['training'].append(training_entry)\n        \n        # Save training set periodically\n        if len(self.datasets['training']) % 100 == 0:\n            await self._export_training_set()\n    \n    async def _export_training_set(self):\n        \"\"\"Export training set for fine-tuning\"\"\"\n        training_file = self.storage_path / \"training_set.jsonl\"\n        \n        with open(training_file, 'w') as f:\n            for entry in self.datasets['training']:\n                f.write(json.dumps(entry) + '\\n')\n        \n        return training_file\n    \n    def _calculate_session_stats(self) -> Dict:\n        \"\"\"Calculate statistics for current session\"\"\"\n        if not self.current_session_feedback:\n            return {\"total\": 0, \"average_rating\": 0}\n        \n        ratings = [f.rating for f in self.current_session_feedback]\n        \n        return {\n            \"total\": len(self.current_session_feedback),\n            \"average_rating\": sum(ratings) / len(ratings),\n            \"rating_distribution\": {\n                str(i): ratings.count(i) for i in range(1, 6)\n            },\n            \"high_quality_ratio\": len([r for r in ratings if r >= 4]) / len(ratings)\n        }\n    \n    async def generate_analytics_report(self) -> Dict:\n        \"\"\"Generate comprehensive analytics report\"\"\"\n        all_feedback = await self._load_all_feedback()\n        \n        if not all_feedback:\n            return {\"status\": \"no_data\"}\n        \n        df = pd.DataFrame([f.dict() for f in all_feedback])\n        \n        report = {\n            \"total_feedback\": len(df),\n            \"average_rating\": df['rating'].mean(),\n            \"rating_distribution\": df['rating'].value_counts().to_dict(),\n            \"feedback_over_time\": self._analyze_temporal_patterns(df),\n            \"common_queries\": self._extract_common_patterns(df),\n            \"improvement_areas\": self._identify_improvement_areas(df)\n        }\n        \n        return report\n    \n    async def _load_all_feedback(self) -> List[FeedbackEntry]:\n        \"\"\"Load all feedback from storage\"\"\"\n        feedback_list = []\n        \n        for file in self.storage_path.glob(\"feedback_*.jsonl\"):\n            with open(file, 'r') as f:\n                for line in f:\n                    feedback_list.append(FeedbackEntry(**json.loads(line)))\n        \n        return feedback_list\n    \n    def _analyze_temporal_patterns(self, df: pd.DataFrame) -> Dict:\n        \"\"\"Analyze feedback patterns over time\"\"\"\n        df['date'] = pd.to_datetime(df['timestamp']).dt.date\n        daily_stats = df.groupby('date')['rating'].agg(['mean', 'count'])\n        \n        return {\n            \"daily_average\": daily_stats['mean'].to_dict(),\n            \"daily_volume\": daily_stats['count'].to_dict()\n        }\n    \n    def _extract_common_patterns(self, df: pd.DataFrame) -> List[str]:\n        \"\"\"Extract common query patterns\"\"\"\n        # Simplified - in production use NLP techniques\n        queries = df['query'].value_counts().head(10)\n        return queries.index.tolist()\n    \n    def _identify_improvement_areas(self, df: pd.DataFrame) -> List[Dict]:\n        \"\"\"Identify areas needing improvement\"\"\"\n        low_rated = df[df['rating'] <= 2]\n        \n        if low_rated.empty:\n            return []\n        \n        # Group by query patterns\n        improvement_areas = []\n        for _, row in low_rated.iterrows():\n            improvement_areas.append({\n                \"query\": row['query'][:100],\n                \"rating\": row['rating'],\n                \"issue\": \"Low user satisfaction\"\n            })\n        \n        return improvement_areas[:5]  # Top 5 issues\n\n# Integration with LangServe\nasync def create_feedback_endpoint(app: FastAPI, collector: FeedbackCollector):\n    \"\"\"Add feedback endpoint to FastAPI app\"\"\"\n    \n    @app.post(\"/feedback\")\n    async def submit_feedback(feedback: FeedbackEntry):\n        result = await collector.collect_feedback(feedback)\n        return result\n    \n    @app.get(\"/feedback/analytics\")\n    async def get_analytics():\n        report = await collector.generate_analytics_report()\n        return report\n    \n    @app.get(\"/feedback/export\")\n    async def export_training_data():\n        file_path = await collector._export_training_set()\n        return {\"status\": \"exported\", \"path\": str(file_path)}"
            },
            {
              "id": "code-7-2-3",
              "title": "Agent Graph with LangServe",
              "language": "python",
              "code": "from typing import TypedDict, Annotated, List, Union\nfrom langchain_core.agents import AgentAction, AgentFinish\nfrom langchain_core.messages import BaseMessage\nfrom langgraph.graph import StateGraph, END\nfrom langchain_openai import ChatOpenAI\nfrom langchain.tools import Tool\nfrom langserve import add_routes\nimport operator\n\n# Define the state for our agent graph\nclass AgentState(TypedDict):\n    messages: Annotated[List[BaseMessage], operator.add]\n    next_step: Union[AgentAction, AgentFinish, None]\n    intermediate_steps: Annotated[List[tuple], operator.add]\n    final_answer: str\n\n# Create specialized agents\nclass ResearchAgent:\n    \"\"\"Agent for research tasks\"\"\"\n    \n    def __init__(self):\n        self.llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n        self.tools = [\n            Tool(\n                name=\"search\",\n                func=self._search,\n                description=\"Search for information\"\n            ),\n            Tool(\n                name=\"analyze\",\n                func=self._analyze,\n                description=\"Analyze gathered information\"\n            )\n        ]\n    \n    def _search(self, query: str) -> str:\n        # Simulate search\n        return f\"Search results for: {query}\"\n    \n    def _analyze(self, data: str) -> str:\n        # Simulate analysis\n        return f\"Analysis of: {data}\"\n    \n    async def process(self, state: AgentState) -> AgentState:\n        \"\"\"Process research request\"\"\"\n        last_message = state['messages'][-1].content if state['messages'] else \"\"\n        \n        # Perform research\n        search_result = self._search(last_message)\n        analysis = self._analyze(search_result)\n        \n        state['intermediate_steps'].append((\"research\", analysis))\n        return state\n\nclass WritingAgent:\n    \"\"\"Agent for content creation\"\"\"\n    \n    def __init__(self):\n        self.llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.8)\n    \n    async def process(self, state: AgentState) -> AgentState:\n        \"\"\"Generate written content\"\"\"\n        research_data = \"\"\n        for step_type, data in state['intermediate_steps']:\n            if step_type == \"research\":\n                research_data += data + \"\\n\"\n        \n        prompt = f\"Based on this research:\\n{research_data}\\nWrite a comprehensive response.\"\n        \n        response = await self.llm.ainvoke(prompt)\n        state['intermediate_steps'].append((\"writing\", response.content))\n        \n        return state\n\nclass ReviewAgent:\n    \"\"\"Agent for quality review\"\"\"\n    \n    def __init__(self):\n        self.llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.3)\n    \n    async def process(self, state: AgentState) -> AgentState:\n        \"\"\"Review and refine content\"\"\"\n        content = \"\"\n        for step_type, data in state['intermediate_steps']:\n            if step_type == \"writing\":\n                content = data\n                break\n        \n        prompt = f\"Review and improve this content:\\n{content}\"\n        \n        response = await self.llm.ainvoke(prompt)\n        state['final_answer'] = response.content\n        \n        return state\n\n# Create the graph\ndef create_agent_graph():\n    \"\"\"Create multi-agent graph\"\"\"\n    \n    # Initialize agents\n    research_agent = ResearchAgent()\n    writing_agent = WritingAgent()\n    review_agent = ReviewAgent()\n    \n    # Create graph\n    workflow = StateGraph(AgentState)\n    \n    # Add nodes\n    workflow.add_node(\"research\", research_agent.process)\n    workflow.add_node(\"writing\", writing_agent.process)\n    workflow.add_node(\"review\", review_agent.process)\n    \n    # Define routing logic\n    def route_next(state: AgentState) -> str:\n        if not state.get('intermediate_steps'):\n            return \"research\"\n        \n        last_step = state['intermediate_steps'][-1][0]\n        \n        if last_step == \"research\":\n            return \"writing\"\n        elif last_step == \"writing\":\n            return \"review\"\n        else:\n            return END\n    \n    # Add edges\n    workflow.set_entry_point(\"research\")\n    workflow.add_edge(\"research\", \"writing\")\n    workflow.add_edge(\"writing\", \"review\")\n    workflow.add_edge(\"review\", END)\n    \n    # Compile graph\n    app = workflow.compile()\n    \n    return app\n\n# Create supervisor pattern\nclass SupervisorAgent:\n    \"\"\"Orchestrates multiple specialized agents\"\"\"\n    \n    def __init__(self):\n        self.llm = ChatOpenAI(model=\"gpt-4\", temperature=0.5)\n        self.agents = {\n            \"research\": ResearchAgent(),\n            \"writing\": WritingAgent(),\n            \"review\": ReviewAgent()\n        }\n    \n    async def delegate_task(self, task: str) -> str:\n        \"\"\"Determine which agent should handle the task\"\"\"\n        \n        prompt = f\"\"\"\n        Task: {task}\n        \n        Available agents:\n        - research: For gathering information\n        - writing: For creating content\n        - review: For quality assurance\n        \n        Which agent should handle this task? Respond with only the agent name.\n        \"\"\"\n        \n        response = await self.llm.ainvoke(prompt)\n        agent_name = response.content.strip().lower()\n        \n        if agent_name in self.agents:\n            return agent_name\n        return \"research\"  # Default\n    \n    async def coordinate(self, request: str) -> Dict:\n        \"\"\"Coordinate agents to complete request\"\"\"\n        \n        # Analyze request complexity\n        complexity = await self._assess_complexity(request)\n        \n        if complexity > 0.7:\n            # Complex request - use full pipeline\n            workflow = create_agent_graph()\n            initial_state = {\n                \"messages\": [{\"content\": request}],\n                \"intermediate_steps\": [],\n                \"final_answer\": \"\"\n            }\n            result = await workflow.ainvoke(initial_state)\n            return {\n                \"response\": result['final_answer'],\n                \"complexity\": complexity,\n                \"workflow\": \"full_pipeline\"\n            }\n        else:\n            # Simple request - direct delegation\n            agent_name = await self.delegate_task(request)\n            agent = self.agents[agent_name]\n            \n            state = AgentState(\n                messages=[{\"content\": request}],\n                intermediate_steps=[],\n                final_answer=\"\"\n            )\n            \n            result = await agent.process(state)\n            \n            return {\n                \"response\": result.get('final_answer', \"Processing complete\"),\n                \"complexity\": complexity,\n                \"workflow\": f\"single_agent_{agent_name}\"\n            }\n    \n    async def _assess_complexity(self, request: str) -> float:\n        \"\"\"Assess request complexity\"\"\"\n        \n        # Simple heuristic - in production use better methods\n        word_count = len(request.split())\n        has_multiple_parts = \"and\" in request.lower() or \",\" in request\n        \n        complexity = min(1.0, word_count / 50)\n        if has_multiple_parts:\n            complexity = min(1.0, complexity * 1.5)\n        \n        return complexity\n\n# Integration with FastAPI\ndef create_graph_endpoints(app: FastAPI):\n    \"\"\"Add graph-based agent endpoints\"\"\"\n    \n    supervisor = SupervisorAgent()\n    \n    @app.post(\"/agent/process\")\n    async def process_with_agents(request: str):\n        result = await supervisor.coordinate(request)\n        return result\n    \n    @app.get(\"/agent/workflow\")\n    async def get_workflow_info():\n        return {\n            \"agents\": [\"research\", \"writing\", \"review\"],\n            \"workflow_types\": [\"full_pipeline\", \"single_agent\"],\n            \"complexity_threshold\": 0.7\n        }"
            },
            {
              "id": "code-7-2-4",
              "title": "Production Deployment Configuration",
              "language": "python",
              "code": "import os\nfrom typing import Optional\nfrom pydantic import BaseSettings\nimport logging\nfrom functools import lru_cache\n\nclass Settings(BaseSettings):\n    \"\"\"Application settings for production deployment\"\"\"\n    \n    # API Keys\n    openai_api_key: str\n    langchain_api_key: Optional[str] = None\n    \n    # Application Config\n    app_name: str = \"LangServe Production API\"\n    app_version: str = \"1.0.0\"\n    debug_mode: bool = False\n    \n    # Server Config\n    host: str = \"0.0.0.0\"\n    port: int = 8000\n    workers: int = 4\n    \n    # Security\n    api_key_header: str = \"X-API-Key\"\n    allowed_origins: list = [\"*\"]\n    rate_limit_per_minute: int = 60\n    \n    # Database\n    database_url: Optional[str] = None\n    redis_url: Optional[str] = None\n    \n    # Monitoring\n    enable_metrics: bool = True\n    metrics_port: int = 9090\n    log_level: str = \"INFO\"\n    \n    # Model Config\n    default_model: str = \"gpt-3.5-turbo\"\n    max_tokens: int = 2000\n    temperature: float = 0.7\n    \n    # Caching\n    enable_cache: bool = True\n    cache_ttl: int = 3600  # 1 hour\n    \n    class Config:\n        env_file = \".env\"\n        env_file_encoding = \"utf-8\"\n\n@lru_cache()\ndef get_settings():\n    return Settings()\n\n# Logging configuration\ndef setup_logging(settings: Settings):\n    logging.basicConfig(\n        level=getattr(logging, settings.log_level),\n        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n        handlers=[\n            logging.StreamHandler(),\n            logging.FileHandler('app.log')\n        ]\n    )\n\n# Security middleware\nfrom fastapi import Request, HTTPException\nfrom fastapi.security import APIKeyHeader\nfrom starlette.middleware.base import BaseHTTPMiddleware\nimport time\n\nclass RateLimitMiddleware(BaseHTTPMiddleware):\n    \"\"\"Rate limiting middleware\"\"\"\n    \n    def __init__(self, app, calls: int = 60, period: int = 60):\n        super().__init__(app)\n        self.calls = calls\n        self.period = period\n        self.clients = {}\n    \n    async def dispatch(self, request: Request, call_next):\n        client_id = request.client.host\n        now = time.time()\n        \n        # Clean old entries\n        self.clients = {\n            k: v for k, v in self.clients.items() \n            if now - v['first_call'] < self.period\n        }\n        \n        if client_id not in self.clients:\n            self.clients[client_id] = {\n                'first_call': now,\n                'calls': 0\n            }\n        \n        client_data = self.clients[client_id]\n        \n        if now - client_data['first_call'] > self.period:\n            client_data['first_call'] = now\n            client_data['calls'] = 0\n        \n        if client_data['calls'] >= self.calls:\n            raise HTTPException(\n                status_code=429,\n                detail=\"Rate limit exceeded\"\n            )\n        \n        client_data['calls'] += 1\n        \n        response = await call_next(request)\n        return response\n\n# Health check and monitoring\nfrom prometheus_client import Counter, Histogram, generate_latest\nimport psutil\n\n# Metrics\nrequest_count = Counter('app_requests_total', 'Total requests')\nrequest_duration = Histogram('app_request_duration_seconds', 'Request duration')\nerror_count = Counter('app_errors_total', 'Total errors')\n\nclass HealthCheck:\n    \"\"\"Health check utilities\"\"\"\n    \n    @staticmethod\n    def get_system_health():\n        return {\n            \"status\": \"healthy\",\n            \"cpu_percent\": psutil.cpu_percent(),\n            \"memory_percent\": psutil.virtual_memory().percent,\n            \"disk_percent\": psutil.disk_usage('/').percent\n        }\n    \n    @staticmethod\n    def get_dependencies_health(settings: Settings):\n        health = {}\n        \n        # Check OpenAI API\n        try:\n            from openai import OpenAI\n            client = OpenAI(api_key=settings.openai_api_key)\n            client.models.list()\n            health['openai'] = \"healthy\"\n        except Exception as e:\n            health['openai'] = f\"unhealthy: {str(e)}\"\n        \n        # Check database if configured\n        if settings.database_url:\n            try:\n                # Database check logic\n                health['database'] = \"healthy\"\n            except Exception as e:\n                health['database'] = f\"unhealthy: {str(e)}\"\n        \n        return health\n\n# Docker configuration\n\"\"\"\n# Dockerfile\nFROM python:3.9-slim\n\nWORKDIR /app\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    gcc \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Install Python dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy application\nCOPY . .\n\n# Create non-root user\nRUN useradd -m appuser && chown -R appuser:appuser /app\nUSER appuser\n\n# Expose ports\nEXPOSE 8000 9090\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\\n    CMD python -c \"import requests; requests.get('http://localhost:8000/health')\"\n\n# Run application\nCMD [\"uvicorn\", \"app.server:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n\"\"\"\n\n# docker-compose.yml\n\"\"\"\nversion: '3.8'\n\nservices:\n  langserve:\n    build: .\n    ports:\n      - \"8000:8000\"\n      - \"9090:9090\"\n    environment:\n      - OPENAI_API_KEY=${OPENAI_API_KEY}\n      - LANGCHAIN_API_KEY=${LANGCHAIN_API_KEY}\n      - DATABASE_URL=postgresql://user:pass@db:5432/langserve\n      - REDIS_URL=redis://redis:6379\n    depends_on:\n      - db\n      - redis\n    restart: unless-stopped\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n  \n  db:\n    image: postgres:14\n    environment:\n      - POSTGRES_USER=user\n      - POSTGRES_PASSWORD=pass\n      - POSTGRES_DB=langserve\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n    restart: unless-stopped\n  \n  redis:\n    image: redis:7-alpine\n    restart: unless-stopped\n    volumes:\n      - redis_data:/data\n\n  nginx:\n    image: nginx:alpine\n    ports:\n      - \"80:80\"\n      - \"443:443\"\n    volumes:\n      - ./nginx.conf:/etc/nginx/nginx.conf\n      - ./certs:/etc/nginx/certs\n    depends_on:\n      - langserve\n    restart: unless-stopped\n\nvolumes:\n  postgres_data:\n  redis_data:\n\"\"\""
            }
          ],
          "checklist": [
            {
              "id": "check-7-2-1",
              "text": "Set up LangServe with FastAPI for production deployment",
              "completed": false
            },
            {
              "id": "check-7-2-2",
              "text": "Deploy application to Render or similar platform",
              "completed": false
            },
            {
              "id": "check-7-2-3",
              "text": "Implement feedback collection and dataset creation",
              "completed": false
            },
            {
              "id": "check-7-2-4",
              "text": "Build agent graphs with state management",
              "completed": false
            },
            {
              "id": "check-7-2-5",
              "text": "Configure authentication and security layers",
              "completed": false
            },
            {
              "id": "check-7-2-6",
              "text": "Set up monitoring and observability",
              "completed": false
            },
            {
              "id": "check-7-2-7",
              "text": "Implement caching and performance optimization",
              "completed": false
            },
            {
              "id": "check-7-2-8",
              "text": "Deploy with Docker and orchestration",
              "completed": false
            }
          ],
          "timestamps": [
            {
              "time": "0:09:10",
              "label": "Course Introduction",
              "description": "Introduction to LangServe deep dive session"
            },
            {
              "time": "0:09:38",
              "label": "Learning Objectives",
              "description": "Learn architecture and production implementation techniques"
            },
            {
              "time": "0:10:13",
              "label": "Render Deployment",
              "description": "Introduction to Render as lightweight hosting platform"
            },
            {
              "time": "0:10:42",
              "label": "Feedback Loops",
              "description": "Setting up feedback collection and dataset creation"
            },
            {
              "time": "0:11:30",
              "label": "LangServe Framework",
              "description": "Part of LangChain ecosystem for deploying LLM chains"
            },
            {
              "time": "0:11:55",
              "label": "API Endpoints",
              "description": "Standardized endpoints: invoke, batch, stream, stream_log"
            },
            {
              "time": "0:12:25",
              "label": "FastAPI Integration",
              "description": "How LangServe leverages FastAPI for web services"
            },
            {
              "time": "0:15:00",
              "label": "Installation Setup",
              "description": "Configure architecture and production parameters and dependencies"
            },
            {
              "time": "0:18:00",
              "label": "Basic Server",
              "description": "Creating first LangServe application structure"
            },
            {
              "time": "0:22:00",
              "label": "Add Routes",
              "description": "Setting up LangServe routes with playground"
            },
            {
              "time": "0:25:00",
              "label": "Render Setup",
              "description": "Configuring application for Render deployment"
            },
            {
              "time": "0:28:00",
              "label": "Environment Variables",
              "description": "Managing API keys and configuration"
            },
            {
              "time": "0:32:00",
              "label": "Feedback Collection",
              "description": "Learn architecture and production implementation techniques"
            },
            {
              "time": "0:35:00",
              "label": "Dataset Creation",
              "description": "Building training datasets from feedback"
            },
            {
              "time": "0:38:00",
              "label": "Rating System",
              "description": "Learn architecture and production implementation techniques"
            },
            {
              "time": "0:42:00",
              "label": "Agent Graphs",
              "description": "Learn architecture and production implementation techniques"
            },
            {
              "time": "0:45:00",
              "label": "State Management",
              "description": "Managing state in agent workflows"
            },
            {
              "time": "0:48:00",
              "label": "Sequential Agents",
              "description": "Building research, writer, reviewer pipeline"
            },
            {
              "time": "0:52:00",
              "label": "Conditional Routing",
              "description": "Dynamic agent selection based on task"
            },
            {
              "time": "0:55:00",
              "label": "Supervisor Pattern",
              "description": "Learn architecture and production implementation techniques"
            },
            {
              "time": "0:58:00",
              "label": "Security Layers",
              "description": "API keys, OAuth2, and rate limiting"
            },
            {
              "time": "1:02:00",
              "label": "Monitoring Setup",
              "description": "Configure architecture and production parameters and dependencies"
            },
            {
              "time": "1:05:00",
              "label": "Scaling Strategies",
              "description": "Horizontal scaling and load balancing"
            },
            {
              "time": "1:08:00",
              "label": "Performance Optimization",
              "description": "Caching strategies and token management"
            },
            {
              "time": "1:12:00",
              "label": "Docker Deployment",
              "description": "Deploy architecture and production to production environment"
            },
            {
              "time": "1:15:00",
              "label": "Production Checklist",
              "description": "Learn architecture and production implementation techniques"
            },
            {
              "time": "1:18:00",
              "label": "Cost Management",
              "description": "Optimizing costs and budget controls"
            },
            {
              "time": "1:20:00",
              "label": "Test API",
              "description": "Questions about LangServe and production deployment"
            }
          ],
          "videoPath": "cohort_2/week_7_class_2_2024-07-10.mp4"
        }
      ]
    },
    {
      "id": "week-8",
      "title": "Week 8: Performance & Optimization",
      "description": "Advanced frameworks, agent optimization strategies, and capstone project development",
      "lessons": [
        {
          "id": "lesson-8-1",
          "title": "Cost Optimization Strategies",
          "duration": "1:26:00",
          "videoUrl": "/videos/cohort_2/week_08/week_8_class_1_2024-07-15.mp4",
          "videoId": "week_8_class_1_2024-07-15",
          "content": "# CrewAI Framework for Agent Optimization\n\n## Overview\n\nThis lesson introduces CrewAI, an opinionated framework built on LangChain that abstracts agent workflow creation. CrewAI accelerates development by providing pre-built hierarchical team structures and reducing boilerplate code, allowing developers to focus primarily on prompt engineering and task design.\n\n## Core CrewAI Philosophy\n\n### Hierarchical Teams Architecture\n- **Supervisor Pattern**: Single contact point for users with natural language interface\n- **Team Structure**: Supervisor routes tasks to specialized teams of agents\n- **Agent Specialization**: Each agent has defined roles, goals, backstories, and tools\n- **Task Distribution**: Clear delegation from supervisor to appropriate team members\n\n### Framework Benefits\n- **Velocity Acceleration**: Fast iteration cycles and rapid prototyping\n- **Reduced Boilerplate**: Minimal code required compared to building with LangChain directly\n- **Declarative Programming**: Describe what you want rather than implementing step-by-step\n- **Focus on Prompting**: Spend time on prompt engineering where the real value lies\n\n### When to Use CrewAI vs LangChain\n- **Use CrewAI when**: Problem fits hierarchical team pattern, need rapid prototyping, want to focus on prompts\n- **Use LangChain when**: Need custom graph architectures, require fine-grained control, building complex non-standard flows\n\n## Key Components\n\n### Agents\n- **Role**: Job title and primary function (e.g., \"City Selector\", \"Travel Researcher\")\n- **Goal**: Specific purpose and objectives for the agent\n- **Backstory**: Context that influences how the agent approaches problems\n- **Tools**: Available functions and APIs the agent can utilize\n- **Verbose Mode**: Enable detailed logging for debugging\n\n### Tasks\n- **Description**: Detailed instructions for what needs to be accomplished\n- **Agent Assignment**: Which agent should handle the task\n- **Expected Output**: Specifications for the final deliverable\n- **Context Variables**: Dynamic inputs from user or previous tasks\n\n### Crews\n- **Team Definition**: Groups of agents working together\n- **Process Configuration**: How tasks flow between agents\n- **Memory System**: Built-in information storage and recall\n- **Kickoff Method**: Entry point to start the workflow\n\n## Practical Implementation\n\n### Trip Planner Example\n- **System Architecture**: User \u2192 Supervisor \u2192 City Selector \u2192 Travel Researcher \u2192 Itinerary Planner\n- **Agent Roles**: Each agent specializes in one aspect of travel planning\n- **Task Flow**: Sequential processing from city selection to detailed itinerary\n- **Tools Integration**: Web search, calculator, and custom research tools\n\n### Code Structure\n- **Main Class**: Single class containing the entire crew setup (~80 lines)\n- **Agent Definitions**: Methods returning configured agent objects\n- **Task Specifications**: Detailed prompts with context variables\n- **Tool Integration**: LangChain-compatible tools with CrewAI decorators\n\n## Prompt Engineering Best Practices\n\n### Agent Motivation\n- Include incentive language (\"If you do your best work, I'll tip you $100\")\n- Frame agents as experts in their domain\n- Provide clear success criteria\n\n### Task Clarity\n- Use specific, actionable language\n- Include context about what the task involves\n- Specify exact output format requirements\n- Leverage template variables for dynamic content\n\n### System Integration\n- Agents can spawn sub-agents within tools\n- Support for human-in-the-loop workflows\n- Compatible with existing LangChain tool ecosystem\n\n## Planning Your Capstone Project\n\n### Problem Identification\n- Look for tasks suitable for partial automation\n- Consider human-in-the-loop scenarios\n- Use ticket system analysis to find patterns\n- Focus on repetitive, well-defined processes\n\n### Solution Design\n- Break down into manageable tasks\n- Plan agent architecture and responsibilities\n- Choose appropriate LLM strategy\n- Define measurement metrics early\n\n### Development Approach\n- Start with minimal viable system\n- Implement fast feedback loops\n- Focus on complete user workflow first\n- Iterate based on testing and feedback",
          "codeExamples": [
            {
              "id": "code-8-1-1",
              "title": "CrewAI Basic Setup",
              "language": "python",
              "code": "from crewai import Crew, Agent, Task\nfrom crewai.tools import tool\n\n# Define agent with role, goal, and backstory\ncity_selector = Agent(\n    role=\"City Selector\",\n    goal=\"Select the best city that matches the user's preferences\",\n    backstory=\"You are an expert travel advisor with extensive knowledge of global destinations...\",\n    tools=[search_tool, calculator_tool],\n    verbose=True\n)\n\n# Define task with detailed instructions\nselect_city_task = Task(\n    description=\"\"\"\n    Analyze and select the best city for travel based on:\n    - User preferences: {interests}\n    - Travel dates: {travel_dates}\n    - Starting location: {origin}\n    \n    Consider weather, cultural attractions, and accessibility.\n    Provide detailed reasoning for your recommendation.\n    \"\"\",\n    agent=city_selector\n)"
            },
            {
              "id": "code-8-1-2",
              "title": "Crew Assembly and Execution",
              "language": "python",
              "code": "# Create crew with agents and tasks\ntravel_crew = Crew(\n    agents=[city_selector, researcher, planner],\n    tasks=[select_city_task, research_task, plan_task],\n    verbose=True\n)\n\n# Execute the crew with user inputs\nresult = travel_crew.kickoff({\n    'origin': 'S\u00e3o Paulo, Brazil',\n    'destinations': 'New York, London, Prague',\n    'travel_dates': 'anytime in the next three weeks',\n    'interests': 'museums and architecture'\n})"
            },
            {
              "id": "code-8-1-3",
              "title": "Custom Tool with Agent Integration",
              "language": "python",
              "code": "@tool\ndef research_destination(query: str) -> str:\n    \"\"\"Research detailed information about a destination\"\"\"\n    \n    # Create specialized research agent within tool\n    researcher = Agent(\n        role=\"Destination Researcher\",\n        goal=\"Provide comprehensive destination information\",\n        backstory=\"Expert in travel research and local insights\",\n        tools=[web_search_tool]\n    )\n    \n    # Execute research task\n    research_task = Task(\n        description=f\"Research and compile information about: {query}\",\n        agent=researcher\n    )\n    \n    return research_task.execute()"
            },
            {
              "id": "code-8-1-4",
              "title": "Task Definition with Context Variables",
              "language": "python",
              "code": "def plan_itinerary_task(self, agent):\n    return Task(\n        description=dedent(\"\"\"\n            Create a detailed 7-day travel itinerary for {destination}.\n            \n            Include:\n            - Daily schedules with specific attractions\n            - Restaurant recommendations for each meal\n            - Weather considerations and clothing suggestions\n            - Transportation options between locations\n            - Estimated costs for activities\n            \n            Format as a day-by-day breakdown with times and locations.\n            \n            If you do your best work, I'll tip you $100.\n        \"\"\"),\n        agent=agent,\n        context_variables=['destination', 'interests', 'travel_dates']\n    )"
            }
          ],
          "checklist": [
            {
              "id": "check-8-1-1",
              "text": "Install CrewAI framework and dependencies",
              "completed": false
            },
            {
              "id": "check-8-1-2",
              "text": "Define agent roles, goals, and backstories for your use case",
              "completed": false
            },
            {
              "id": "check-8-1-3",
              "text": "Create task descriptions with clear instructions and expected outputs",
              "completed": false
            },
            {
              "id": "check-8-1-4",
              "text": "Implement crew structure with agent and task assignments",
              "completed": false
            },
            {
              "id": "check-8-1-5",
              "text": "Test crew execution with sample inputs",
              "completed": false
            },
            {
              "id": "check-8-1-6",
              "text": "Add appropriate tools and integrations for your agents",
              "completed": false
            },
            {
              "id": "check-8-1-7",
              "text": "Implement verbose mode for debugging and monitoring",
              "completed": false
            },
            {
              "id": "check-8-1-8",
              "text": "Plan capstone project problem identification and solution design",
              "completed": false
            }
          ],
          "timestamps": [
            {
              "time": "0:06:05",
              "label": "Course Introduction",
              "description": "What is CrewAI and why use it for agent workflows"
            },
            {
              "time": "0:12:15",
              "label": "Framework Philosophy",
              "description": "Opinionated approach vs custom implementations"
            },
            {
              "time": "0:18:44",
              "label": "Hierarchical Teams",
              "description": "Supervisor pattern and team structure explanation"
            },
            {
              "time": "0:25:14",
              "label": "Core Components",
              "description": "Agents, tasks, processes, crews, and memory systems"
            },
            {
              "time": "0:29:48",
              "label": "Trip Planner Demo",
              "description": "Code walkthrough of complete CrewAI implementation"
            },
            {
              "time": "0:37:14",
              "label": "Agent Definitions",
              "description": "Role, goal, backstory, and tool configuration"
            },
            {
              "time": "0:44:23",
              "label": "Task Specifications",
              "description": "Detailed task prompts and context variables"
            },
            {
              "time": "0:52:30",
              "label": "Live Execution",
              "description": "Running the trip planner with real inputs"
            },
            {
              "time": "0:58:07",
              "label": "Capstone Planning",
              "description": "Project identification and solution design strategies"
            },
            {
              "time": "1:04:18",
              "label": "API Configuration",
              "description": "Questions about CrewAI and project development"
            }
          ],
          "videoPath": "cohort_2/week_8_class_1_2024-07-15.mp4"
        },
        {
          "id": "lesson-8-2",
          "title": "Advanced Prompt Engineering & Optimization",
          "duration": "1:33:00",
          "videoUrl": "/videos/cohort_2/week_08/week_8_class_2_2024-07-17.mp4",
          "videoId": "week_8_class_2_2024-07-17",
          "content": "# Capstone Project Development & Kevin Architecture\n\n## Overview\n\nThis lesson introduces the capstone project structure and demonstrates Kevin, a sophisticated AI-powered development agent system. Kevin showcases advanced prompt engineering, multi-agent workflows, and production-ready architecture for automating software development tasks.\n\n## Capstone Project Framework\n\n### Project Structure\n- **Problem Identification**: Find tasks suitable for partial automation\n- **Solution Design**: Plan agent architecture and measurement metrics\n- **Prototype Development**: Build minimal viable system with complete user loop\n- **Testing & Iteration**: Implement feedback cycles and continuous improvement\n\n### Problem Selection Criteria\n- Focus on partial automation rather than complete replacement\n- Human-in-the-loop systems for quality control\n- Repetitive tasks with consistent patterns\n- Measurable impact and clear success metrics\n\n### Planning Best Practices\n- Start with 3-4 sentence problem description\n- Include visual architecture diagram\n- Define specific measurement approaches\n- Avoid over-planning - adapt as you learn\n\n## Kevin: AI Development Agent\n\n### System Architecture\nKevin is a manager agent designed to handle Jira development tickets (3 story points or less) under developer supervision. The system transforms 3-point tickets into 1-point tickets by automating initial implementation while maintaining human oversight.\n\n### Core Workflow\n1. **Generate**: Parse requirements and write initial code\n2. **Review**: Self-assessment and improvement recommendations\n3. **Test**: Containerized execution and test suite validation\n4. **PR**: Automated pull request creation for human review\n\n### API Architecture\n\n#### Generate Endpoint\n- **Parse Query**: Determine task type (create, update, or fix)\n- **RAG Integration**: Retrieve relevant codebase context\n- **Code Generation**: LLM produces code with dependencies and documentation\n- **Output**: Structured response with code, descriptions, and metadata\n\n#### Review Endpoint\n- **Self-Assessment**: Kevin reviews its own generated code\n- **Improvement Recommendations**: Suggests specific enhancements\n- **Human Decision Point**: Developer chooses whether to iterate or proceed\n- **Feedback Loop**: Route recommendations back to generate for improvements\n\n#### Test Endpoint\n- **Containerized Execution**: Run code in isolated Docker environment\n- **Test Suite Integration**: Execute existing and generated tests\n- **Error Analysis**: Identify failures and route back to generate for fixes\n- **Quality Gates**: Define thresholds for acceptable test results\n\n#### PR Endpoint\n- **GitHub Integration**: Use GitHub API to create pull requests\n- **Template Generation**: Structure PR description and context\n- **Human Review**: Developer reviews and merges after validation\n- **Version Control**: Maintain proper Git workflow and branch management\n\n## Advanced RAG Implementation\n\n### Codebase Vectorization\n- **GitHub Integration**: Connect to repositories via GitHub API\n- **Pinecone Vector DB**: Store code embeddings with cosine distance\n- **Recursive Text Splitting**: Handle large files with 1000 token chunks and 100 token overlap\n- **Semantic Search**: Retrieve relevant code context for generation tasks\n\n### Embedding Strategy\n- **Model**: OpenAI embeddings with 1536 dimensions\n- **Indexing**: Create and maintain vector indices for codebase\n- **Query Processing**: Semantic similarity matching for context retrieval\n- **Performance**: Optimized for real-time code generation workflows\n\n## LangServe Integration\n\n### API Framework\n- **FastAPI Backend**: High-performance async API endpoints\n- **Auto-Documentation**: Generated API docs with interactive playground\n- **Route Organization**: Separate endpoints for each workflow stage\n- **Schema Validation**: Input/output typing and validation\n\n### Development Benefits\n- **Rapid Prototyping**: Built-in UI for testing and development\n- **Documentation**: Automatic API documentation generation\n- **Deployment**: Production-ready scaling and monitoring\n- **Integration**: Compatible with existing development workflows\n\n## Production Considerations\n\n### Safety Measures\n- **Containerized Execution**: Isolate AI-generated code execution\n- **Human Oversight**: Mandatory review before production deployment\n- **Error Handling**: Robust failure modes and recovery strategies\n- **Version Control**: Proper Git workflow with branch protection\n\n### Quality Assurance\n- **Test Integration**: Run existing test suites against new code\n- **Performance Monitoring**: Track system effectiveness and efficiency\n- **Feedback Loops**: Continuous improvement through usage data\n- **Human Validation**: Multiple checkpoints for quality control\n\n### Scalability Features\n- **Microservice Architecture**: Independent scaling of components\n- **API Rate Limiting**: Prevent resource exhaustion\n- **Monitoring Integration**: Observability and debugging capabilities\n- **Multi-User Support**: Concurrent development workflows\n\n## Prompt Engineering Insights\n\n### Context Management\n- **Codebase Context**: Include relevant existing code in prompts\n- **Task Specificity**: Clear instructions for create vs update vs fix operations\n- **Output Formatting**: Structured responses with code, documentation, and metadata\n- **Error Context**: Include error messages and stack traces for fixes\n\n### Multi-Agent Coordination\n- **Role Separation**: Distinct agents for generation, review, and testing\n- **State Management**: Pass context between workflow stages\n- **Feedback Integration**: Route recommendations and errors between agents\n- **Human Integration**: Strategic human decision points in workflow\n\n## Development Timeline\n\n### Week 9: Architecture & Design\n- Identify problem and create solution design\n- Architect agent system and define interfaces\n- Plan measurement metrics and success criteria\n\n### Week 10: Implementation & Feedback\n- Build minimal viable prototype\n- Implement core workflow with basic functionality\n- Test with real scenarios and gather feedback\n\n### Final Showcase\n- Demonstrate working prototype\n- Present architecture decisions and learnings\n- Share results and future improvement plans\n\n## Continued Support\n- **Six-month access**: Retain course materials and Slack support\n- **Certificate requirements**: Submit all projects within timeline\n- **Implementation support**: Apply projects at your company\n- **Success sharing**: Report real-world impact and results",
          "codeExamples": [
            {
              "id": "code-8-2-1",
              "title": "Kevin Generate Endpoint Structure",
              "language": "python",
              "code": "@app.post(\"/generate\")\nasync def generate_code(request: GenerateRequest):\n    # Parse the query to determine task type\n    task_type = parse_query(request.query)\n    \n    # Retrieve relevant code context from RAG\n    context = await get_code_context(request.query, request.repo_id)\n    \n    # Generate code using LLM with context\n    result = await llm_generate(\n        task_type=task_type,\n        context=context,\n        query=request.query\n    )\n    \n    return GenerateResponse(\n        code=result.code,\n        dependencies=result.dependencies,\n        description=result.description,\n        task_type=task_type\n    )"
            },
            {
              "id": "code-8-2-2",
              "title": "RAG Vector Database Setup",
              "language": "python",
              "code": "import pinecone\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n# Initialize Pinecone vector database\npinecone.init(api_key=PINECONE_API_KEY)\nindex = pinecone.Index(\"kevin-codebase\")\n\n# Setup embeddings and text splitter\nembeddings = OpenAIEmbeddings()\nsplitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=100\n)\n\n# Vectorize repository files\ndef vectorize_repository(repo_path):\n    for file_path in get_code_files(repo_path):\n        content = read_file(file_path)\n        chunks = splitter.split_text(content)\n        \n        for chunk in chunks:\n            vector = embeddings.embed_query(chunk)\n            index.upsert([(file_path, vector, {\"content\": chunk})])"
            },
            {
              "id": "code-8-2-3",
              "title": "Docker Test Execution",
              "language": "python",
              "code": "import docker\nimport asyncio\n\nasync def test_generated_code(code, test_suite):\n    client = docker.from_env()\n    \n    # Create isolated container environment\n    container = client.containers.run(\n        \"python:3.9\",\n        command=\"/bin/bash\",\n        detach=True,\n        working_dir=\"/app\",\n        volumes={test_dir: {\"bind\": \"/app\", \"mode\": \"rw\"}}\n    )\n    \n    try:\n        # Write generated code to container\n        exec_result = container.exec_run(\n            f\"echo '{code}' > generated_module.py\"\n        )\n        \n        # Run test suite\n        test_result = container.exec_run(\n            \"python -m pytest test_suite.py -v\"\n        )\n        \n        return {\n            \"exit_code\": test_result.exit_code,\n            \"output\": test_result.output.decode(),\n            \"passed\": test_result.exit_code == 0\n        }\n        \n    finally:\n        container.stop()\n        container.remove()"
            },
            {
              "id": "code-8-2-4",
              "title": "GitHub PR Creation",
              "language": "python",
              "code": "from github import Github\n\nasync def create_pull_request(repo_name, branch_name, title, description, code_changes):\n    g = Github(GITHUB_TOKEN)\n    repo = g.get_repo(repo_name)\n    \n    # Create new branch\n    source_branch = repo.get_branch(\"main\")\n    repo.create_git_ref(\n        ref=f\"refs/heads/{branch_name}\",\n        sha=source_branch.commit.sha\n    )\n    \n    # Commit changes to branch\n    for file_path, content in code_changes.items():\n        repo.update_file(\n            path=file_path,\n            message=f\"Kevin: Update {file_path}\",\n            content=content,\n            sha=repo.get_contents(file_path, ref=branch_name).sha,\n            branch=branch_name\n        )\n    \n    # Create pull request\n    pr = repo.create_pull(\n        title=title,\n        body=description,\n        head=branch_name,\n        base=\"main\"\n    )\n    \n    return pr.html_url"
            },
            {
              "id": "code-8-2-5",
              "title": "Connect Four Game Generation",
              "language": "javascript",
              "code": "// Generated Connect Four game implementation\nclass ConnectFour {\n    constructor() {\n        this.board = Array(6).fill().map(() => Array(7).fill(0));\n        this.currentPlayer = 1;\n        this.gameOver = false;\n    }\n    \n    dropPiece(column) {\n        if (this.gameOver || column < 0 || column >= 7) return false;\n        \n        for (let row = 5; row >= 0; row--) {\n            if (this.board[row][column] === 0) {\n                this.board[row][column] = this.currentPlayer;\n                \n                if (this.checkWin(row, column)) {\n                    this.gameOver = true;\n                    return { winner: this.currentPlayer, position: [row, column] };\n                }\n                \n                this.currentPlayer = this.currentPlayer === 1 ? 2 : 1;\n                return { success: true, position: [row, column] };\n            }\n        }\n        \n        return false; // Column full\n    }\n    \n    checkWin(row, col) {\n        const directions = [[0,1], [1,0], [1,1], [1,-1]];\n        \n        for (let [dr, dc] of directions) {\n            let count = 1;\n            \n            // Check positive direction\n            for (let i = 1; i < 4; i++) {\n                const r = row + dr * i, c = col + dc * i;\n                if (r >= 0 && r < 6 && c >= 0 && c < 7 && \n                    this.board[r][c] === this.currentPlayer) {\n                    count++;\n                } else break;\n            }\n            \n            // Check negative direction\n            for (let i = 1; i < 4; i++) {\n                const r = row - dr * i, c = col - dc * i;\n                if (r >= 0 && r < 6 && c >= 0 && c < 7 && \n                    this.board[r][c] === this.currentPlayer) {\n                    count++;\n                } else break;\n            }\n            \n            if (count >= 4) return true;\n        }\n        \n        return false;\n    }\n}"
            }
          ],
          "checklist": [
            {
              "id": "check-8-2-1",
              "text": "Identify a specific problem suitable for AI automation at your company",
              "completed": false
            },
            {
              "id": "check-8-2-2",
              "text": "Design solution architecture with agent roles and responsibilities",
              "completed": false
            },
            {
              "id": "check-8-2-3",
              "text": "Create visual diagram of system architecture and data flow",
              "completed": false
            },
            {
              "id": "check-8-2-4",
              "text": "Define measurement metrics for system effectiveness",
              "completed": false
            },
            {
              "id": "check-8-2-5",
              "text": "Set up development environment with LangServe and FastAPI",
              "completed": false
            },
            {
              "id": "check-8-2-6",
              "text": "Implement basic RAG system for context retrieval",
              "completed": false
            },
            {
              "id": "check-8-2-7",
              "text": "Build minimal viable prototype with complete user workflow",
              "completed": false
            },
            {
              "id": "check-8-2-8",
              "text": "Add containerized testing environment for safety",
              "completed": false
            },
            {
              "id": "check-8-2-9",
              "text": "Implement human-in-the-loop review checkpoints",
              "completed": false
            },
            {
              "id": "check-8-2-10",
              "text": "Test prototype with real scenarios and gather feedback",
              "completed": false
            }
          ],
          "timestamps": [
            {
              "time": "0:00:10",
              "label": "Course Introduction",
              "description": "Overview of context and production and learning objectives"
            },
            {
              "time": "0:07:34",
              "label": "Problem Identification",
              "description": "Strategies for finding suitable automation problems"
            },
            {
              "time": "0:14:22",
              "label": "Solution Design",
              "description": "Planning approach and measurement metrics"
            },
            {
              "time": "0:18:46",
              "label": "Test API",
              "description": "Introduction to Kevin AI development agent system"
            },
            {
              "time": "0:24:37",
              "label": "Kevin Architecture",
              "description": "Detailed system design and workflow explanation"
            },
            {
              "time": "0:32:14",
              "label": "Generate Endpoint",
              "description": "Code generation workflow and RAG integration"
            },
            {
              "time": "0:39:28",
              "label": "Test API",
              "description": "Validate context and production functionality and edge cases"
            },
            {
              "time": "0:46:53",
              "label": "Testing Framework",
              "description": "Containerized execution and test suite integration"
            },
            {
              "time": "0:54:17",
              "label": "PR Creation",
              "description": "Learn context and production implementation techniques"
            },
            {
              "time": "1:01:42",
              "label": "Live Demo",
              "description": "Kevin generating Connect Four game implementation"
            },
            {
              "time": "1:12:36",
              "label": "RAG Implementation",
              "description": "Codebase vectorization and context retrieval"
            },
            {
              "time": "1:21:05",
              "label": "Timeline & Support",
              "description": "Project timeline and continued support structure"
            }
          ],
          "videoPath": "cohort_2/week_8_class_2_2024-07-17.mp4"
        }
      ]
    },
    {
      "id": "week-9",
      "title": "Week 9: Advanced Topics & Integration",
      "description": "Architecting AI applications, choosing the right patterns, and production deployment strategies",
      "lessons": [
        {
          "id": "lesson-9-1",
          "title": "Cutting-Edge Techniques & Business Integration",
          "duration": "0:35:59",
          "videoUrl": "/videos/cohort_2/week_09/week_9_class_1_2024-07-22.mp4",
          "videoId": "week_9_class_1_2024-07-22",
          "content": "# Architecting AI Applications: Pro Tips and Best Practices\n\n## Overview\n\nThis lesson focuses on designing and architecting AI applications from a practical perspective. Instead of diving into code, we'll explore strategic approaches to building production-ready AI systems that deliver real business value.\n\n## Key Topics Covered\n\n### 1. Identifying and Working with Subject Matter Experts\n\nBefore starting any AI project, it's crucial to have your experts in place:\n\n- **Subject matter experts** understand the workflow and domain\n- **Domain expertise** is essential whether building AI or traditional software\n- **Real-world example**: Building a food service application requires consulting:\n  - Point of sale operators\n  - Kitchen staff\n  - Process managers\n  - Order takers and drive-through operators\n\n### Case Study: Engineering Document RAG System\n\nA real example from a team working on a RAG system over terabytes of engineering documents:\n\n- **Initial approach**: The team wanted to build a RAG system immediately\n- **Better approach**: First tested if an LLM could already answer their questions\n- **Expert feedback revealed**:\n  - Responses needed more detail and specificity\n  - 70-80 years of proprietary engineering knowledge should be incorporated\n  - Generic LLM responses weren't sufficient for specialized antenna design queries\n\n### 2. Building Evaluation into Your Pipeline\n\n#### Continuous Evaluation Strategy\n\n- Implement consistent evaluation metrics throughout development\n- Use simple feedback mechanisms: \"like/dislike\" responses\n- Consider Reinforcement Learning from Human Feedback (RLHF)\n- Regular intervals for checking data performance\n\n#### Key Evaluation Metrics\n\n- **Factual correctness**: Is the information accurate?\n- **Level of detail**: Does it meet user expectations?\n- **Adoption metrics**: Are users actually using the system?\n- **Business value**: Does it solve the intended problem?\n\n### 3. Facilitating Continuous Improvement\n\n#### LLMOps/MLOps Principles\n\n- Build repeatable and reproducible operational processes\n- Streamline continuous improvement cycles\n- Establish clear processes for:\n  - Updating prompts in production\n  - Model versioning and deployment\n  - Performance monitoring\n\n#### The Iterative Process\n\n1. Understand the business problem\n2. Understand required data and outputs\n3. Ask clarifying questions\n4. Prepare and process data\n5. Create evaluation cycles\n6. Deploy the solution\n7. Iterate based on feedback\n\n## Choosing the Right AI Architecture\n\n### Different Patterns for Different Problems\n\n#### Graphs\n- **Best for**: Complex workflows requiring looping or state management\n- **Use cases**: \n  - Workflows that cycle through various steps\n  - Asynchronous task handling\n  - Systems needing to remember state\n\n#### Agents\n- **Best for**: Tasks requiring reasoning and decision-making\n- **Example**: Research report generation\n  - Research agent gathers information\n  - Fact-checking agent verifies accuracy\n  - Writing agent creates the report\n  - Editor agent refines the output\n  - Supervisor manages the process\n\n#### Chains\n- **Best for**: Straightforward, linear LLM workflows\n- **Example**: Weather query processing\n  - Parse natural language input\n  - Extract structured data\n  - Make API call\n  - Return formatted response\n\n#### Single LLM Calls\n- **Best for**: Simple, one-time queries\n- **Characteristics**:\n  - Specific prompts with few or no examples\n  - Single response needed\n  - Classic ChatGPT use case\n\n#### Manual Code\n- **Best for**: Deterministic operations\n- **Example**: Bank balance queries don't need LLM processing\n\n#### Combination Approaches\n- Most real-world applications combine multiple patterns\n- Mix and match based on specific requirements\n\n## Choosing the Right LLM\n\n### Model Selection Considerations\n\n#### Performance vs. Cost Trade-offs\n\n- Different LLMs behave differently (GPT-4 vs. Mixtral vs. Gemini)\n- Test multiple models for your specific use case\n- Consider both performance and budget constraints\n\n#### Strategic Model Deployment\n\n- **Premium models** (GPT-4, Claude): For complex writing and reasoning\n- **Local models** (Llama 3): For bulk processing and summarization\n- **Specialized models**: For domain-specific tasks\n\n#### Objective Measurement Criteria\n\n- Model output quality\n- Cost per token\n- Response latency\n- Meeting performance thresholds within budget\n\n## Developing Prompts with DSPy\n\n### When You Don't Have Enough Data for Fine-Tuning\n\nInstruction fine-tuning requires 5,000-10,000 examples, which isn't feasible for many projects. DSPy offers an alternative:\n\n### DSPy Framework Process\n\n1. **Define task**: Specify input/output behavior\n2. **Outline pipeline**: Start with simple processes\n3. **Test extensively**: Try different examples and models\n4. **Generate training data**: DSPy helps create evaluation datasets\n5. **Define metrics**: Establish quality measurements\n6. **Run evaluations**: Identify baseline performance\n7. **Optimize instructions**: DSPy optimizers improve prompts automatically\n8. **Iterate continuously**: Refine based on results\n\n### DSPy Limitations\n\n- Tool calling isn't well supported yet\n- Best for simpler prompt patterns\n- More complex prompts may require workarounds\n\n## Production Best Practices\n\n### Human Verification and Transparency\n\n#### Always Include Human Review\n\n- Get expert verification of AI-generated content\n- Ask: \"Are these outputs correct?\"\n- Identify missing elements or inaccuracies\n- Build trust through validation\n\n#### Be Transparent About AI Usage\n\n- Users interact differently with AI vs. human content\n- Clear labeling prevents misconceptions\n- Example: Amazon's AI-generated review summaries\n  - Users know it's AI-generated\n  - They still find value but verify with individual reviews\n\n### Prompt Management Strategies\n\n#### Store Prompts Outside the Codebase\n\n- Prompts generate significant data volume\n- Enable version control for prompts\n- Consider prompt management platforms\n- Treat prompts like models - version and release them\n\n#### Observability Platforms\n\n- **LangSmith**: Comprehensive tracing and monitoring\n- **Weights & Biases**: Popular for ML experiments\n- **LangFuse**: Open-source alternative to LangSmith\n- Store prompt data and evaluate performance systematically\n\n## Real-World Implementation Example\n\n### Code Analysis for Plugin Verification\n\nA practical use case discussed in class:\n\n**Problem**: E-commerce platform needs to analyze partner plugin code\n\n**Requirements**:\n- Security vulnerability detection\n- Pattern recognition for malicious code\n- External request validation\n- Code quality assessment\n- Version comparison to detect major changes\n\n**Solution Approach**:\n1. Initial AI analysis for common issues\n2. Flag items requiring human review\n3. Automated pre-analysis to reduce manual work\n4. Detection of complete code replacements vs. updates\n\n**Framework Considerations**:\n- CrewAI for rapid prototyping\n- Haystack as an alternative\n- LangGraph for enterprise-grade solutions\n- Start simple, then migrate to more robust solutions\n\n## Key Takeaways\n\n1. **Always involve domain experts** throughout the development process\n2. **Build evaluation into your pipeline** from day one\n3. **Choose the right architecture** for your specific problem\n4. **Test multiple LLMs** to find the best fit for performance and cost\n5. **Use DSPy** when you lack data for fine-tuning\n6. **Maintain transparency** about AI-generated content\n7. **Implement proper prompt management** and observability\n8. **Start with simpler frameworks** (CrewAI) for MVPs, then migrate to production-grade solutions\n9. **Focus on business value**, not just technical implementation\n10. **Iterate continuously** based on user feedback and evaluation metrics",
          "codeExamples": [
            {
              "id": "code-9-1-1",
              "title": "Setting Up Evaluation Metrics",
              "language": "python",
              "code": "from typing import Dict, List, Any\nfrom dataclasses import dataclass\nimport json\nfrom datetime import datetime\n\n@dataclass\nclass EvaluationMetrics:\n    \"\"\"Store evaluation metrics for AI responses\"\"\"\n    factual_correctness: bool\n    detail_level: int  # 1-5 scale\n    user_satisfaction: bool\n    response_time: float\n    model_used: str\n    cost_per_request: float\n    \nclass AIEvaluationPipeline:\n    def __init__(self):\n        self.metrics_history = []\n        \n    def evaluate_response(self, \n                         prompt: str, \n                         response: str, \n                         expert_feedback: Dict[str, Any]) -> EvaluationMetrics:\n        \"\"\"Evaluate an AI response with expert feedback\"\"\"\n        \n        metrics = EvaluationMetrics(\n            factual_correctness=expert_feedback.get('is_correct', False),\n            detail_level=expert_feedback.get('detail_score', 3),\n            user_satisfaction=expert_feedback.get('satisfied', False),\n            response_time=expert_feedback.get('response_time', 0.0),\n            model_used=expert_feedback.get('model', 'unknown'),\n            cost_per_request=self._calculate_cost(response, expert_feedback.get('model'))\n        )\n        \n        self.metrics_history.append({\n            'timestamp': datetime.now().isoformat(),\n            'prompt': prompt,\n            'metrics': metrics.__dict__\n        })\n        \n        return metrics\n    \n    def _calculate_cost(self, response: str, model: str) -> float:\n        \"\"\"Calculate cost based on model and token usage\"\"\"\n        # Simplified cost calculation\n        token_count = len(response.split()) * 1.3  # Rough token estimate\n        \n        cost_per_1k_tokens = {\n            'gpt-4': 0.03,\n            'gpt-3.5-turbo': 0.002,\n            'claude': 0.025,\n            'llama-local': 0.0\n        }\n        \n        rate = cost_per_1k_tokens.get(model, 0.01)\n        return (token_count / 1000) * rate\n    \n    def get_performance_summary(self) -> Dict[str, Any]:\n        \"\"\"Generate summary statistics from evaluation history\"\"\"\n        if not self.metrics_history:\n            return {}\n            \n        total = len(self.metrics_history)\n        correct = sum(1 for m in self.metrics_history \n                     if m['metrics']['factual_correctness'])\n        satisfied = sum(1 for m in self.metrics_history \n                       if m['metrics']['user_satisfaction'])\n        \n        return {\n            'total_evaluations': total,\n            'accuracy_rate': correct / total if total > 0 else 0,\n            'satisfaction_rate': satisfied / total if total > 0 else 0,\n            'average_detail_score': sum(m['metrics']['detail_level'] \n                                       for m in self.metrics_history) / total,\n            'total_cost': sum(m['metrics']['cost_per_request'] \n                            for m in self.metrics_history)\n        }"
            },
            {
              "id": "code-9-1-2",
              "title": "Architecture Pattern Selection Helper",
              "language": "python",
              "code": "from enum import Enum\nfrom typing import List, Optional\n\nclass ArchitecturePattern(Enum):\n    GRAPH = \"graph\"\n    AGENT = \"agent\"\n    CHAIN = \"chain\"\n    SINGLE_CALL = \"single_call\"\n    MANUAL = \"manual\"\n    COMBINATION = \"combination\"\n\nclass PatternSelector:\n    \"\"\"Help select the right architecture pattern for AI tasks\"\"\"\n    \n    @staticmethod\n    def recommend_pattern(requirements: dict) -> ArchitecturePattern:\n        \"\"\"\n        Recommend architecture pattern based on requirements\n        \n        Args:\n            requirements: Dict with keys like 'needs_reasoning', 'has_loops',\n                        'is_linear', 'is_deterministic', etc.\n        \"\"\"\n        \n        # Check for deterministic operations first\n        if requirements.get('is_deterministic', False):\n            return ArchitecturePattern.MANUAL\n            \n        # Check for complex workflows with loops\n        if requirements.get('has_loops', False) or \\\n           requirements.get('needs_state', False):\n            return ArchitecturePattern.GRAPH\n            \n        # Check for reasoning requirements\n        if requirements.get('needs_reasoning', False) or \\\n           requirements.get('multiple_perspectives', False):\n            return ArchitecturePattern.AGENT\n            \n        # Check for linear workflows\n        if requirements.get('is_linear', False) and \\\n           requirements.get('multiple_steps', False):\n            return ArchitecturePattern.CHAIN\n            \n        # Simple one-off queries\n        if requirements.get('single_response', False):\n            return ArchitecturePattern.SINGLE_CALL\n            \n        # Default to combination for complex requirements\n        return ArchitecturePattern.COMBINATION\n    \n    @staticmethod\n    def get_pattern_description(pattern: ArchitecturePattern) -> str:\n        \"\"\"Get detailed description of when to use each pattern\"\"\"\n        \n        descriptions = {\n            ArchitecturePattern.GRAPH: \n                \"Use for complex workflows with loops, state management, \"\n                \"and asynchronous task handling. Example: Multi-stage \"\n                \"document processing with review cycles.\",\n                \n            ArchitecturePattern.AGENT:\n                \"Use when tasks require reasoning, decision-making, or \"\n                \"multiple perspectives. Example: Research report generation \"\n                \"with fact-checking and editing agents.\",\n                \n            ArchitecturePattern.CHAIN:\n                \"Use for straightforward, linear workflows with clear \"\n                \"steps. Example: Query processing -> API call -> Response \"\n                \"formatting.\",\n                \n            ArchitecturePattern.SINGLE_CALL:\n                \"Use for simple, one-time queries with specific prompts. \"\n                \"Example: Basic Q&A or text generation tasks.\",\n                \n            ArchitecturePattern.MANUAL:\n                \"Use for deterministic operations that don't need AI. \"\n                \"Example: Database queries, calculations, API calls.\",\n                \n            ArchitecturePattern.COMBINATION:\n                \"Use when mixing multiple patterns for different parts \"\n                \"of the system. Most production systems use this approach.\"\n        }\n        \n        return descriptions.get(pattern, \"Unknown pattern\")\n\n# Example usage\nif __name__ == \"__main__\":\n    # Research task requirements\n    research_requirements = {\n        'needs_reasoning': True,\n        'multiple_perspectives': True,\n        'has_loops': False,\n        'is_linear': False\n    }\n    \n    selector = PatternSelector()\n    pattern = selector.recommend_pattern(research_requirements)\n    print(f\"Recommended: {pattern.value}\")\n    print(selector.get_pattern_description(pattern))"
            },
            {
              "id": "code-9-1-3",
              "title": "Model Selection and Cost Optimization",
              "language": "python",
              "code": "from typing import Dict, List, Tuple\nimport time\n\nclass ModelSelector:\n    \"\"\"Select optimal model based on task requirements and budget\"\"\"\n    \n    def __init__(self):\n        self.models = {\n            'gpt-4': {\n                'cost_per_1k_tokens': 0.03,\n                'quality_score': 0.95,\n                'speed': 'slow',\n                'best_for': ['complex_reasoning', 'creative_writing']\n            },\n            'gpt-3.5-turbo': {\n                'cost_per_1k_tokens': 0.002,\n                'quality_score': 0.75,\n                'speed': 'fast',\n                'best_for': ['simple_queries', 'basic_summarization']\n            },\n            'claude-2': {\n                'cost_per_1k_tokens': 0.025,\n                'quality_score': 0.90,\n                'speed': 'medium',\n                'best_for': ['long_context', 'analysis']\n            },\n            'llama-3-local': {\n                'cost_per_1k_tokens': 0.0,\n                'quality_score': 0.70,\n                'speed': 'very_fast',\n                'best_for': ['bulk_processing', 'privacy_sensitive']\n            }\n        }\n    \n    def select_model(self, \n                    task_type: str,\n                    budget_per_request: float,\n                    quality_threshold: float = 0.7,\n                    estimated_tokens: int = 1000) -> Tuple[str, Dict]:\n        \"\"\"\n        Select the best model for a given task and budget\n        \n        Returns:\n            Tuple of (model_name, model_info)\n        \"\"\"\n        \n        suitable_models = []\n        \n        for model_name, info in self.models.items():\n            # Check if model is suitable for task\n            if task_type in info['best_for'] or task_type == 'general':\n                # Calculate cost for this request\n                cost = (estimated_tokens / 1000) * info['cost_per_1k_tokens']\n                \n                # Check budget and quality constraints\n                if cost <= budget_per_request and info['quality_score'] >= quality_threshold:\n                    suitable_models.append((model_name, info, cost))\n        \n        if not suitable_models:\n            # Fallback to cheapest model if no suitable model found\n            return self._get_cheapest_model()\n        \n        # Sort by quality score (descending) and return best\n        suitable_models.sort(key=lambda x: x[1]['quality_score'], reverse=True)\n        best_model = suitable_models[0]\n        \n        return best_model[0], {\n            **best_model[1],\n            'estimated_cost': best_model[2]\n        }\n    \n    def _get_cheapest_model(self) -> Tuple[str, Dict]:\n        \"\"\"Get the cheapest available model\"\"\"\n        cheapest = min(self.models.items(), \n                      key=lambda x: x[1]['cost_per_1k_tokens'])\n        return cheapest\n    \n    def benchmark_models(self, test_prompt: str, test_tasks: List[str]) -> Dict:\n        \"\"\"Benchmark multiple models on the same task\"\"\"\n        results = {}\n        \n        for model_name in self.models:\n            print(f\"Testing {model_name}...\")\n            \n            # Simulate model testing (in reality, you'd call the actual model)\n            start_time = time.time()\n            \n            # Simulate processing time based on speed\n            speed_delays = {'very_fast': 0.1, 'fast': 0.3, 'medium': 0.5, 'slow': 1.0}\n            time.sleep(speed_delays.get(self.models[model_name]['speed'], 0.5))\n            \n            response_time = time.time() - start_time\n            \n            results[model_name] = {\n                'response_time': response_time,\n                'quality_score': self.models[model_name]['quality_score'],\n                'cost_per_1k': self.models[model_name]['cost_per_1k_tokens'],\n                'suitable_tasks': self.models[model_name]['best_for']\n            }\n        \n        return results\n\n# Example usage\nif __name__ == \"__main__\":\n    selector = ModelSelector()\n    \n    # Select model for complex reasoning with budget constraint\n    model, info = selector.select_model(\n        task_type='complex_reasoning',\n        budget_per_request=0.05,\n        quality_threshold=0.8,\n        estimated_tokens=1500\n    )\n    \n    print(f\"Selected model: {model}\")\n    print(f\"Estimated cost: ${info['estimated_cost']:.4f}\")\n    print(f\"Quality score: {info['quality_score']}\")"
            }
          ],
          "checklist": [
            {
              "id": "check-9-1-1",
              "text": "Identify and engage subject matter experts for your AI project",
              "completed": false
            },
            {
              "id": "check-9-1-2",
              "text": "Build evaluation metrics and feedback loops into your pipeline",
              "completed": false
            },
            {
              "id": "check-9-1-3",
              "text": "Choose the appropriate AI architecture pattern for your use case",
              "completed": false
            },
            {
              "id": "check-9-1-4",
              "text": "Test multiple LLMs to find the best performance/cost balance",
              "completed": false
            },
            {
              "id": "check-9-1-5",
              "text": "Implement prompt management and versioning strategies",
              "completed": false
            },
            {
              "id": "check-9-1-6",
              "text": "Set up observability platforms for monitoring AI performance",
              "completed": false
            },
            {
              "id": "check-9-1-7",
              "text": "Establish human verification processes for AI-generated content",
              "completed": false
            },
            {
              "id": "check-9-1-8",
              "text": "Create a continuous improvement process for your AI system",
              "completed": false
            }
          ],
          "timestamps": [
            {
              "time": "0:00:08",
              "label": "Course Introduction",
              "description": "Introduction to architecting AI applications from a pro-tip perspective"
            },
            {
              "time": "0:00:44",
              "label": "RAG Pipeline",
              "description": "Overview of topics: acceptable output, business metrics, identifying experts"
            },
            {
              "time": "0:01:47",
              "label": "Identifying Subject Matter Experts",
              "description": "Critical importance of having experts in place before starting any project"
            },
            {
              "time": "0:03:26",
              "label": "Engineering RAG Case Study",
              "description": "Real-world example of building RAG over terabytes of engineering documents"
            },
            {
              "time": "0:05:54",
              "label": "Building Evaluation into Pipeline",
              "description": "Implementing consistent evaluation metrics and feedback mechanisms"
            },
            {
              "time": "0:08:12",
              "label": "Continuous Improvement & LLMOps",
              "description": "Building repeatable processes for LLMOps and continuous improvement"
            },
            {
              "time": "0:11:06",
              "label": "AI Architecture Patterns",
              "description": "Choosing between graphs, chains, agents, and single LLM calls"
            },
            {
              "time": "0:14:43",
              "label": "Choosing the Right LLM",
              "description": "Model selection strategies based on performance, cost, and use case"
            },
            {
              "time": "0:17:23",
              "label": "RAG Pipeline",
              "description": "Automated prompt engineering when you lack training data"
            },
            {
              "time": "0:20:05",
              "label": "Human Verification & Transparency",
              "description": "Importance of human-in-the-loop and AI content transparency"
            },
            {
              "time": "0:21:54",
              "label": "Prompt Management Systems",
              "description": "Storing prompts outside codebase and version control strategies"
            },
            {
              "time": "0:22:49",
              "label": "Capstone Project Guidelines",
              "description": "Instructions for capstone design and feedback process"
            },
            {
              "time": "0:23:20",
              "label": "Q&A Session Begins",
              "description": "Opening floor for questions and discussion"
            },
            {
              "time": "0:24:00",
              "label": "RAG Pipeline",
              "description": "Marcus's question about Crew AI and framework boundaries"
            },
            {
              "time": "0:26:43",
              "label": "Code Analysis Use Case",
              "description": "E-commerce plugin security analysis with AI"
            },
            {
              "time": "0:31:11",
              "label": "Framework Recommendations",
              "description": "Comparing Crew AI, Haystack, and LangGraph for MVPs"
            },
            {
              "time": "0:35:00",
              "label": "Closing Remarks",
              "description": "Final thoughts and support availability"
            }
          ],
          "resources": [
            {
              "id": "resource-9-1-1",
              "title": "DSPy Framework Documentation",
              "type": "documentation",
              "url": "https://dspy-docs.vercel.app/"
            },
            {
              "id": "resource-9-1-2",
              "title": "LangSmith Observability Platform",
              "type": "tool",
              "url": "https://smith.langchain.com/"
            },
            {
              "id": "resource-9-1-3",
              "title": "CrewAI Framework",
              "type": "framework",
              "url": "https://github.com/joaomdmoura/crewAI"
            },
            {
              "id": "resource-9-1-4",
              "title": "Haystack AI Framework",
              "type": "framework",
              "url": "https://haystack.deepset.ai/"
            },
            {
              "id": "resource-9-1-5",
              "title": "WISER Prompt Engineering Framework",
              "type": "methodology",
              "url": "https://github.com/microsoft/prompt-engineering"
            }
          ],
          "videoPath": "cohort_2/week_9_class_1_2024-07-22.mp4"
        }
      ]
    }
  ]
}