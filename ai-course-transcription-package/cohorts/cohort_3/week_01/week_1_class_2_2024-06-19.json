{
  "source_url": "https://aitra-main.s3.us-east-2.amazonaws.com/afdp_cohort_3_recordings/week_1_class_2_2024-06-19.mp4?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA6ELKOKYDDOCGTW4H%2F20250814%2Fus-east-2%2Fs3%2Faws4_request&X-Amz-Date=20250814T194232Z&X-Amz-Expires=3600&X-Amz-Signature=089ecb5bee708fc57bcf8d2c7ebb709708e87adfadc628dc27c17017b8c5c722&X-Amz-SignedHeaders=host&x-amz-checksum-mode=ENABLED&x-id=GetObject",
  "duration": 4143.904,
  "language": "en",
  "processing_time": 306.51207304000854,
  "segments_count": 854,
  "transcript_text": "[00:04 - 00:09] Okay, so we'll see if anybody else shows up for right now.\n[00:09 - 00:12] It's just going to be you and I.\n[00:12 - 00:17] So I'm going to share my screen here just a second.\n[00:17 - 01:42] Okay, and you're seeing my screen, okay, correct?\n[01:42 - 01:44] Yeah, that's right.\n[01:44 - 02:10] Okay, so the code that I'm going to share real quick,\n[02:10 - 02:13] and if there's anything I'm going to run through it pretty quick.\n[02:13 - 02:16] Since it's just you and I, and if you have any questions,\n[02:16 - 02:21] just pop on the audio and say, so I'm going to set this up.\n[02:21 - 03:34] You'll wake.\n[03:34 - 03:35] Firements up.\n[03:35 - 04:13] X.\n[04:13 - 04:18] And I'm just going to go grab the open AI key that we were using the other day.\n[04:18 - 04:31] Thank you.\n[04:31 - 04:39] It's going to dot in then here.\n[04:39 - 04:40] So we.\n[04:40 - 04:41] Two.\n[04:41 - 04:48] Case.\n[04:48 - 05:01] And got the key.\n[05:01 - 05:06] Okay, so the example code for tonight is a simple chatbot.\n[05:06 - 05:08] It's not done in line chain.\n[05:08 - 05:13] It's just it's using the open API API.\n[05:13 - 05:18] So we have a really simple main file.\n[05:18 - 05:25] We import the summarizing chatbot class that we'll look at here in just a second.\n[05:25 - 05:35] And the only thing we've got to find here is the simple message or class that we import.\n[05:35 - 05:44] And basically you set up where it's going to prompt the user for their input.\n[05:44 - 05:50] And until you type exit, it'll keep prompting for the input.\n[05:51 - 05:56] And it calls that function.\n[05:56 - 06:04] So the core of this is in the chatbot file.\n[06:04 - 06:16] And all we do here is we import Python dot end and load dot ends.\n[06:16 - 06:21] If you're not familiar with that, Julian was talking about it last night.\n[06:21 - 06:24] It's an easy way to handle a dot end file.\n[06:24 - 06:27] So we don't have to manually with whatever keys that we put in here.\n[06:27 - 06:37] We're not having to manually load them all when using an export statement or using a little bit more complicated statement in the shell.\n[06:37 - 06:40] This handles everything for us. So we just go here.\n[06:40 - 06:43] We set our local variable to whatever we want.\n[06:43 - 06:47] OS get ends.\n[06:47 - 06:51] And the open API key and that pulls it directly from my dot end file.\n[06:51 - 06:56] I did it manually beforehand, just in case, but that does function.\n[06:56 - 07:02] I have heard that it was not functioning for some pine cone.\n[07:02 - 07:07] I've never had a problem with with Python dot end by use it quite regularly with work.\n[07:07 - 07:13] But we'll see later on in the course when we start with some of the more complicated code.\n[07:13 - 07:20] We'll be using poetry, which handles both dependencies and environmental variables.\n[07:20 - 07:27] So the need for the dot end for manually exporting the environmental variables will go away.\n[07:27 - 07:30] So we set up our API key here.\n[07:30 - 07:33] And we've imported open AI as well.\n[07:33 - 07:43] And we just set the API key and then we'll define a few functions here.\n[07:43 - 07:48] A generate response function.\n[07:48 - 07:53] So we have a cat completion where we set the model to the engine.\n[07:53 - 07:55] We could put anything we want here.\n[07:55 - 07:57] We've got gpt 3.5 turbo.\n[07:57 - 07:59] We could put 40 or whatnot.\n[07:59 - 08:02] If you don't put anything, it'll.\n[08:02 - 08:07] I believe it defaults to the 3.5 turbo like line chain does.\n[08:07 - 08:08] It may have changed.\n[08:08 - 08:13] But for now, we'll use the 3.5 turbo.\n[08:13 - 08:22] We'll send up our messages with the role as user and the content will be the prompt that it takes in.\n[08:22 - 08:24] We set a max token here.\n[08:24 - 08:33] The context window is obviously a lot bigger, but this will limit the response to 150 tokens.\n[08:33 - 08:44] Probably is around 125 words, maybe about three quarters of a word equals to a token.\n[08:44 - 08:48] This n equals one is just going to get us one response.\n[08:48 - 08:51] We change that to something like three.\n[08:51 - 08:55] It would give us three separate responses.\n[08:55 - 09:13] So what we're going to do is just a way of ending the sequence on a certain you could put a period you could put something like a line's end of line or whatever you wanted.\n[09:13 - 09:18] But in this case, we're going to put 9 and we'll just let it run out to a maximum of 150 tokens.\n[09:18 - 09:20] We can talk about the temperature last night.\n[09:20 - 09:24] We'll set the temperature 0.7 here.\n[09:24 - 09:36] But again, just to go back over the lower the temperature, the less creative the model will be the higher the temperature, the temperature range between 0 and 1 to higher the temperature.\n[09:36 - 09:40] The more creatives the model would be if you would.\n[09:40 - 09:45] And then here we're just looking for choices in the response.\n[09:45 - 09:49] And as I said, we're only generating one choice.\n[09:49 - 09:51] So it's going to select that choice.\n[09:51 - 10:01] And this is just cleaning up the output so that we don't have a large output that open AI would normally return.\n[10:01 - 10:03] So we'll take the content.\n[10:03 - 10:07] We'll just strip off any white space.\n[10:07 - 10:14] And we just got an else in case there was an unexpected response.\n[10:14 - 10:16] And it'll print that out.\n[10:16 - 10:22] And then we have a little bit of air handling here just so that we don't throw off the whole chatbot.\n[10:22 - 10:31] If some sort of an error occurs, if an error occurs, it'll print the error and just return a blank string.\n[10:31 - 10:34] Then we also have a summarize text.\n[10:34 - 10:35] This will come into play.\n[10:35 - 10:40] You'll see at the very end where we try to.\n[10:40 - 10:47] The whole point of this chat, one of the points of this chatbot and we had talked about it last night was saving the context of the message.\n[10:47 - 10:51] Since we're not talking to chat GPT in the UI.\n[10:51 - 11:05] We, if we want to have context in the conversation and when we're hitting it with the API, each API call is not going to have context of the previous API call.\n[11:05 - 11:12] If we don't include what the conversation has been, so we'll keep a running conversation here that we'll see in a bit.\n[11:12 - 11:23] And this summarizes just a way that we're used to control the size of the chat context, if need be.\n[11:23 - 11:30] So it's just a summary prompt that we'll give to open AI if we need to that says summarizes conversation.\n[11:30 - 11:40] And we'll give it the entire chat text and then we go to generate response, which we looked at here.\n[11:40 - 11:50] And then assess complexity is just another prompt that it's asking to assess the complexity of this conversation.\n[11:50 - 11:59] And we'll be joining the history to that and then getting a complexity score.\n[11:59 - 12:08] And the complexity text is just generating the response using that complexity prompt.\n[12:08 - 12:15] And in the response, we're looking for a complexity score.\n[12:15 - 12:23] So if you actually play with this a little bit and put some print statements in, it doesn't always return this prompt could be improved upon we could talk about that later.\n[12:23 - 12:34] Well, what we're doing here is we're asking the the LOM give us a complexity of this conversation at this point.\n[12:34 - 12:38] And in this case, just hoping it returns a numerical score.\n[12:38 - 12:46] So if it does return a score, we'll return that score in the term of an integer.\n[12:46 - 13:00] If it doesn't return anything, maybe it just returns, hey, this conversation is a medium complexity, then we'll return 20.\n[13:00 - 13:11] So then we also have the class that we talked about, and this is what we're importing along with the rest of the code into the main.\n[13:11 - 13:18] So we just have a basic init here. We set a base history length of 10 more of that here in a bit.\n[13:18 - 13:34] And we set our open a i engine to the 3.5 turbo will start with a blank list for the chat history and we'll build up on that list.\n[13:34 - 13:50] And we'll set the base history length to the 10 we've got here get response is we're simply going to join.\n[13:50 - 13:58] Create a variable chat history string and we will add the chat history to the response.\n[13:58 - 14:05] So as we build the chat, it'll keep adding the history in front of the actual prompt.\n[14:05 - 14:15] Then we'll have the prompt here or adding both the chat history string and the input text.\n[14:16 - 14:31] And then we'll do a response by generating response with the prompt and the open a a i call and then we'll update the history.\n[14:31 - 14:47] So here in update history, we're simply sending it the response to the chat history will add the user input and the bot response.\n[14:47 - 14:56] We'll get the complexity score with the function we talked about earlier.\n[14:56 - 15:02] And then we'll come up with an adapted history link and there's nothing.\n[15:02 - 15:07] This is just something that was written out. There's nothing official or magical about this.\n[15:07 - 15:15] But basically what this code is doing right here is it's giving it a length in between 10 and 50.\n[15:15 - 15:31] So we're taking our complexity score floor dividing it by five and taking the minimum of that be it 50 or whatever the result of this may be.\n[15:31 - 15:45] And then we're taking the max of that and then we're looking at the length of the chat history and if it's greater than the adapted history that we calculated multiplied by two will do a summarized history.\n[15:45 - 16:12] So all this does is sends it back to the summarize text and we'll get a summary and then we'll take this summarize history and concatenated with the cat history and all we're doing here is a little bit of fancy Python where we're adding the adaptive history links divided by two.\n[16:12 - 16:19] And we add the summary and then add the last bit of the conversation to the summary.\n[16:19 - 16:33] So that's how we are able to keep track of the chat history but maintain the size of the chat history so that it doesn't become too long or too complex.\n[16:33 - 17:17] Now I'm here and just run main and you see we get a response what's on your mind and we can put anything we want and this will just have a running conversation.\n[17:17 - 17:24] This is just meant to be a basic example of a chatbot.\n[17:24 - 17:31] If you'd like to touch to it, it's basic Python and the open AI API.\n[17:31 - 17:46] If you run it long enough, it's kind of hard to see what's going on inside of it without a bunch of print statements, but if you'd like the code to this, I'd be happy to either taste it into the into the channel and I think you would have access to the repo as well.\n[17:46 - 17:56] If you have any particular questions about the code, I kind of glossed over it pretty fast because it's just the two of us in here.\n[17:56 - 18:04] So if you wanted to go back over anything in particular, just talk about the approach or ask about the homework.\n[18:04 - 18:12] It's all you so we can do whatever you'd like to do.\n[18:12 - 18:15] Yeah.\n[18:15 - 18:19] Not a particular question about this.\n[18:19 - 18:33] I mean, I think I understand that well, can you share that repo to us and just analyze it later.\n[18:33 - 18:37] Yeah, I'll definitely share the repo, the repo.\n[18:37 - 18:39] Just keep in mind that I'll put it.\n[18:39 - 18:41] I'm going to post it in the channel.\n[18:41 - 18:46] If for some reason you don't have access to it, let me get it right now while we're talking about it.\n[18:46 - 18:50] We'll see if you should have access to it.\n[18:50 - 19:49] Give me just a second here.\n[19:49 - 20:08] See I'm putting it in the channel right now. See if you've got access to this if you would.\n[20:08 - 20:11] Okay, awesome.\n[20:11 - 20:15] Just know there's three branches in there.\n[20:15 - 20:26] Go to the 24 a two or the 24 a three branch main branch is the starter repo, but it doesn't have the majority of the code in it.\n[20:26 - 20:36] So either 24 a two or a three, you can go to and that's got the completed code in it.\n[20:36 - 20:43] Like I said, it's pretty pretty basic code Python open AI API.\n[20:43 - 20:52] And as far as and you said you had no questions about this correct.\n[20:52 - 20:57] Well, maybe you just won.\n[20:57 - 21:05] Why are we using well in this code we're using we're not using chain.\n[21:05 - 21:08] I think it's the library. We use the last class.\n[21:08 - 21:12] What is the difference between both?\n[21:12 - 21:16] Okay, so this code is just basic.\n[21:16 - 21:19] Just a curiosity is we're going forward.\n[21:19 - 21:23] How much experience do you have in Python quite a bit?\n[21:23 - 21:27] Yeah, I mean.\n[21:27 - 21:36] Python. Yeah, I quite a bit, but not not much about the.\n[21:36 - 21:39] Yeah, all of them.\n[21:39 - 21:48] Okay, so this file is just Python and the open AI API.\n[21:48 - 21:52] Exactly why it was written in this way.\n[21:52 - 21:56] I couldn't tell you from starting next week.\n[21:56 - 22:05] I'm going to be writing my own code and kind of try to get a feel of what may be helpful to the people that come to the office.\n[22:05 - 22:10] Office hours. I have a feeling because it's a holiday in the United States.\n[22:10 - 22:15] And because it's the first week, a lot of people didn't come and hopefully we'll have more people.\n[22:15 - 22:20] But, you know, whoever shows up, then, you know, we'll do we'll do with the group wants to do.\n[22:20 - 22:29] But I'll be writing my own code based on what's going on in the lessons and kind of the feel that I get and what would be best for the group.\n[22:29 - 22:33] But the main difference is this is just calling in the same thing.\n[22:33 - 22:36] You could write this in JavaScript. You could write this in any language.\n[22:36 - 22:42] Langchain is a framework that is principally written in Python.\n[22:42 - 22:49] It does have a JavaScript version that's getting pretty mature, even though it's still a bit buggy at times.\n[22:49 - 22:52] And a new Java version just came out.\n[22:52 - 22:56] What Langchain does is Langchain allows you.\n[22:56 - 23:05] It's a framework that allows you to tie in your LOM, your data sources, output parsers, database.\n[23:05 - 23:11] It allows you to create applications with large language models without.\n[23:11 - 23:14] It's a lot of syntactic sugar is all it is.\n[23:14 - 23:17] You could roll your own if you wanted to.\n[23:17 - 23:26] There's other options, but like I said, this is just Python code where it's Langchain is a framework that allows you to work with LOM.\n[23:26 - 23:34] In a much quicker and easier way, it's a great way to prototype things quickly.\n[23:34 - 23:41] And then within the Langchain ecosystem, there's also Langsmith, which will be going over tomorrow.\n[23:41 - 23:49] As I was saying here with this Python code, it's hard to see what's going on while we're making these calls to the LOM.\n[23:49 - 24:00] Langsmith, as you'll see tomorrow, let you look into what's going on with your application, what's going on with with your API calls.\n[24:00 - 24:06] And it gives you a lot more of a granular view and allows you to do a lot of debugging.\n[24:06 - 24:17] Lang serve, which I believe that will cover at the end towards the more towards the end of the course is a quick way within the ecosystem to launch an API.\n[24:17 - 24:31] With the code that you've created with Langchain and Langgraph is more concentrated on being able to create agents with Langchain and multi agent systems.\n[24:31 - 24:47] We'll be using that as well. And I think towards the end of this course, the plan right now is also the integrate crew AI, which is another system that's based on AI agents and multi agent systems.\n[24:47 - 24:55] facilitating creating those systems to accomplish whatever your personal and professional goals are.\n[24:55 - 24:58] Did that answer your question?\n[24:58 - 25:16] Yeah, yeah, okay, I understand Langchain is basically a higher level and here we're using a more, I don't know, yeah, lower level of the API.\n[25:16 - 25:21] Yeah, we're using actually the API or open AI.\n[25:21 - 25:24] Yeah, this is just the idea.\n[25:24 - 25:43] What Langchain would do if this was me, like we had this maybe if I tried to open, I think it's still open from last night.\n[25:43 - 25:51] So yeah, like here, this basic code that that Julian was going over last night, this uses Langchain.\n[25:51 - 26:02] It distracts away. We're still pulling in open AI and it's just importing into Langchain has this chat open AI.\n[26:02 - 26:05] And then we're able to very quickly.\n[26:05 - 26:16] You don't even need to put any of this in. You literally could just take this out with no model.\n[26:16 - 26:19] I don't know if I've still got an environmental.\n[26:19 - 26:25] No, I don't.\n[26:25 - 26:57] Different open AI key there don't have the Python dot end.\n[26:57 - 27:05] You just make sure got my open AI key now.\n[27:05 - 27:20] Just saying system.\n[27:20 - 27:26] It's running a little bit slow and none of the output is parsed, but yeah, it's just allowing.\n[27:26 - 27:30] And this isn't even the thing.\n[27:30 - 27:34] So we get to and we do two imports.\n[27:34 - 27:36] We create an LOM here.\n[27:36 - 27:45] I said, you don't even need it just automatically picks 3.5 and it picks up on the fact that here in the dot end file.\n[27:45 - 27:52] Or not in the dot end file, excuse me in the environmental variables that the open AI key is there.\n[27:52 - 27:58] And it allows us to start creating messages and just write down here.\n[27:58 - 28:03] Instead of having to go through everything we did there, we just say LOM dot and vote messages.\n[28:03 - 28:06] And we get a response.\n[28:06 - 28:09] So it's like I said, it's impacting sugar.\n[28:09 - 28:16] If you would over the Python that you would need to be able to start interacting with the LOM.\n[28:16 - 28:20] Where it really gets helpful is when you start.\n[28:20 - 28:25] You're needing a lot of tools to say you want to integrate vector databases.\n[28:25 - 28:33] Say you need to do embedding. Say you need to involve more than one LOM.\n[28:33 - 28:35] They're constantly updating.\n[28:35 - 28:42] I don't even know how many different entities they have integrated into their system, but it's quite large.\n[28:43 - 28:47] It makes it easier and lets you write a lot less code.\n[28:47 - 28:49] Kind of a downside to it is.\n[28:49 - 28:52] Is that it's constantly changing.\n[28:52 - 29:00] So it's almost helpful at times just like you would with Python packages to lock a version in.\n[29:00 - 29:05] Because sometimes if you have some line chain code and then you go and.\n[29:05 - 29:10] You do your pit install and you pull in everything and it pulls in the most.\n[29:10 - 29:12] The most recent version of line chain.\n[29:12 - 29:16] You try to run the older code that's maybe six months old.\n[29:16 - 29:17] It breaks.\n[29:17 - 29:20] So it's.\n[29:20 - 29:22] A rapidly.\n[29:22 - 29:24] Evolving field.\n[29:24 - 29:27] A rapidly evolving framework.\n[29:27 - 29:28] And.\n[29:28 - 29:31] It's to me.\n[29:31 - 29:33] It's pretty good.\n[29:35 - 29:42] Probably the best option that we have at this point, but it's not without its own problems.\n[29:42 - 29:49] A lot of places in production roll their own systems to do whatever they need and to have more granular control over it.\n[29:49 - 29:50] But.\n[29:50 - 29:51] Yes.\n[29:51 - 29:53] So.\n[29:53 - 29:54] Does that answer.\n[29:54 - 29:58] More of what you were what you were serious about with line chain.\n[29:58 - 29:59] Yeah.\n[29:59 - 30:00] Yeah.\n[30:00 - 30:01] That's perfect.\n[30:01 - 30:03] And.\n[30:03 - 30:08] And I was going to say and through the course through the next few weeks, we're going to be we're going to be a going over.\n[30:08 - 30:11] Lang Smith, we're going to be going over line chain expression language.\n[30:11 - 30:15] He's going to deep dive a lot more into this and during the office hours.\n[30:15 - 30:21] We're going to dive a lot more into this because you're going to understand a lot more about the entire line chain.\n[30:21 - 30:24] They've got the kind of ecosystem.\n[30:24 - 30:26] Lang Smith, Lang Serr, blank graph.\n[30:26 - 30:36] Is we move along through the lectures to your homework.\n[30:36 - 30:37] Through the guided projects.\n[30:37 - 30:39] And.\n[30:39 - 30:47] So if you, if you've got a pretty decent understanding of what it is, and you know what it does for us.\n[30:47 - 30:51] I responses brief, so I don't just babble on and on\n[30:51 - 30:56] because I have a bad habit of just talking at times.\n[30:56 - 30:58] OK, yeah, no, thank you.\n[30:58 - 31:12] That's all of my, all of my, yeah, that's good.\n[31:12 - 31:14] OK, and you were talking about the homework.\n[31:14 - 31:21] What questions did you have about the homework?\n[31:21 - 31:24] Yeah, well, I mean, I did it, but I'm\n[31:24 - 31:29] not happy with the results because I don't know how to,\n[31:29 - 31:31] I want to know what is the correct way to do it,\n[31:31 - 31:36] because in my interactions, what I see\n[31:36 - 31:40] is that the two LAMs are talking to each other,\n[31:40 - 31:49] but the problem is that I want them to behave as a human,\n[31:49 - 31:57] but these LLAMs don't forget that they are a AIA system.\n[31:57 - 32:00] So in some point of the conversation,\n[32:00 - 32:04] they say they don't respond to the correctly\n[32:04 - 32:08] to the other LLAM because they say something like,\n[32:08 - 32:11] I'm AIA system, how can I help you?\n[32:11 - 32:15] But this LLAM is supposed to ask questions\n[32:15 - 32:18] and to be more like a human.\n[32:18 - 32:22] But I don't, I can't do that.\n[32:22 - 32:27] I don't know if you have some insight about that or.\n[32:27 - 32:29] Did you want, you can send it in here.\n[32:29 - 32:31] Do you have it in a GitHub repo,\n[32:31 - 32:34] or in a jist, or in a shareable format?\n[32:34 - 32:42] Yeah, I can share you in the chat of the meeting.\n[32:42 - 32:43] Yeah.\n[32:43 - 32:48] OK, let me use the speakers.\n[32:50 - 32:53] Yeah, that's it.\n[32:53 - 32:54] Let me see.\n[32:54 - 33:08] Yeah, I can give you one second.\n[33:08 - 33:10] I'm going to open it up.\n[33:10 - 33:22] Yep.\n[33:22 - 33:24] What did you got it in the?\n[33:24 - 33:34] Oh, first we see the repo.\n[33:34 - 33:37] I have another specific question.\n[33:37 - 33:40] Can you go to the example that you see in here?\n[33:40 - 33:42] That's right.\n[33:42 - 33:49] Ah, you're not in the last class repo, right, can you?\n[33:49 - 33:51] Yeah, did you want me to go to the repo from the last?\n[33:51 - 33:53] I was looking at what you sent me\n[33:53 - 33:56] and I was looking for where your code was.\n[33:56 - 34:03] Oh, my code is in classic samples folder.\n[34:03 - 34:08] And I just override the simple chat history.\n[34:08 - 34:11] Well, yeah, sure.\n[34:11 - 34:16] Let me open, let me open that.\n[34:16 - 34:22] And I'll just leave that, but let me open the repo.\n[34:22 - 34:27] So you wanted to open this repo.\n[34:27 - 34:35] So this is a repo from last night.\n[34:35 - 34:41] Oh, yeah, but I'm interested in the simple chat history file.\n[34:41 - 34:42] OK.\n[34:42 - 34:44] Yeah, there.\n[34:44 - 34:48] Why are we, for example, in the line 16?\n[34:48 - 34:56] Why don't we use the AA message function\n[34:56 - 35:04] to encapsulate to encapsulate results 16 line?\n[35:04 - 35:05] Are you talking?\n[35:05 - 35:09] Hold on one second.\n[35:09 - 35:24] Yeah.\n[35:24 - 35:25] Yeah.\n[35:25 - 35:27] Are you talking about what, like, for the output\n[35:27 - 35:30] where we're getting this human message,\n[35:30 - 35:32] and then we're getting AI message.\n[35:32 - 35:38] And then as I start to talk, interact with the chatbot,\n[35:38 - 35:43] it's just returning.\n[35:43 - 35:48] It's just coming back.\n[35:48 - 35:49] Yeah, yeah.\n[35:49 - 35:55] Why is it, you're asking why it's coming back like this\n[35:55 - 35:59] and it doesn't have that AI message wrapper around it?\n[35:59 - 36:02] Yeah, yeah, exactly.\n[36:02 - 36:07] That, were you in class last night?\n[36:07 - 36:08] Yeah.\n[36:08 - 36:14] I think it may have been after he dismissed class a few people\n[36:14 - 36:17] stuck around because somebody asked about it.\n[36:17 - 36:19] I just don't remember the exact moment.\n[36:19 - 36:24] And what it was, it was an oversight of the program.\n[36:24 - 36:28] Julian went in and made some, he and I were making changes\n[36:28 - 36:30] before class yesterday.\n[36:30 - 36:32] And he made a change.\n[36:32 - 36:34] And he overlooked it.\n[36:34 - 36:35] And then I saw it.\n[36:35 - 36:38] And I saw the same thing you're seeing.\n[36:38 - 36:39] And I forgot to mention it.\n[36:39 - 36:42] So it was just a simple oversight.\n[36:42 - 36:45] What he remember here, what he was doing,\n[36:45 - 36:47] and he was adding this dot content.\n[36:47 - 36:52] So if you take this dot content out,\n[36:52 - 36:56] think if you just go here, I saved it.\n[36:56 - 36:59] Let me get out of here.\n[36:59 - 37:02] Let me run it again.\n[37:02 - 37:06] Simple chat history.\n[37:06 - 37:06] OK, boom.\n[37:06 - 37:11] So we have the chat history.\n[37:11 - 37:24] OK, it works.\n[37:24 - 37:25] OK.\n[37:25 - 37:30] See, the human message here, could you\n[37:30 - 37:32] give me the result in actually Tom's?\n[37:32 - 37:38] And then you have the AI message wrapper here,\n[37:38 - 37:40] but with all this metadata.\n[37:40 - 37:46] So to be able to showcase it to the class,\n[37:46 - 37:49] he just went in and added dot content,\n[37:49 - 37:52] quit to be able to pull the content.\n[37:52 - 37:57] And when he did that, it cut off this, the AI wrapper.\n[37:57 - 38:05] That was it, there was, this was just a quick program\n[38:05 - 38:08] to demonstrate, saving chat history.\n[38:08 - 38:10] There was no reason to take it off.\n[38:10 - 38:13] And that's the reason it came off.\n[38:13 - 38:16] And it was a simple oversight.\n[38:16 - 38:22] OK, yeah, I got it.\n[38:22 - 38:25] Is there anything else about the code from the other night?\n[38:25 - 38:28] Did she have any questions about while we're on Twitter?\n[38:33 - 38:39] Basically, yeah, what is the difference between,\n[38:39 - 38:43] I mean, it's a very simple question.\n[38:43 - 38:47] What is the difference between AI message and human message?\n[38:47 - 38:53] And for example, if I use the function invoke,\n[38:53 - 38:59] is that mean that the last message has to be a human message\n[38:59 - 39:06] since the last message will be a AI message?\n[39:06 - 39:09] Yeah, so the difference between the AI message\n[39:09 - 39:11] and the human message, yes, the human message\n[39:11 - 39:15] is what's going to the LOM.\n[39:15 - 39:19] The AI message is what's coming back from the LOM.\n[39:19 - 39:23] Here we created this, here we created just as an example,\n[39:23 - 39:25] we created this previous context.\n[39:25 - 39:30] So we had a human message, the AI response, the human message.\n[39:30 - 39:34] And then we invoked the LOM with it.\n[39:34 - 39:39] And the LOM gave a result, the AI message, like we saw.\n[39:39 - 39:43] So the difference between them is the human message.\n[39:43 - 39:48] This is a long chain abstraction.\n[39:48 - 39:51] The human message is what the user is putting in,\n[39:51 - 39:55] the AI message is whatever LOM we're using is sending back.\n[39:55 - 40:03] So for example, if we granted a kin,\n[40:03 - 40:05] I think I saved that here.\n[40:05 - 40:11] So we'll get back to a cleaner version.\n[40:11 - 40:15] So you see here, we get the human message, translate,\n[40:15 - 40:16] this is just the history that we gave.\n[40:16 - 40:18] It translates the sentence to English,\n[40:18 - 40:21] to French hello programming, the AI message,\n[40:21 - 40:22] translate it in the human message.\n[40:22 - 40:24] What did you say?\n[40:24 - 40:27] And that's where our history ends.\n[40:27 - 40:34] And we invoked the message here with just these messages\n[40:34 - 40:38] in our fake history that we've created.\n[40:38 - 40:43] But then it answers back, I said, and it gives you,\n[40:43 - 40:47] which means I love programming in French.\n[40:47 - 40:52] So ideally, if we had not abstracted away with this.content,\n[40:52 - 40:53] if we had taken this out,\n[40:53 - 40:55] you'd have that AI message wrapper,\n[40:55 - 40:57] like we saw a few minutes ago around it.\n[40:57 - 41:00] That's it.\n[41:00 - 41:02] Thanks.\n[41:02 - 41:08] And in reality, almost any time that comes to mind\n[41:08 - 41:10] that you're going to be doing with these,\n[41:10 - 41:12] you're going to be using output parsers.\n[41:12 - 41:15] So the output parser, depending on what your goal is\n[41:15 - 41:17] and what you're trying to do with the output,\n[41:17 - 41:20] will take care of a lot of that for you.\n[41:20 - 41:23] But yeah, in this case, it's just a message\n[41:23 - 41:25] that we're sending or the user is sending\n[41:25 - 41:27] versus what we're getting back.\n[41:27 - 41:31] Does that make sense?\n[41:31 - 41:33] OK, cool.\n[41:33 - 41:39] Yeah, I think that is everything about this repo.\n[41:39 - 41:44] And I don't know if we can see the one that I share.\n[41:44 - 41:50] So I can show you what I did.\n[41:50 - 41:52] Yeah, hold on one second, let me.\n[41:52 - 41:53] Yeah.\n[41:53 - 42:09] Minimize this and let me go back.\n[42:09 - 42:12] And I'm sure you won't, but we are recording this.\n[42:12 - 42:17] So you're OK with me pulling this up on the recording.\n[42:17 - 42:18] Yeah, of course.\n[42:18 - 42:21] Yeah, I thought it was.\n[42:21 - 42:26] I just wanted to be clear.\n[42:26 - 42:33] So yeah, the thing here is, what do you think about what I did\n[42:33 - 42:37] because my best results about the conversation\n[42:37 - 42:40] was having two messages.\n[42:40 - 42:42] It's time in two arrays of messages,\n[42:42 - 42:46] where the first one starts with a human message.\n[42:46 - 42:52] And the second one starts with AI message.\n[42:52 - 43:02] And in this way, the LLM always responds as AI message.\n[43:02 - 43:09] Because if I don't do that, the conversation goes really\n[43:09 - 43:10] back.\n[43:10 - 43:15] I don't know why.\n[43:15 - 43:16] This had a curiosity.\n[43:16 - 43:17] So I'm going to have to look.\n[43:17 - 43:18] I'm just going to think of this code real quick.\n[43:18 - 43:19] What are these covers?\n[43:19 - 43:21] What are these?\n[43:21 - 43:29] Just to call or the text to see better the conversation,\n[43:29 - 43:31] the chat.\n[43:31 - 43:35] I mean, when I print in, yeah.\n[43:35 - 43:39] Because I print the conversation instead of print the,\n[43:39 - 43:40] for example, yeah, that's output.\n[43:40 - 43:44] It is some example of that code.\n[43:44 - 43:51] And yeah, at some point, one of the chat\n[43:51 - 44:00] pot thinks that they are AI a system\n[44:00 - 44:06] and the conversation goes back.\n[44:06 - 44:10] Just because I was still looking at the code,\n[44:10 - 44:13] did, how did you sell it?\n[44:13 - 44:22] You had this on your system, right now, simple.\n[44:22 - 44:26] So when you were running this, you didn't limit it in any way.\n[44:26 - 44:27] You just ran it.\n[44:27 - 44:30] You killed it manually, or is there something\n[44:30 - 44:34] that I'm worried that you were just conversing?\n[44:34 - 44:40] Oh, I just press a key to continue the conversation.\n[44:40 - 44:51] All right, that's just OK.\n[44:51 - 44:57] Yeah, and that is my best try because, yeah,\n[44:57 - 45:03] if I don't do this having two types of messages,\n[45:03 - 45:06] message A and message B, I only use one.\n[45:06 - 45:13] The dilemmas are a little confused.\n[45:13 - 45:20] I don't have a specific conversation.\n[45:20 - 45:22] I didn't save it.\n[45:22 - 45:31] But they basically repeat what the other LLM say.\n[45:31 - 45:34] But I don't know if this is necessary.\n[45:34 - 45:43] My logic here was having the last message\n[45:43 - 45:49] as I use as a human message.\n[45:49 - 45:52] For example, as you can see in message A,\n[45:52 - 45:53] the last message is a human message.\n[45:53 - 45:58] That means that the last person to respond\n[45:58 - 46:12] will be chat A, that is a CIA message.\n[46:12 - 46:14] Yeah, I'm going to hold on one second.\n[46:14 - 46:16] I'm going to throw this into.\n[46:16 - 46:39] Yep.\n[46:39 - 46:42] I am using the same requirements.\n[46:42 - 47:04] I pull this into the chat, so that's going to be a problem.\n[47:04 - 47:58] And the chat B translate the sense from English to French.\n[47:58 - 48:01] I will put in chat A, chat B.\n[48:01 - 48:02] What did you say, chat A?\n[48:02 - 48:05] I said, it means I'll have programming in French.\n[48:05 - 48:15] Press to continue the conversation.\n[48:15 - 48:26] So every way, translate the sense A, OK, it's turning the whole.\n[48:26 - 48:27] How do you say, what is it?\n[48:27 - 48:30] OK, so the chat B is asking, how do you say,\n[48:30 - 48:39] what is your name in French gives the correct answer?\n[48:39 - 48:43] Whether languages are you interested in?\n[48:43 - 48:53] So chat B, jumps, chat B starts asking,\n[48:53 - 48:57] what are the interesting learning learning languages.\n[48:57 - 49:03] That's great.\n[49:03 - 49:05] How do you plan to practice and improve your language skills\n[49:05 - 49:07] in these languages?\n[49:07 - 49:10] I engage in conversation.\n[49:10 - 49:14] So I mean, you've got.\n[49:14 - 49:16] Have you tried using language?\n[49:16 - 49:17] No, it's not bad at all.\n[49:17 - 49:24] And I'm just looking at just what it's doing.\n[49:24 - 49:27] It's a language exchange project.\n[49:27 - 49:30] I haven't tried using language exchange.\n[49:30 - 49:34] Now I would say, actually, the purpose of this assignment\n[49:34 - 49:39] was just to kind of get you moving in the course.\n[49:39 - 49:42] Obviously, this is a really simplistic version\n[49:42 - 49:45] and everything that we're going to be dealing with,\n[49:45 - 49:49] with useful applications is going to be a lot more complicated.\n[49:49 - 49:52] It's going to involve a lot of prompt engineering.\n[49:52 - 49:55] It's going to involve a lot more moving parts.\n[49:55 - 50:03] But certainly, what LLM do you normally use GPP card?\n[50:03 - 50:05] What's your go to?\n[50:05 - 50:10] Yeah, chat GPT, the last version, and club, both of them.\n[50:10 - 50:11] OK.\n[50:11 - 50:17] So because this is using 3.5, when you were running it,\n[50:17 - 50:20] were you using 3.5 as well?\n[50:20 - 50:22] Oh, yeah.\n[50:22 - 50:28] What I'm telling you?\n[50:28 - 50:29] OK.\n[50:29 - 50:32] So there is no magic formula for this.\n[50:32 - 50:37] I mean, I have to modify the prompt\n[50:37 - 50:45] and tell the LLM how to behave and try it and try it to see\n[50:45 - 50:47] two results.\n[50:47 - 50:47] Yeah.\n[50:47 - 50:51] And like I said, what we'll get into tomorrow with Lang Smith,\n[50:51 - 50:55] you'll be able to see a lot more of what's going on inside\n[50:55 - 50:59] of each of the calls.\n[50:59 - 51:05] And yeah, for something like this, and using 3.5,\n[51:05 - 51:18] what I was curious about when it changed this to 4.0,\n[51:18 - 51:22] I am into 4.0.\n[51:22 - 51:24] I'll say we get a temperature.\n[51:24 - 51:30] Let's leave the temperature as is.\n[51:30 - 51:33] Let me just try to run this again.\n[51:33 - 51:41] OK.\n[51:41 - 51:50] I'm just, oh, I see.\n[51:50 - 51:52] How do you say I'm learning a new language?\n[51:52 - 52:01] OK.\n[52:01 - 52:02] That's interesting.\n[52:02 - 52:04] How do you pronounce J?\n[52:04 - 52:06] Sure, here's a simple way to break down the pronunciation.\n[52:06 - 52:09] Do you see what kind of better results you're getting?\n[52:09 - 52:11] You got to think this is 3.5 turbo.\n[52:11 - 52:12] The 3.5 turbo is great.\n[52:12 - 52:15] It's economical for a lot of calls.\n[52:15 - 52:16] You can fine tune it pretty cheap.\n[52:16 - 52:23] But 3.5 turbo came out when it was already 4 came out\n[52:23 - 52:27] at the beginning of last year.\n[52:27 - 52:29] So when we put a better model into it,\n[52:29 - 52:32] you're already seeing the difference\n[52:32 - 52:38] in the outputs that we're getting just by changing the model.\n[52:38 - 52:40] If you were going to say you were going to do something,\n[52:40 - 52:45] let's obviously we wouldn't be doing something so trivial\n[52:45 - 52:45] for production.\n[52:45 - 52:48] But let's just say we were doing this in production,\n[52:48 - 52:51] but we couldn't use 4.0 because it was going to blow our budget up.\n[52:51 - 52:53] So we're going to use 3.5.\n[52:53 - 52:56] Well, there we start thinking about how do we prompt engineer this?\n[52:56 - 53:01] What can we do internally with and without AI\n[53:01 - 53:02] to get better results?\n[53:02 - 53:05] Maybe the hey can we fine tune this model\n[53:05 - 53:08] and get better results?\n[53:08 - 53:11] And there's just depending on the task,\n[53:11 - 53:13] there's so many different models.\n[53:13 - 53:16] There's a ton of models that have already been fine tuned,\n[53:16 - 53:20] especially the lava models that you can go get a ugly face.\n[53:20 - 53:22] So it really depends on your,\n[53:22 - 53:24] it's going to depend on your use case.\n[53:24 - 53:28] It's going to depend on what kind of inference time\n[53:28 - 53:30] you're looking for.\n[53:30 - 53:33] It's going to depend on what kind of budget that she's got.\n[53:33 - 53:35] There's so many variables that go into it,\n[53:35 - 53:40] but you see, I think that's pretty cool that it was already.\n[53:40 - 53:42] Yeah.\n[53:42 - 53:47] Have a nice lighty and a response.\n[53:47 - 53:51] Yeah, so feel free as well to mess around\n[53:51 - 53:59] with the model and see the different types of responses\n[53:59 - 54:00] that you get.\n[54:00 - 54:02] Later on in the class, we'll specifically\n[54:02 - 54:09] address fine tuning models to be able to get concentrate\n[54:09 - 54:11] on the type of results exactly that you're looking for.\n[54:11 - 54:15] So you'd how much do you know about fine tuning\n[54:15 - 54:20] and models in general and whatnot?\n[54:20 - 54:29] No, basically, I can say nothing, basically.\n[54:29 - 54:29] That's fine.\n[54:29 - 54:32] I'll be, you know, to tell you a little bit about myself,\n[54:32 - 54:36] I got into, I was originally an epidemiologist.\n[54:36 - 54:40] I got into software engineering seven years ago.\n[54:40 - 54:43] I started out with the MIRN stack.\n[54:43 - 54:46] I moved on and learned Python, I learned C.\n[54:46 - 54:49] I spent about up to now half my time in production\n[54:49 - 54:51] and about the other half in education,\n[54:51 - 54:54] doing everything from teaching classes\n[54:54 - 54:57] to curriculum development.\n[54:57 - 55:01] Right now I work for a startup in production,\n[55:01 - 55:02] kind of across the stack.\n[55:02 - 55:05] So I'm still working with the front end of the back end,\n[55:05 - 55:07] but also work with the AI.\n[55:07 - 55:09] I work with the models.\n[55:09 - 55:12] And I've got about a year doing it,\n[55:12 - 55:15] but this day and age, the way things are changing,\n[55:15 - 55:18] you know, a year's a long time.\n[55:18 - 55:20] So, I mean, by the time you're done with this course,\n[55:20 - 55:22] you're going to look back and you're going to,\n[55:22 - 55:24] after 10, I guess 11 weeks,\n[55:24 - 55:27] because there's one week off during the July 4th holiday\n[55:27 - 55:31] for the United States, you're going to look back\n[55:31 - 55:34] and be like, man, I just learned a ton.\n[55:34 - 55:37] And then looking at the news of everything that comes out\n[55:37 - 55:39] on how much AI you keep up with,\n[55:39 - 55:42] but so many new models come out every day.\n[55:42 - 55:45] And a lot of times, so yeah.\n[55:45 - 55:47] I guess with the fine tuning,\n[55:47 - 55:50] just a 30,000 foot of it,\n[55:50 - 55:54] you can have a base model that's pretty dang smart,\n[55:54 - 55:57] but cheat to use, like GPG 3.5.\n[55:57 - 56:01] And what you can do is you can go in with various methods\n[56:01 - 56:04] and you can train that model more specifically\n[56:04 - 56:08] on the task that you're looking for it for it to do.\n[56:08 - 56:12] Let's say in the case that prompt engineering didn't work,\n[56:12 - 56:17] the next step may be to do some degree of fine tuning.\n[56:19 - 56:22] And you can see a lot of times,\n[56:22 - 56:26] even with some very minimal tuning,\n[56:26 - 56:28] and a pretty small budget,\n[56:28 - 56:32] you can really increase the quality of the responses\n[56:32 - 56:37] that you're getting specific to the goals that you have.\n[56:37 - 56:41] So these big models like GPT 4,\n[56:42 - 56:45] they're pretty expensive when you start making tens,\n[56:45 - 56:48] hundreds, thousands, millions of calls to them.\n[56:48 - 56:49] It's some of the smaller models,\n[56:49 - 56:50] and then another thing,\n[56:50 - 56:53] have you messed around at all with running models locally?\n[56:57 - 57:01] I try to run some time.\n[57:05 - 57:08] This image engineering model,\n[57:08 - 57:09] I forget the name,\n[57:10 - 57:14] what it is, the stable diffusion.\n[57:14 - 57:19] But yeah, I couldn't in that time and I forget it,\n[57:19 - 57:24] but I like to run some locally.\n[57:24 - 57:29] That's the depending on your work and where you're working\n[57:29 - 57:30] and what you need a lot of times,\n[57:30 - 57:33] you won't be able to make a call to open AI\n[57:33 - 57:35] because you're dealing with proprietary data,\n[57:35 - 57:38] you're dealing with sensitive data,\n[57:38 - 57:42] and in that case, we'll be covering that very shortly,\n[57:42 - 57:45] ways of interacting in-house with your own data,\n[57:45 - 57:50] and there are ways of making calls to APIs from the edge\n[57:52 - 57:56] and then working in-house with your own data\n[57:56 - 57:59] or you might be using everything totally in-house\n[57:59 - 58:00] on your own servers,\n[58:00 - 58:03] it just really depends on the circumstance.\n[58:03 - 58:04] Like here, they set it up,\n[58:04 - 58:07] I don't think they ever did it last night,\n[58:07 - 58:11] but you could switch over to,\n[58:11 - 58:18] let me see, what is it?\n[58:18 - 58:21] I think it's five, three many.\n[58:21 - 58:22] Okay.\n[58:22 - 58:25] I'm running off of MacBook Air.\n[58:25 - 58:27] Right now, when I need to do anything heavy,\n[58:27 - 58:29] I use my desktop and Linux,\n[58:29 - 58:33] but I like my MacBook, but I need to upgrade my notebook\n[58:33 - 58:35] because I've got a MacBook Air in one\n[58:35 - 58:36] with eight gigs of memory,\n[58:36 - 58:38] and if I run that one, a three,\n[58:38 - 58:41] it will freezes up for a belt.\n[58:43 - 58:47] For about one minute, so let me save that.\n[58:47 - 58:50] Let me run, we'll undo this, too, just fun.\n[58:51 - 59:02] Save it, that's system.\n[59:02 - 59:04] And you can see it still floats.\n[59:04 - 59:06] But this is running via the end of your disk.\n[59:06 - 59:09] This is running completely on my computer.\n[59:09 - 59:12] Open AI, nobody is gathering.\n[59:14 - 59:15] Okay, that's cool.\n[59:15 - 59:17] Wow, it's running slow.\n[59:18 - 59:19] If I put one of three in there,\n[59:19 - 59:22] it may be like two minutes to run it.\n[59:22 - 59:28] But I think you'll see as the course progresses there,\n[59:28 - 59:29] and we just got an answer.\n[59:29 - 59:33] So absolutely, let me channel my inner entrepreneurial spirit\n[59:33 - 59:36] and provide you with the most inspiring options.\n[59:36 - 59:39] Out of socks, a lot, rainbows, anchor, monochrome,\n[59:39 - 59:41] and not many manufacturers,\n[59:41 - 59:43] just kidding, let's stick to something catching up,\n[59:43 - 59:46] powerful, pigmented poetry partners,\n[59:46 - 59:47] or rainbows, right?\n[59:47 - 59:49] Hopefully one of these tickles your funny bones\n[59:49 - 59:51] sparks that business genius.\n[59:51 - 59:54] So even with this tiny Microsoft model\n[59:54 - 59:57] that I'm running locally,\n[59:57 - 59:59] you can get some pretty decent results.\n[59:59 - 01:00:02] What you can do, what's kind of cool,\n[01:00:02 - 01:00:04] is you can take this like this five, three many,\n[01:00:04 - 01:00:09] and you can add retrieval augmented generation,\n[01:00:09 - 01:00:11] like I was talking about RAG,\n[01:00:11 - 01:00:14] where we take this, we combine it\n[01:00:14 - 01:00:15] with some sort of a real prompt\n[01:00:15 - 01:00:17] and not a silly prompt here.\n[01:00:17 - 01:00:19] We run some more code.\n[01:00:19 - 01:00:23] We connect it to some context, that context,\n[01:00:23 - 01:00:25] let's just say for example, it's proprietary data.\n[01:00:25 - 01:00:28] So we've taken, let's say we have a ton of data\n[01:00:28 - 01:00:31] that we don't want to leave, let leave the business.\n[01:00:31 - 01:00:34] There's ways, do you know anything about vector databases\n[01:00:34 - 01:00:37] and bad things, there's ways specific\n[01:00:37 - 01:00:41] to our language models that we can save that data\n[01:00:41 - 01:00:45] and then retrieve relative pieces of that data\n[01:00:45 - 01:00:50] and integrate it into our LLM calls.\n[01:00:50 - 01:00:54] And with this, it's a small model.\n[01:00:54 - 01:00:58] This is a three billion, three billion, I think,\n[01:00:58 - 01:01:00] the country many.\n[01:01:00 - 01:01:03] They've got a seven and a 14 out now.\n[01:01:03 - 01:01:06] But if you mix this with retrieval augmented generation\n[01:01:06 - 01:01:08] and run it locally with Obama,\n[01:01:08 - 01:01:12] you can get pretty darn good results just with that\n[01:01:12 - 01:01:14] and with not a lot of program.\n[01:01:14 - 01:01:20] I wrote one with this, with maybe 200 lines of code\n[01:01:22 - 01:01:26] at most, maybe 100 lines of code at most, I forget.\n[01:01:26 - 01:01:28] And was getting really good results with it.\n[01:01:28 - 01:01:29] So like I said, you'll see a lot.\n[01:01:29 - 01:01:31] I think a lot of the questions that you may have\n[01:01:31 - 01:01:36] in your mind, you'll see answer to in the coming lessons\n[01:01:36 - 01:01:38] and you'll build on that.\n[01:01:38 - 01:01:40] And then a few weeks they're gonna start hitting\n[01:01:40 - 01:01:42] really hard with the agents and things\n[01:01:42 - 01:01:46] that are gonna get super interesting, super complex\n[01:01:46 - 01:01:48] and super useful as being able to,\n[01:01:48 - 01:01:51] hey, I can actually build something useful.\n[01:01:51 - 01:01:55] But right now, like the first lesson like Julian pointed out,\n[01:01:55 - 01:01:58] we wanted to make sure everybody was on the same page\n[01:01:58 - 01:02:02] and tomorrow is going to start diving into line Smith.\n[01:02:02 - 01:02:06] But like for example, what you did with your homework,\n[01:02:06 - 01:02:08] if you had just posted that in the channel,\n[01:02:08 - 01:02:09] that asked, hey, can I get some feedback\n[01:02:09 - 01:02:11] out of throwing that into my system?\n[01:02:11 - 01:02:16] And I thought, hey man, you've done with a very minimal\n[01:02:16 - 01:02:18] amount of code, a very minimal amount of experience\n[01:02:18 - 01:02:20] and very minimal instructions.\n[01:02:20 - 01:02:24] You're getting a good response even with three, five turbo.\n[01:02:24 - 01:02:26] You could probably even throw something local\n[01:02:26 - 01:02:28] in like this five, three many.\n[01:02:28 - 01:02:31] I'm not sure if it how well it deals with foreign languages\n[01:02:31 - 01:02:36] or not, but it should shift away from foreign languages\n[01:02:36 - 01:02:39] and ask it something else, but you saw when we moved up\n[01:02:39 - 01:02:43] to 40, the quality, how it increased.\n[01:02:43 - 01:02:47] So I don't really think there was any problem\n[01:02:47 - 01:02:48] as much with your code.\n[01:02:48 - 01:02:52] It was just, and like I said, I was asking what you use,\n[01:02:52 - 01:02:52] nothing's perfect.\n[01:02:52 - 01:02:57] These are not deterministic machines.\n[01:02:57 - 01:03:00] They're very indeterministic.\n[01:03:00 - 01:03:03] So, you know, you put one thing into it one minute\n[01:03:03 - 01:03:05] and it gives you one response\n[01:03:05 - 01:03:06] and you put the same thing into it.\n[01:03:06 - 01:03:08] You get a totally different response,\n[01:03:08 - 01:03:11] they can hallucinate, they can give you false info.\n[01:03:11 - 01:03:15] They're certainly not something 100% trustable\n[01:03:15 - 01:03:18] that you don't want a human looking over\n[01:03:18 - 01:03:20] for anything important.\n[01:03:22 - 01:03:23] Yeah.\n[01:03:23 - 01:03:24] Cool.\n[01:03:24 - 01:03:27] I would post it in the channel.\n[01:03:27 - 01:03:29] Yeah, as far as your code goes,\n[01:03:29 - 01:03:30] you know, when I was looking over it\n[01:03:30 - 01:03:32] and then I took a look at it.\n[01:03:32 - 01:03:34] It's like, well, it's a good setup.\n[01:03:34 - 01:03:38] I like how you set up the two separate instances\n[01:03:38 - 01:03:41] of the chat.\n[01:03:41 - 01:03:45] And I remember you asking a question about this invoke\n[01:03:45 - 01:03:51] and this is just earlier and totally bouncing just off subject.\n[01:03:51 - 01:03:53] But back to that, that's just line chain.\n[01:03:53 - 01:03:56] I think Julian may have mentioned it in class before\n[01:03:56 - 01:03:58] in the past, it was something different.\n[01:03:58 - 01:04:00] I think it was chat, but that's just its way\n[01:04:00 - 01:04:03] of calling the LOM in the end.\n[01:04:04 - 01:04:05] But I was looking at your code.\n[01:04:05 - 01:04:07] I was like, well, this looks at pretty solid code\n[01:04:07 - 01:04:08] for what they've got.\n[01:04:08 - 01:04:12] What's been, then we started looking at it.\n[01:04:12 - 01:04:14] You know, I think you hit the nail on the head\n[01:04:14 - 01:04:20] and did you have any more questions\n[01:04:20 - 01:04:21] about anything from the class,\n[01:04:21 - 01:04:25] anything that's coming, anything about the homework,\n[01:04:25 - 01:04:30] anything I've added about tonight or just AI in general?\n[01:04:32 - 01:04:34] No, there's a lot of information today\n[01:04:34 - 01:04:39] so I need to process and maybe the next class or, yeah,\n[01:04:42 - 01:04:45] right now I don't have questions.\n[01:04:45 - 01:04:49] Okay, you know, if you ever call out,\n[01:04:49 - 01:04:52] I'll talk to you to anybody that is in here.\n[01:04:52 - 01:04:54] I'll always say the same thing.\n[01:04:54 - 01:05:00] If you ever have any questions, feel free to post them in Slack.\n[01:05:02 - 01:05:05] Feel free to, I'm gonna create a thread every week\n[01:05:05 - 01:05:07] like I did in the channel for homework\n[01:05:07 - 01:05:10] if you want to post your homework there\n[01:05:10 - 01:05:12] to get any feedback.\n[01:05:12 - 01:05:16] If you've got, if you've got anything particular\n[01:05:16 - 01:05:17] that made me for some reason,\n[01:05:17 - 01:05:20] you don't want to put in the channel, you could DM me.\n[01:05:20 - 01:05:23] I can't guarantee, I'll fast, I'll get back to it.\n[01:05:23 - 01:05:25] I will get back to it.\n[01:05:25 - 01:05:28] But this is something I'm doing at night for them.\n[01:05:28 - 01:05:31] So I've gotten a regular day job and things going on.\n[01:05:31 - 01:05:34] But yeah, I'm in this salon too.\n[01:05:34 - 01:05:37] Like I said, I've only been really big\n[01:05:37 - 01:05:41] into working with ML and AI for the last year.\n[01:05:41 - 01:05:44] So every day, I'm learning something new as well.\n[01:05:44 - 01:05:48] So any question how they're basic or how they're advanced,\n[01:05:48 - 01:05:50] it would be, I'd be happy to answer it.\n[01:05:50 - 01:05:53] If I don't know the answer, I'll go to the professor.\n[01:05:53 - 01:05:55] We can, you know, in a small setting like this\n[01:05:55 - 01:05:58] or if there's a few people, we can kind of brainstorm it\n[01:05:58 - 01:06:00] through together if that's what everybody wants to do\n[01:06:00 - 01:06:01] and go on a high.\n[01:06:01 - 01:06:04] And, you know, one thing I would suggest\n[01:06:04 - 01:06:07] is you're working through this.\n[01:06:08 - 01:06:10] Do you use any sort of co-pilot\n[01:06:10 - 01:06:12] with your work?\n[01:06:14 - 01:06:17] You mean, for example, the co-pilot?\n[01:06:19 - 01:06:22] Ah, yeah, I always, yeah, you always co-pilot\n[01:06:22 - 01:06:25] in being a co-pilot, yeah.\n[01:06:25 - 01:06:26] Yeah.\n[01:06:26 - 01:06:27] So yeah, co-pilot's good.\n[01:06:27 - 01:06:30] I'm assuming you put things in the L and the times\n[01:06:30 - 01:06:34] when you're working on codes.\n[01:06:34 - 01:06:38] And you know, they're good.\n[01:06:38 - 01:06:40] I'm sure as you know, never press 100%\n[01:06:40 - 01:06:41] there out.\n[01:06:41 - 01:06:42] Yeah.\n[01:06:42 - 01:06:43] Yeah, exactly.\n[01:06:43 - 01:06:44] Like, for example, one thing I discovered,\n[01:06:44 - 01:06:49] the first time I messed with anything related to an L.O.L.M.\n[01:06:49 - 01:06:51] I was doing curriculum development.\n[01:06:51 - 01:06:54] And I think it was December 1st when GPT-4 came out,\n[01:06:54 - 01:06:57] my boss said a message, like, bro, look at this.\n[01:06:57 - 01:06:59] This is really awesome.\n[01:06:59 - 01:07:01] Start messing with it, you know, don't abuse it\n[01:07:01 - 01:07:05] but use it to help out while you're working.\n[01:07:05 - 01:07:06] This is cool.\n[01:07:06 - 01:07:09] I messed a little bit with chatbots, with my friends,\n[01:07:09 - 01:07:12] years of co-pilot had never seen anything like that.\n[01:07:12 - 01:07:14] And, you know, just kind of went from there\n[01:07:14 - 01:07:18] and I started, it was GPT-3 that you could fine tune\n[01:07:18 - 01:07:21] and I was fine tuning a little bit for the company.\n[01:07:21 - 01:07:25] And then when I saw that, hey, I was going to take my job\n[01:07:25 - 01:07:29] and decided to start studying AI and AI and D took my job.\n[01:07:29 - 01:07:30] Oh, yeah.\n[01:07:30 - 01:07:31] Yeah.\n[01:07:31 - 01:07:33] Fortunately, I was able to pick something up\n[01:07:33 - 01:07:35] for that after a little bit of time.\n[01:07:35 - 01:07:39] But, you know, any, like, for example,\n[01:07:39 - 01:07:43] something I discovered quick, I never had to mess with Regettes.\n[01:07:43 - 01:07:44] Again, I'd put something into it.\n[01:07:44 - 01:07:46] It would put out a pretty good answer\n[01:07:46 - 01:07:48] and, you know, maybe you'd have to tank her with it a little bit\n[01:07:48 - 01:07:50] but it's great with something like that.\n[01:07:50 - 01:07:54] It's still horrible with complex problem-solving\n[01:07:54 - 01:07:55] and complex math the other day.\n[01:07:55 - 01:07:59] I saw somebody, the layers of a neural network,\n[01:07:59 - 01:08:04] they were working in pie torch and they tried to calculate\n[01:08:04 - 01:08:11] the layers using GPT-4 and it put out the most ridiculous nonsense.\n[01:08:11 - 01:08:16] You can imagine it because it couldn't do the math\n[01:08:16 - 01:08:18] and the winner of your algebra was all thrown off\n[01:08:18 - 01:08:22] and it was just throwing errors left and right in pie torch.\n[01:08:22 - 01:08:23] Yeah.\n[01:08:23 - 01:08:28] It's using co-pilot and use L-insport and cans.\n[01:08:28 - 01:08:31] And like I said, I think you'll see\n[01:08:31 - 01:08:34] that your knowledge and expertise\n[01:08:34 - 01:08:38] and this flies up dramatically in the weeks to come\n[01:08:38 - 01:08:42] and certainly over the next 11 weeks.\n[01:08:42 - 01:08:45] If you don't have anything else, I'm going to end up recording.\n[01:08:45 - 01:08:48] Uh, I'm going to have to go something in.\n[01:08:48 - 01:08:55] Yeah, I'm going to go ahead and I'm not sharing the screen anymore.",
  "transcript_segments": [
    {
      "start": 4.5,
      "end": 9.5,
      "text": "Okay, so we'll see if anybody else shows up for right now."
    },
    {
      "start": 9.5,
      "end": 12.5,
      "text": "It's just going to be you and I."
    },
    {
      "start": 12.5,
      "end": 17.98,
      "text": "So I'm going to share my screen here just a second."
    },
    {
      "start": 17.98,
      "end": 102.33,
      "text": "Okay, and you're seeing my screen, okay, correct?"
    },
    {
      "start": 102.33,
      "end": 104.33,
      "text": "Yeah, that's right."
    },
    {
      "start": 104.33,
      "end": 130.34,
      "text": "Okay, so the code that I'm going to share real quick,"
    },
    {
      "start": 130.34,
      "end": 133.34,
      "text": "and if there's anything I'm going to run through it pretty quick."
    },
    {
      "start": 133.34,
      "end": 136.34,
      "text": "Since it's just you and I, and if you have any questions,"
    },
    {
      "start": 136.34,
      "end": 141.11,
      "text": "just pop on the audio and say, so I'm going to set this up."
    },
    {
      "start": 141.11,
      "end": 214.22,
      "text": "You'll wake."
    },
    {
      "start": 214.22,
      "end": 215.22,
      "text": "Firements up."
    },
    {
      "start": 215.22,
      "end": 253.72,
      "text": "X."
    },
    {
      "start": 253.72,
      "end": 258.72,
      "text": "And I'm just going to go grab the open AI key that we were using the other day."
    },
    {
      "start": 258.72,
      "end": 271.82,
      "text": "Thank you."
    },
    {
      "start": 271.82,
      "end": 279.1,
      "text": "It's going to dot in then here."
    },
    {
      "start": 279.1,
      "end": 280.32,
      "text": "So we."
    },
    {
      "start": 280.32,
      "end": 281.32,
      "text": "Two."
    },
    {
      "start": 281.32,
      "end": 288.62,
      "text": "Case."
    },
    {
      "start": 288.62,
      "end": 301.15,
      "text": "And got the key."
    },
    {
      "start": 301.15,
      "end": 306.15,
      "text": "Okay, so the example code for tonight is a simple chatbot."
    },
    {
      "start": 306.15,
      "end": 308.15,
      "text": "It's not done in line chain."
    },
    {
      "start": 308.15,
      "end": 313.15,
      "text": "It's just it's using the open API API."
    },
    {
      "start": 313.15,
      "end": 318.38,
      "text": "So we have a really simple main file."
    },
    {
      "start": 318.38,
      "end": 325.76,
      "text": "We import the summarizing chatbot class that we'll look at here in just a second."
    },
    {
      "start": 325.76,
      "end": 335.14,
      "text": "And the only thing we've got to find here is the simple message or class that we import."
    },
    {
      "start": 335.14,
      "end": 344.54,
      "text": "And basically you set up where it's going to prompt the user for their input."
    },
    {
      "start": 344.54,
      "end": 350.8,
      "text": "And until you type exit, it'll keep prompting for the input."
    },
    {
      "start": 351.22,
      "end": 356.05,
      "text": "And it calls that function."
    },
    {
      "start": 356.05,
      "end": 364.61,
      "text": "So the core of this is in the chatbot file."
    },
    {
      "start": 364.61,
      "end": 376.38,
      "text": "And all we do here is we import Python dot end and load dot ends."
    },
    {
      "start": 376.38,
      "end": 381.38,
      "text": "If you're not familiar with that, Julian was talking about it last night."
    },
    {
      "start": 381.38,
      "end": 384.38,
      "text": "It's an easy way to handle a dot end file."
    },
    {
      "start": 384.38,
      "end": 387.38,
      "text": "So we don't have to manually with whatever keys that we put in here."
    },
    {
      "start": 387.38,
      "end": 397.5,
      "text": "We're not having to manually load them all when using an export statement or using a little bit more complicated statement in the shell."
    },
    {
      "start": 397.5,
      "end": 400.5,
      "text": "This handles everything for us. So we just go here."
    },
    {
      "start": 400.5,
      "end": 403.5,
      "text": "We set our local variable to whatever we want."
    },
    {
      "start": 403.5,
      "end": 407.11,
      "text": "OS get ends."
    },
    {
      "start": 407.11,
      "end": 411.11,
      "text": "And the open API key and that pulls it directly from my dot end file."
    },
    {
      "start": 411.11,
      "end": 416.11,
      "text": "I did it manually beforehand, just in case, but that does function."
    },
    {
      "start": 416.11,
      "end": 422.11,
      "text": "I have heard that it was not functioning for some pine cone."
    },
    {
      "start": 422.11,
      "end": 427.11,
      "text": "I've never had a problem with with Python dot end by use it quite regularly with work."
    },
    {
      "start": 427.11,
      "end": 433.11,
      "text": "But we'll see later on in the course when we start with some of the more complicated code."
    },
    {
      "start": 433.11,
      "end": 440.11,
      "text": "We'll be using poetry, which handles both dependencies and environmental variables."
    },
    {
      "start": 440.11,
      "end": 447.85,
      "text": "So the need for the dot end for manually exporting the environmental variables will go away."
    },
    {
      "start": 447.85,
      "end": 450.85,
      "text": "So we set up our API key here."
    },
    {
      "start": 450.85,
      "end": 453.85,
      "text": "And we've imported open AI as well."
    },
    {
      "start": 453.85,
      "end": 463.63,
      "text": "And we just set the API key and then we'll define a few functions here."
    },
    {
      "start": 463.63,
      "end": 468.1,
      "text": "A generate response function."
    },
    {
      "start": 468.1,
      "end": 473.52,
      "text": "So we have a cat completion where we set the model to the engine."
    },
    {
      "start": 473.52,
      "end": 475.52,
      "text": "We could put anything we want here."
    },
    {
      "start": 475.52,
      "end": 477.52,
      "text": "We've got gpt 3.5 turbo."
    },
    {
      "start": 477.52,
      "end": 479.52,
      "text": "We could put 40 or whatnot."
    },
    {
      "start": 479.52,
      "end": 482.87,
      "text": "If you don't put anything, it'll."
    },
    {
      "start": 482.87,
      "end": 487.87,
      "text": "I believe it defaults to the 3.5 turbo like line chain does."
    },
    {
      "start": 487.87,
      "end": 488.87,
      "text": "It may have changed."
    },
    {
      "start": 488.87,
      "end": 493.58,
      "text": "But for now, we'll use the 3.5 turbo."
    },
    {
      "start": 493.58,
      "end": 502.02,
      "text": "We'll send up our messages with the role as user and the content will be the prompt that it takes in."
    },
    {
      "start": 502.02,
      "end": 504.02,
      "text": "We set a max token here."
    },
    {
      "start": 504.02,
      "end": 513.28,
      "text": "The context window is obviously a lot bigger, but this will limit the response to 150 tokens."
    },
    {
      "start": 513.28,
      "end": 524.41,
      "text": "Probably is around 125 words, maybe about three quarters of a word equals to a token."
    },
    {
      "start": 524.41,
      "end": 528.41,
      "text": "This n equals one is just going to get us one response."
    },
    {
      "start": 528.41,
      "end": 531.41,
      "text": "We change that to something like three."
    },
    {
      "start": 531.41,
      "end": 535.73,
      "text": "It would give us three separate responses."
    },
    {
      "start": 535.73,
      "end": 553.18,
      "text": "So what we're going to do is just a way of ending the sequence on a certain you could put a period you could put something like a line's end of line or whatever you wanted."
    },
    {
      "start": 553.18,
      "end": 558.18,
      "text": "But in this case, we're going to put 9 and we'll just let it run out to a maximum of 150 tokens."
    },
    {
      "start": 558.18,
      "end": 560.18,
      "text": "We can talk about the temperature last night."
    },
    {
      "start": 560.18,
      "end": 564.18,
      "text": "We'll set the temperature 0.7 here."
    },
    {
      "start": 564.18,
      "end": 576.18,
      "text": "But again, just to go back over the lower the temperature, the less creative the model will be the higher the temperature, the temperature range between 0 and 1 to higher the temperature."
    },
    {
      "start": 576.18,
      "end": 580.18,
      "text": "The more creatives the model would be if you would."
    },
    {
      "start": 580.18,
      "end": 585.56,
      "text": "And then here we're just looking for choices in the response."
    },
    {
      "start": 585.56,
      "end": 589.56,
      "text": "And as I said, we're only generating one choice."
    },
    {
      "start": 589.56,
      "end": 591.56,
      "text": "So it's going to select that choice."
    },
    {
      "start": 591.56,
      "end": 601.56,
      "text": "And this is just cleaning up the output so that we don't have a large output that open AI would normally return."
    },
    {
      "start": 601.56,
      "end": 603.56,
      "text": "So we'll take the content."
    },
    {
      "start": 603.56,
      "end": 607.3,
      "text": "We'll just strip off any white space."
    },
    {
      "start": 607.3,
      "end": 614.29,
      "text": "And we just got an else in case there was an unexpected response."
    },
    {
      "start": 614.29,
      "end": 616.29,
      "text": "And it'll print that out."
    },
    {
      "start": 616.29,
      "end": 622.29,
      "text": "And then we have a little bit of air handling here just so that we don't throw off the whole chatbot."
    },
    {
      "start": 622.29,
      "end": 631.64,
      "text": "If some sort of an error occurs, if an error occurs, it'll print the error and just return a blank string."
    },
    {
      "start": 631.64,
      "end": 634.67,
      "text": "Then we also have a summarize text."
    },
    {
      "start": 634.67,
      "end": 635.67,
      "text": "This will come into play."
    },
    {
      "start": 635.67,
      "end": 640.93,
      "text": "You'll see at the very end where we try to."
    },
    {
      "start": 640.93,
      "end": 647.93,
      "text": "The whole point of this chat, one of the points of this chatbot and we had talked about it last night was saving the context of the message."
    },
    {
      "start": 647.93,
      "end": 651.93,
      "text": "Since we're not talking to chat GPT in the UI."
    },
    {
      "start": 651.93,
      "end": 665.28,
      "text": "We, if we want to have context in the conversation and when we're hitting it with the API, each API call is not going to have context of the previous API call."
    },
    {
      "start": 665.28,
      "end": 672.28,
      "text": "If we don't include what the conversation has been, so we'll keep a running conversation here that we'll see in a bit."
    },
    {
      "start": 672.28,
      "end": 683.88,
      "text": "And this summarizes just a way that we're used to control the size of the chat context, if need be."
    },
    {
      "start": 683.88,
      "end": 690.88,
      "text": "So it's just a summary prompt that we'll give to open AI if we need to that says summarizes conversation."
    },
    {
      "start": 690.88,
      "end": 700.61,
      "text": "And we'll give it the entire chat text and then we go to generate response, which we looked at here."
    },
    {
      "start": 700.61,
      "end": 710.75,
      "text": "And then assess complexity is just another prompt that it's asking to assess the complexity of this conversation."
    },
    {
      "start": 710.75,
      "end": 719.06,
      "text": "And we'll be joining the history to that and then getting a complexity score."
    },
    {
      "start": 719.06,
      "end": 728.92,
      "text": "And the complexity text is just generating the response using that complexity prompt."
    },
    {
      "start": 728.92,
      "end": 735.92,
      "text": "And in the response, we're looking for a complexity score."
    },
    {
      "start": 735.92,
      "end": 743.92,
      "text": "So if you actually play with this a little bit and put some print statements in, it doesn't always return this prompt could be improved upon we could talk about that later."
    },
    {
      "start": 743.92,
      "end": 754.08,
      "text": "Well, what we're doing here is we're asking the the LOM give us a complexity of this conversation at this point."
    },
    {
      "start": 754.08,
      "end": 758.08,
      "text": "And in this case, just hoping it returns a numerical score."
    },
    {
      "start": 758.08,
      "end": 766.34,
      "text": "So if it does return a score, we'll return that score in the term of an integer."
    },
    {
      "start": 766.34,
      "end": 780.98,
      "text": "If it doesn't return anything, maybe it just returns, hey, this conversation is a medium complexity, then we'll return 20."
    },
    {
      "start": 780.98,
      "end": 791.74,
      "text": "So then we also have the class that we talked about, and this is what we're importing along with the rest of the code into the main."
    },
    {
      "start": 791.74,
      "end": 798.74,
      "text": "So we just have a basic init here. We set a base history length of 10 more of that here in a bit."
    },
    {
      "start": 798.74,
      "end": 814.03,
      "text": "And we set our open a i engine to the 3.5 turbo will start with a blank list for the chat history and we'll build up on that list."
    },
    {
      "start": 814.03,
      "end": 830.63,
      "text": "And we'll set the base history length to the 10 we've got here get response is we're simply going to join."
    },
    {
      "start": 830.63,
      "end": 838.69,
      "text": "Create a variable chat history string and we will add the chat history to the response."
    },
    {
      "start": 838.69,
      "end": 845.69,
      "text": "So as we build the chat, it'll keep adding the history in front of the actual prompt."
    },
    {
      "start": 845.69,
      "end": 855.01,
      "text": "Then we'll have the prompt here or adding both the chat history string and the input text."
    },
    {
      "start": 856.14,
      "end": 871.46,
      "text": "And then we'll do a response by generating response with the prompt and the open a a i call and then we'll update the history."
    },
    {
      "start": 871.46,
      "end": 887.46,
      "text": "So here in update history, we're simply sending it the response to the chat history will add the user input and the bot response."
    },
    {
      "start": 887.46,
      "end": 896.97,
      "text": "We'll get the complexity score with the function we talked about earlier."
    },
    {
      "start": 896.97,
      "end": 902.06,
      "text": "And then we'll come up with an adapted history link and there's nothing."
    },
    {
      "start": 902.06,
      "end": 907.06,
      "text": "This is just something that was written out. There's nothing official or magical about this."
    },
    {
      "start": 907.06,
      "end": 915.06,
      "text": "But basically what this code is doing right here is it's giving it a length in between 10 and 50."
    },
    {
      "start": 915.06,
      "end": 931.01,
      "text": "So we're taking our complexity score floor dividing it by five and taking the minimum of that be it 50 or whatever the result of this may be."
    },
    {
      "start": 931.01,
      "end": 945.3,
      "text": "And then we're taking the max of that and then we're looking at the length of the chat history and if it's greater than the adapted history that we calculated multiplied by two will do a summarized history."
    },
    {
      "start": 945.3,
      "end": 972.49,
      "text": "So all this does is sends it back to the summarize text and we'll get a summary and then we'll take this summarize history and concatenated with the cat history and all we're doing here is a little bit of fancy Python where we're adding the adaptive history links divided by two."
    },
    {
      "start": 972.49,
      "end": 979.49,
      "text": "And we add the summary and then add the last bit of the conversation to the summary."
    },
    {
      "start": 979.49,
      "end": 993.03,
      "text": "So that's how we are able to keep track of the chat history but maintain the size of the chat history so that it doesn't become too long or too complex."
    },
    {
      "start": 993.03,
      "end": 1037.03,
      "text": "Now I'm here and just run main and you see we get a response what's on your mind and we can put anything we want and this will just have a running conversation."
    },
    {
      "start": 1037.03,
      "end": 1044.03,
      "text": "This is just meant to be a basic example of a chatbot."
    },
    {
      "start": 1044.03,
      "end": 1051.7,
      "text": "If you'd like to touch to it, it's basic Python and the open AI API."
    },
    {
      "start": 1051.7,
      "end": 1066.7,
      "text": "If you run it long enough, it's kind of hard to see what's going on inside of it without a bunch of print statements, but if you'd like the code to this, I'd be happy to either taste it into the into the channel and I think you would have access to the repo as well."
    },
    {
      "start": 1066.7,
      "end": 1076.98,
      "text": "If you have any particular questions about the code, I kind of glossed over it pretty fast because it's just the two of us in here."
    },
    {
      "start": 1076.98,
      "end": 1084.98,
      "text": "So if you wanted to go back over anything in particular, just talk about the approach or ask about the homework."
    },
    {
      "start": 1084.98,
      "end": 1092.67,
      "text": "It's all you so we can do whatever you'd like to do."
    },
    {
      "start": 1092.67,
      "end": 1095.67,
      "text": "Yeah."
    },
    {
      "start": 1095.67,
      "end": 1099.67,
      "text": "Not a particular question about this."
    },
    {
      "start": 1099.67,
      "end": 1113.41,
      "text": "I mean, I think I understand that well, can you share that repo to us and just analyze it later."
    },
    {
      "start": 1113.41,
      "end": 1117.41,
      "text": "Yeah, I'll definitely share the repo, the repo."
    },
    {
      "start": 1117.41,
      "end": 1119.41,
      "text": "Just keep in mind that I'll put it."
    },
    {
      "start": 1119.41,
      "end": 1121.41,
      "text": "I'm going to post it in the channel."
    },
    {
      "start": 1121.41,
      "end": 1126.41,
      "text": "If for some reason you don't have access to it, let me get it right now while we're talking about it."
    },
    {
      "start": 1126.41,
      "end": 1130.95,
      "text": "We'll see if you should have access to it."
    },
    {
      "start": 1130.95,
      "end": 1189.48,
      "text": "Give me just a second here."
    },
    {
      "start": 1189.48,
      "end": 1208.91,
      "text": "See I'm putting it in the channel right now. See if you've got access to this if you would."
    },
    {
      "start": 1208.91,
      "end": 1211.91,
      "text": "Okay, awesome."
    },
    {
      "start": 1211.91,
      "end": 1215.91,
      "text": "Just know there's three branches in there."
    },
    {
      "start": 1215.91,
      "end": 1226.91,
      "text": "Go to the 24 a two or the 24 a three branch main branch is the starter repo, but it doesn't have the majority of the code in it."
    },
    {
      "start": 1226.91,
      "end": 1236.74,
      "text": "So either 24 a two or a three, you can go to and that's got the completed code in it."
    },
    {
      "start": 1236.74,
      "end": 1243.74,
      "text": "Like I said, it's pretty pretty basic code Python open AI API."
    },
    {
      "start": 1243.74,
      "end": 1252.85,
      "text": "And as far as and you said you had no questions about this correct."
    },
    {
      "start": 1252.85,
      "end": 1257.04,
      "text": "Well, maybe you just won."
    },
    {
      "start": 1257.04,
      "end": 1265.04,
      "text": "Why are we using well in this code we're using we're not using chain."
    },
    {
      "start": 1265.04,
      "end": 1268.04,
      "text": "I think it's the library. We use the last class."
    },
    {
      "start": 1268.04,
      "end": 1272.13,
      "text": "What is the difference between both?"
    },
    {
      "start": 1272.13,
      "end": 1276.13,
      "text": "Okay, so this code is just basic."
    },
    {
      "start": 1276.13,
      "end": 1279.13,
      "text": "Just a curiosity is we're going forward."
    },
    {
      "start": 1279.13,
      "end": 1283.96,
      "text": "How much experience do you have in Python quite a bit?"
    },
    {
      "start": 1283.96,
      "end": 1287.99,
      "text": "Yeah, I mean."
    },
    {
      "start": 1287.99,
      "end": 1296.18,
      "text": "Python. Yeah, I quite a bit, but not not much about the."
    },
    {
      "start": 1296.18,
      "end": 1299.27,
      "text": "Yeah, all of them."
    },
    {
      "start": 1299.27,
      "end": 1308.3,
      "text": "Okay, so this file is just Python and the open AI API."
    },
    {
      "start": 1308.3,
      "end": 1312.3,
      "text": "Exactly why it was written in this way."
    },
    {
      "start": 1312.3,
      "end": 1316.3,
      "text": "I couldn't tell you from starting next week."
    },
    {
      "start": 1316.3,
      "end": 1325.53,
      "text": "I'm going to be writing my own code and kind of try to get a feel of what may be helpful to the people that come to the office."
    },
    {
      "start": 1325.53,
      "end": 1330.53,
      "text": "Office hours. I have a feeling because it's a holiday in the United States."
    },
    {
      "start": 1330.53,
      "end": 1335.53,
      "text": "And because it's the first week, a lot of people didn't come and hopefully we'll have more people."
    },
    {
      "start": 1335.53,
      "end": 1340.53,
      "text": "But, you know, whoever shows up, then, you know, we'll do we'll do with the group wants to do."
    },
    {
      "start": 1340.53,
      "end": 1349.53,
      "text": "But I'll be writing my own code based on what's going on in the lessons and kind of the feel that I get and what would be best for the group."
    },
    {
      "start": 1349.53,
      "end": 1353.53,
      "text": "But the main difference is this is just calling in the same thing."
    },
    {
      "start": 1353.53,
      "end": 1356.53,
      "text": "You could write this in JavaScript. You could write this in any language."
    },
    {
      "start": 1356.53,
      "end": 1362.53,
      "text": "Langchain is a framework that is principally written in Python."
    },
    {
      "start": 1362.53,
      "end": 1369.53,
      "text": "It does have a JavaScript version that's getting pretty mature, even though it's still a bit buggy at times."
    },
    {
      "start": 1369.53,
      "end": 1372.53,
      "text": "And a new Java version just came out."
    },
    {
      "start": 1372.53,
      "end": 1376.53,
      "text": "What Langchain does is Langchain allows you."
    },
    {
      "start": 1376.53,
      "end": 1385.17,
      "text": "It's a framework that allows you to tie in your LOM, your data sources, output parsers, database."
    },
    {
      "start": 1385.17,
      "end": 1391.17,
      "text": "It allows you to create applications with large language models without."
    },
    {
      "start": 1391.17,
      "end": 1394.17,
      "text": "It's a lot of syntactic sugar is all it is."
    },
    {
      "start": 1394.17,
      "end": 1397.17,
      "text": "You could roll your own if you wanted to."
    },
    {
      "start": 1397.17,
      "end": 1406.49,
      "text": "There's other options, but like I said, this is just Python code where it's Langchain is a framework that allows you to work with LOM."
    },
    {
      "start": 1406.49,
      "end": 1414.68,
      "text": "In a much quicker and easier way, it's a great way to prototype things quickly."
    },
    {
      "start": 1414.68,
      "end": 1421.74,
      "text": "And then within the Langchain ecosystem, there's also Langsmith, which will be going over tomorrow."
    },
    {
      "start": 1421.74,
      "end": 1429.74,
      "text": "As I was saying here with this Python code, it's hard to see what's going on while we're making these calls to the LOM."
    },
    {
      "start": 1429.74,
      "end": 1440.74,
      "text": "Langsmith, as you'll see tomorrow, let you look into what's going on with your application, what's going on with with your API calls."
    },
    {
      "start": 1440.74,
      "end": 1446.74,
      "text": "And it gives you a lot more of a granular view and allows you to do a lot of debugging."
    },
    {
      "start": 1446.74,
      "end": 1457.74,
      "text": "Lang serve, which I believe that will cover at the end towards the more towards the end of the course is a quick way within the ecosystem to launch an API."
    },
    {
      "start": 1457.74,
      "end": 1471.74,
      "text": "With the code that you've created with Langchain and Langgraph is more concentrated on being able to create agents with Langchain and multi agent systems."
    },
    {
      "start": 1471.74,
      "end": 1487.06,
      "text": "We'll be using that as well. And I think towards the end of this course, the plan right now is also the integrate crew AI, which is another system that's based on AI agents and multi agent systems."
    },
    {
      "start": 1487.06,
      "end": 1495.06,
      "text": "facilitating creating those systems to accomplish whatever your personal and professional goals are."
    },
    {
      "start": 1495.06,
      "end": 1498.32,
      "text": "Did that answer your question?"
    },
    {
      "start": 1498.32,
      "end": 1516.75,
      "text": "Yeah, yeah, okay, I understand Langchain is basically a higher level and here we're using a more, I don't know, yeah, lower level of the API."
    },
    {
      "start": 1516.75,
      "end": 1521.1,
      "text": "Yeah, we're using actually the API or open AI."
    },
    {
      "start": 1521.1,
      "end": 1524.1,
      "text": "Yeah, this is just the idea."
    },
    {
      "start": 1524.1,
      "end": 1543.51,
      "text": "What Langchain would do if this was me, like we had this maybe if I tried to open, I think it's still open from last night."
    },
    {
      "start": 1543.51,
      "end": 1551.58,
      "text": "So yeah, like here, this basic code that that Julian was going over last night, this uses Langchain."
    },
    {
      "start": 1551.58,
      "end": 1562.92,
      "text": "It distracts away. We're still pulling in open AI and it's just importing into Langchain has this chat open AI."
    },
    {
      "start": 1562.92,
      "end": 1565.92,
      "text": "And then we're able to very quickly."
    },
    {
      "start": 1565.92,
      "end": 1576.31,
      "text": "You don't even need to put any of this in. You literally could just take this out with no model."
    },
    {
      "start": 1576.31,
      "end": 1579.31,
      "text": "I don't know if I've still got an environmental."
    },
    {
      "start": 1579.31,
      "end": 1585.79,
      "text": "No, I don't."
    },
    {
      "start": 1585.79,
      "end": 1617.35,
      "text": "Different open AI key there don't have the Python dot end."
    },
    {
      "start": 1617.35,
      "end": 1625.51,
      "text": "You just make sure got my open AI key now."
    },
    {
      "start": 1625.51,
      "end": 1640.61,
      "text": "Just saying system."
    },
    {
      "start": 1640.61,
      "end": 1646.61,
      "text": "It's running a little bit slow and none of the output is parsed, but yeah, it's just allowing."
    },
    {
      "start": 1646.61,
      "end": 1650.11,
      "text": "And this isn't even the thing."
    },
    {
      "start": 1650.11,
      "end": 1654.11,
      "text": "So we get to and we do two imports."
    },
    {
      "start": 1654.11,
      "end": 1656.11,
      "text": "We create an LOM here."
    },
    {
      "start": 1656.11,
      "end": 1665.11,
      "text": "I said, you don't even need it just automatically picks 3.5 and it picks up on the fact that here in the dot end file."
    },
    {
      "start": 1665.11,
      "end": 1672.68,
      "text": "Or not in the dot end file, excuse me in the environmental variables that the open AI key is there."
    },
    {
      "start": 1672.68,
      "end": 1678.68,
      "text": "And it allows us to start creating messages and just write down here."
    },
    {
      "start": 1678.68,
      "end": 1683.68,
      "text": "Instead of having to go through everything we did there, we just say LOM dot and vote messages."
    },
    {
      "start": 1683.68,
      "end": 1686.68,
      "text": "And we get a response."
    },
    {
      "start": 1686.68,
      "end": 1689.68,
      "text": "So it's like I said, it's impacting sugar."
    },
    {
      "start": 1689.68,
      "end": 1696.68,
      "text": "If you would over the Python that you would need to be able to start interacting with the LOM."
    },
    {
      "start": 1696.68,
      "end": 1700.78,
      "text": "Where it really gets helpful is when you start."
    },
    {
      "start": 1700.78,
      "end": 1705.78,
      "text": "You're needing a lot of tools to say you want to integrate vector databases."
    },
    {
      "start": 1705.78,
      "end": 1713.1,
      "text": "Say you need to do embedding. Say you need to involve more than one LOM."
    },
    {
      "start": 1713.1,
      "end": 1715.1,
      "text": "They're constantly updating."
    },
    {
      "start": 1715.1,
      "end": 1722.35,
      "text": "I don't even know how many different entities they have integrated into their system, but it's quite large."
    },
    {
      "start": 1723.35,
      "end": 1727.35,
      "text": "It makes it easier and lets you write a lot less code."
    },
    {
      "start": 1727.35,
      "end": 1729.35,
      "text": "Kind of a downside to it is."
    },
    {
      "start": 1729.35,
      "end": 1732.58,
      "text": "Is that it's constantly changing."
    },
    {
      "start": 1732.58,
      "end": 1740.58,
      "text": "So it's almost helpful at times just like you would with Python packages to lock a version in."
    },
    {
      "start": 1740.58,
      "end": 1745.58,
      "text": "Because sometimes if you have some line chain code and then you go and."
    },
    {
      "start": 1745.58,
      "end": 1750.67,
      "text": "You do your pit install and you pull in everything and it pulls in the most."
    },
    {
      "start": 1750.67,
      "end": 1752.67,
      "text": "The most recent version of line chain."
    },
    {
      "start": 1752.67,
      "end": 1756.67,
      "text": "You try to run the older code that's maybe six months old."
    },
    {
      "start": 1756.67,
      "end": 1757.67,
      "text": "It breaks."
    },
    {
      "start": 1757.67,
      "end": 1760.22,
      "text": "So it's."
    },
    {
      "start": 1760.22,
      "end": 1762.73,
      "text": "A rapidly."
    },
    {
      "start": 1762.73,
      "end": 1764.73,
      "text": "Evolving field."
    },
    {
      "start": 1764.73,
      "end": 1767.73,
      "text": "A rapidly evolving framework."
    },
    {
      "start": 1767.73,
      "end": 1768.73,
      "text": "And."
    },
    {
      "start": 1768.73,
      "end": 1771.98,
      "text": "It's to me."
    },
    {
      "start": 1771.98,
      "end": 1773.98,
      "text": "It's pretty good."
    },
    {
      "start": 1775.14,
      "end": 1782.14,
      "text": "Probably the best option that we have at this point, but it's not without its own problems."
    },
    {
      "start": 1782.14,
      "end": 1789.14,
      "text": "A lot of places in production roll their own systems to do whatever they need and to have more granular control over it."
    },
    {
      "start": 1789.14,
      "end": 1790.78,
      "text": "But."
    },
    {
      "start": 1790.78,
      "end": 1791.78,
      "text": "Yes."
    },
    {
      "start": 1791.78,
      "end": 1793.58,
      "text": "So."
    },
    {
      "start": 1793.58,
      "end": 1794.58,
      "text": "Does that answer."
    },
    {
      "start": 1794.58,
      "end": 1798.58,
      "text": "More of what you were what you were serious about with line chain."
    },
    {
      "start": 1798.58,
      "end": 1799.58,
      "text": "Yeah."
    },
    {
      "start": 1799.58,
      "end": 1800.58,
      "text": "Yeah."
    },
    {
      "start": 1800.58,
      "end": 1801.58,
      "text": "That's perfect."
    },
    {
      "start": 1801.58,
      "end": 1803.58,
      "text": "And."
    },
    {
      "start": 1803.58,
      "end": 1808.58,
      "text": "And I was going to say and through the course through the next few weeks, we're going to be we're going to be a going over."
    },
    {
      "start": 1808.58,
      "end": 1811.58,
      "text": "Lang Smith, we're going to be going over line chain expression language."
    },
    {
      "start": 1811.58,
      "end": 1815.58,
      "text": "He's going to deep dive a lot more into this and during the office hours."
    },
    {
      "start": 1815.58,
      "end": 1821.58,
      "text": "We're going to dive a lot more into this because you're going to understand a lot more about the entire line chain."
    },
    {
      "start": 1821.58,
      "end": 1824.58,
      "text": "They've got the kind of ecosystem."
    },
    {
      "start": 1824.58,
      "end": 1826.58,
      "text": "Lang Smith, Lang Serr, blank graph."
    },
    {
      "start": 1826.58,
      "end": 1836.58,
      "text": "Is we move along through the lectures to your homework."
    },
    {
      "start": 1836.58,
      "end": 1837.58,
      "text": "Through the guided projects."
    },
    {
      "start": 1837.58,
      "end": 1839.7,
      "text": "And."
    },
    {
      "start": 1839.7,
      "end": 1847.7,
      "text": "So if you, if you've got a pretty decent understanding of what it is, and you know what it does for us."
    },
    {
      "start": 1847.7,
      "end": 1851.34,
      "text": "I responses brief, so I don't just babble on and on"
    },
    {
      "start": 1851.34,
      "end": 1856.98,
      "text": "because I have a bad habit of just talking at times."
    },
    {
      "start": 1856.98,
      "end": 1858.42,
      "text": "OK, yeah, no, thank you."
    },
    {
      "start": 1858.42,
      "end": 1872.22,
      "text": "That's all of my, all of my, yeah, that's good."
    },
    {
      "start": 1872.22,
      "end": 1874.86,
      "text": "OK, and you were talking about the homework."
    },
    {
      "start": 1874.86,
      "end": 1881.6,
      "text": "What questions did you have about the homework?"
    },
    {
      "start": 1881.6,
      "end": 1884.88,
      "text": "Yeah, well, I mean, I did it, but I'm"
    },
    {
      "start": 1884.88,
      "end": 1889.36,
      "text": "not happy with the results because I don't know how to,"
    },
    {
      "start": 1889.36,
      "end": 1891.92,
      "text": "I want to know what is the correct way to do it,"
    },
    {
      "start": 1891.92,
      "end": 1896.28,
      "text": "because in my interactions, what I see"
    },
    {
      "start": 1896.28,
      "end": 1900.52,
      "text": "is that the two LAMs are talking to each other,"
    },
    {
      "start": 1900.52,
      "end": 1909.12,
      "text": "but the problem is that I want them to behave as a human,"
    },
    {
      "start": 1909.12,
      "end": 1917.21,
      "text": "but these LLAMs don't forget that they are a AIA system."
    },
    {
      "start": 1917.21,
      "end": 1920.01,
      "text": "So in some point of the conversation,"
    },
    {
      "start": 1920.01,
      "end": 1924.53,
      "text": "they say they don't respond to the correctly"
    },
    {
      "start": 1924.53,
      "end": 1928.21,
      "text": "to the other LLAM because they say something like,"
    },
    {
      "start": 1928.21,
      "end": 1931.49,
      "text": "I'm AIA system, how can I help you?"
    },
    {
      "start": 1931.49,
      "end": 1935.21,
      "text": "But this LLAM is supposed to ask questions"
    },
    {
      "start": 1935.21,
      "end": 1938.57,
      "text": "and to be more like a human."
    },
    {
      "start": 1938.57,
      "end": 1942.79,
      "text": "But I don't, I can't do that."
    },
    {
      "start": 1942.79,
      "end": 1947.97,
      "text": "I don't know if you have some insight about that or."
    },
    {
      "start": 1947.97,
      "end": 1949.61,
      "text": "Did you want, you can send it in here."
    },
    {
      "start": 1949.61,
      "end": 1951.61,
      "text": "Do you have it in a GitHub repo,"
    },
    {
      "start": 1951.61,
      "end": 1954.93,
      "text": "or in a jist, or in a shareable format?"
    },
    {
      "start": 1954.93,
      "end": 1962.48,
      "text": "Yeah, I can share you in the chat of the meeting."
    },
    {
      "start": 1962.48,
      "end": 1963.7,
      "text": "Yeah."
    },
    {
      "start": 1963.7,
      "end": 1968.74,
      "text": "OK, let me use the speakers."
    },
    {
      "start": 1970.98,
      "end": 1973.53,
      "text": "Yeah, that's it."
    },
    {
      "start": 1973.53,
      "end": 1974.53,
      "text": "Let me see."
    },
    {
      "start": 1974.53,
      "end": 1988.5,
      "text": "Yeah, I can give you one second."
    },
    {
      "start": 1988.5,
      "end": 1990.45,
      "text": "I'm going to open it up."
    },
    {
      "start": 1990.45,
      "end": 2002.71,
      "text": "Yep."
    },
    {
      "start": 2002.71,
      "end": 2004.35,
      "text": "What did you got it in the?"
    },
    {
      "start": 2004.35,
      "end": 2014.51,
      "text": "Oh, first we see the repo."
    },
    {
      "start": 2014.51,
      "end": 2017.31,
      "text": "I have another specific question."
    },
    {
      "start": 2017.31,
      "end": 2020.87,
      "text": "Can you go to the example that you see in here?"
    },
    {
      "start": 2020.87,
      "end": 2022.61,
      "text": "That's right."
    },
    {
      "start": 2022.61,
      "end": 2029.17,
      "text": "Ah, you're not in the last class repo, right, can you?"
    },
    {
      "start": 2029.21,
      "end": 2031.61,
      "text": "Yeah, did you want me to go to the repo from the last?"
    },
    {
      "start": 2031.61,
      "end": 2033.89,
      "text": "I was looking at what you sent me"
    },
    {
      "start": 2033.89,
      "end": 2036.93,
      "text": "and I was looking for where your code was."
    },
    {
      "start": 2036.93,
      "end": 2043.23,
      "text": "Oh, my code is in classic samples folder."
    },
    {
      "start": 2043.23,
      "end": 2048.75,
      "text": "And I just override the simple chat history."
    },
    {
      "start": 2048.75,
      "end": 2051.84,
      "text": "Well, yeah, sure."
    },
    {
      "start": 2051.84,
      "end": 2056.5,
      "text": "Let me open, let me open that."
    },
    {
      "start": 2056.5,
      "end": 2062.22,
      "text": "And I'll just leave that, but let me open the repo."
    },
    {
      "start": 2062.22,
      "end": 2067.62,
      "text": "So you wanted to open this repo."
    },
    {
      "start": 2067.62,
      "end": 2075.9,
      "text": "So this is a repo from last night."
    },
    {
      "start": 2075.9,
      "end": 2081.34,
      "text": "Oh, yeah, but I'm interested in the simple chat history file."
    },
    {
      "start": 2081.34,
      "end": 2082.54,
      "text": "OK."
    },
    {
      "start": 2082.54,
      "end": 2084.06,
      "text": "Yeah, there."
    },
    {
      "start": 2084.06,
      "end": 2088.5,
      "text": "Why are we, for example, in the line 16?"
    },
    {
      "start": 2088.5,
      "end": 2096.85,
      "text": "Why don't we use the AA message function"
    },
    {
      "start": 2096.85,
      "end": 2104.09,
      "text": "to encapsulate to encapsulate results 16 line?"
    },
    {
      "start": 2104.09,
      "end": 2105.33,
      "text": "Are you talking?"
    },
    {
      "start": 2105.33,
      "end": 2109.99,
      "text": "Hold on one second."
    },
    {
      "start": 2109.99,
      "end": 2124.86,
      "text": "Yeah."
    },
    {
      "start": 2124.86,
      "end": 2125.86,
      "text": "Yeah."
    },
    {
      "start": 2125.86,
      "end": 2127.86,
      "text": "Are you talking about what, like, for the output"
    },
    {
      "start": 2127.86,
      "end": 2130.34,
      "text": "where we're getting this human message,"
    },
    {
      "start": 2130.34,
      "end": 2132.58,
      "text": "and then we're getting AI message."
    },
    {
      "start": 2132.58,
      "end": 2138.47,
      "text": "And then as I start to talk, interact with the chatbot,"
    },
    {
      "start": 2138.47,
      "end": 2143.46,
      "text": "it's just returning."
    },
    {
      "start": 2143.46,
      "end": 2148.49,
      "text": "It's just coming back."
    },
    {
      "start": 2148.49,
      "end": 2149.77,
      "text": "Yeah, yeah."
    },
    {
      "start": 2149.77,
      "end": 2155.49,
      "text": "Why is it, you're asking why it's coming back like this"
    },
    {
      "start": 2155.49,
      "end": 2159.85,
      "text": "and it doesn't have that AI message wrapper around it?"
    },
    {
      "start": 2159.85,
      "end": 2162.54,
      "text": "Yeah, yeah, exactly."
    },
    {
      "start": 2162.54,
      "end": 2167.34,
      "text": "That, were you in class last night?"
    },
    {
      "start": 2167.34,
      "end": 2168.78,
      "text": "Yeah."
    },
    {
      "start": 2168.78,
      "end": 2174.5,
      "text": "I think it may have been after he dismissed class a few people"
    },
    {
      "start": 2174.5,
      "end": 2177.18,
      "text": "stuck around because somebody asked about it."
    },
    {
      "start": 2177.18,
      "end": 2179.38,
      "text": "I just don't remember the exact moment."
    },
    {
      "start": 2179.38,
      "end": 2184.94,
      "text": "And what it was, it was an oversight of the program."
    },
    {
      "start": 2184.94,
      "end": 2188.26,
      "text": "Julian went in and made some, he and I were making changes"
    },
    {
      "start": 2188.26,
      "end": 2190.82,
      "text": "before class yesterday."
    },
    {
      "start": 2190.82,
      "end": 2192.82,
      "text": "And he made a change."
    },
    {
      "start": 2192.82,
      "end": 2194.58,
      "text": "And he overlooked it."
    },
    {
      "start": 2194.58,
      "end": 2195.66,
      "text": "And then I saw it."
    },
    {
      "start": 2195.66,
      "end": 2198.02,
      "text": "And I saw the same thing you're seeing."
    },
    {
      "start": 2198.02,
      "end": 2199.94,
      "text": "And I forgot to mention it."
    },
    {
      "start": 2199.94,
      "end": 2202.3,
      "text": "So it was just a simple oversight."
    },
    {
      "start": 2202.3,
      "end": 2205.22,
      "text": "What he remember here, what he was doing,"
    },
    {
      "start": 2205.22,
      "end": 2207.74,
      "text": "and he was adding this dot content."
    },
    {
      "start": 2207.74,
      "end": 2212.58,
      "text": "So if you take this dot content out,"
    },
    {
      "start": 2212.58,
      "end": 2216.74,
      "text": "think if you just go here, I saved it."
    },
    {
      "start": 2216.74,
      "end": 2219.74,
      "text": "Let me get out of here."
    },
    {
      "start": 2219.74,
      "end": 2222.59,
      "text": "Let me run it again."
    },
    {
      "start": 2222.59,
      "end": 2226.06,
      "text": "Simple chat history."
    },
    {
      "start": 2226.06,
      "end": 2226.82,
      "text": "OK, boom."
    },
    {
      "start": 2226.82,
      "end": 2231.58,
      "text": "So we have the chat history."
    },
    {
      "start": 2231.58,
      "end": 2244.38,
      "text": "OK, it works."
    },
    {
      "start": 2244.38,
      "end": 2245.58,
      "text": "OK."
    },
    {
      "start": 2245.58,
      "end": 2250.5,
      "text": "See, the human message here, could you"
    },
    {
      "start": 2250.5,
      "end": 2252.78,
      "text": "give me the result in actually Tom's?"
    },
    {
      "start": 2252.78,
      "end": 2258.81,
      "text": "And then you have the AI message wrapper here,"
    },
    {
      "start": 2258.81,
      "end": 2260.77,
      "text": "but with all this metadata."
    },
    {
      "start": 2260.77,
      "end": 2266.4,
      "text": "So to be able to showcase it to the class,"
    },
    {
      "start": 2266.4,
      "end": 2269.68,
      "text": "he just went in and added dot content,"
    },
    {
      "start": 2269.68,
      "end": 2272.24,
      "text": "quit to be able to pull the content."
    },
    {
      "start": 2272.24,
      "end": 2277.76,
      "text": "And when he did that, it cut off this, the AI wrapper."
    },
    {
      "start": 2277.76,
      "end": 2285.02,
      "text": "That was it, there was, this was just a quick program"
    },
    {
      "start": 2285.02,
      "end": 2288.18,
      "text": "to demonstrate, saving chat history."
    },
    {
      "start": 2288.18,
      "end": 2290.62,
      "text": "There was no reason to take it off."
    },
    {
      "start": 2290.62,
      "end": 2293.22,
      "text": "And that's the reason it came off."
    },
    {
      "start": 2293.22,
      "end": 2296.74,
      "text": "And it was a simple oversight."
    },
    {
      "start": 2296.74,
      "end": 2302.32,
      "text": "OK, yeah, I got it."
    },
    {
      "start": 2302.32,
      "end": 2305.32,
      "text": "Is there anything else about the code from the other night?"
    },
    {
      "start": 2305.32,
      "end": 2308.52,
      "text": "Did she have any questions about while we're on Twitter?"
    },
    {
      "start": 2313.1,
      "end": 2319.5,
      "text": "Basically, yeah, what is the difference between,"
    },
    {
      "start": 2319.5,
      "end": 2323.06,
      "text": "I mean, it's a very simple question."
    },
    {
      "start": 2323.06,
      "end": 2327.98,
      "text": "What is the difference between AI message and human message?"
    },
    {
      "start": 2327.98,
      "end": 2333.9,
      "text": "And for example, if I use the function invoke,"
    },
    {
      "start": 2333.9,
      "end": 2339.22,
      "text": "is that mean that the last message has to be a human message"
    },
    {
      "start": 2339.22,
      "end": 2346.02,
      "text": "since the last message will be a AI message?"
    },
    {
      "start": 2346.02,
      "end": 2349.83,
      "text": "Yeah, so the difference between the AI message"
    },
    {
      "start": 2349.83,
      "end": 2351.99,
      "text": "and the human message, yes, the human message"
    },
    {
      "start": 2351.99,
      "end": 2355.55,
      "text": "is what's going to the LOM."
    },
    {
      "start": 2355.55,
      "end": 2359.79,
      "text": "The AI message is what's coming back from the LOM."
    },
    {
      "start": 2359.79,
      "end": 2363.55,
      "text": "Here we created this, here we created just as an example,"
    },
    {
      "start": 2363.55,
      "end": 2365.63,
      "text": "we created this previous context."
    },
    {
      "start": 2365.63,
      "end": 2370.67,
      "text": "So we had a human message, the AI response, the human message."
    },
    {
      "start": 2370.67,
      "end": 2374.35,
      "text": "And then we invoked the LOM with it."
    },
    {
      "start": 2374.35,
      "end": 2379.99,
      "text": "And the LOM gave a result, the AI message, like we saw."
    },
    {
      "start": 2379.99,
      "end": 2383.43,
      "text": "So the difference between them is the human message."
    },
    {
      "start": 2383.43,
      "end": 2388.27,
      "text": "This is a long chain abstraction."
    },
    {
      "start": 2388.27,
      "end": 2391.19,
      "text": "The human message is what the user is putting in,"
    },
    {
      "start": 2391.19,
      "end": 2395.83,
      "text": "the AI message is whatever LOM we're using is sending back."
    },
    {
      "start": 2395.83,
      "end": 2403.31,
      "text": "So for example, if we granted a kin,"
    },
    {
      "start": 2403.31,
      "end": 2405.91,
      "text": "I think I saved that here."
    },
    {
      "start": 2405.91,
      "end": 2411.84,
      "text": "So we'll get back to a cleaner version."
    },
    {
      "start": 2411.84,
      "end": 2415.08,
      "text": "So you see here, we get the human message, translate,"
    },
    {
      "start": 2415.08,
      "end": 2416.84,
      "text": "this is just the history that we gave."
    },
    {
      "start": 2416.84,
      "end": 2418.44,
      "text": "It translates the sentence to English,"
    },
    {
      "start": 2418.44,
      "end": 2421.2,
      "text": "to French hello programming, the AI message,"
    },
    {
      "start": 2421.2,
      "end": 2422.88,
      "text": "translate it in the human message."
    },
    {
      "start": 2422.88,
      "end": 2424.6,
      "text": "What did you say?"
    },
    {
      "start": 2424.6,
      "end": 2427.0,
      "text": "And that's where our history ends."
    },
    {
      "start": 2427.0,
      "end": 2434.08,
      "text": "And we invoked the message here with just these messages"
    },
    {
      "start": 2434.08,
      "end": 2438.78,
      "text": "in our fake history that we've created."
    },
    {
      "start": 2438.78,
      "end": 2443.62,
      "text": "But then it answers back, I said, and it gives you,"
    },
    {
      "start": 2443.62,
      "end": 2447.1,
      "text": "which means I love programming in French."
    },
    {
      "start": 2447.1,
      "end": 2452.18,
      "text": "So ideally, if we had not abstracted away with this.content,"
    },
    {
      "start": 2452.18,
      "end": 2453.86,
      "text": "if we had taken this out,"
    },
    {
      "start": 2453.86,
      "end": 2455.74,
      "text": "you'd have that AI message wrapper,"
    },
    {
      "start": 2455.74,
      "end": 2457.98,
      "text": "like we saw a few minutes ago around it."
    },
    {
      "start": 2457.98,
      "end": 2460.74,
      "text": "That's it."
    },
    {
      "start": 2460.74,
      "end": 2462.74,
      "text": "Thanks."
    },
    {
      "start": 2462.74,
      "end": 2468.47,
      "text": "And in reality, almost any time that comes to mind"
    },
    {
      "start": 2468.47,
      "end": 2470.27,
      "text": "that you're going to be doing with these,"
    },
    {
      "start": 2470.27,
      "end": 2472.39,
      "text": "you're going to be using output parsers."
    },
    {
      "start": 2472.39,
      "end": 2475.39,
      "text": "So the output parser, depending on what your goal is"
    },
    {
      "start": 2475.39,
      "end": 2477.79,
      "text": "and what you're trying to do with the output,"
    },
    {
      "start": 2477.79,
      "end": 2480.15,
      "text": "will take care of a lot of that for you."
    },
    {
      "start": 2480.15,
      "end": 2483.03,
      "text": "But yeah, in this case, it's just a message"
    },
    {
      "start": 2483.03,
      "end": 2485.59,
      "text": "that we're sending or the user is sending"
    },
    {
      "start": 2485.59,
      "end": 2487.71,
      "text": "versus what we're getting back."
    },
    {
      "start": 2487.71,
      "end": 2491.52,
      "text": "Does that make sense?"
    },
    {
      "start": 2491.52,
      "end": 2493.93,
      "text": "OK, cool."
    },
    {
      "start": 2493.93,
      "end": 2499.41,
      "text": "Yeah, I think that is everything about this repo."
    },
    {
      "start": 2499.41,
      "end": 2504.85,
      "text": "And I don't know if we can see the one that I share."
    },
    {
      "start": 2504.85,
      "end": 2510.15,
      "text": "So I can show you what I did."
    },
    {
      "start": 2510.15,
      "end": 2512.07,
      "text": "Yeah, hold on one second, let me."
    },
    {
      "start": 2512.07,
      "end": 2513.07,
      "text": "Yeah."
    },
    {
      "start": 2513.55,
      "end": 2529.76,
      "text": "Minimize this and let me go back."
    },
    {
      "start": 2529.76,
      "end": 2532.44,
      "text": "And I'm sure you won't, but we are recording this."
    },
    {
      "start": 2532.44,
      "end": 2537.04,
      "text": "So you're OK with me pulling this up on the recording."
    },
    {
      "start": 2537.04,
      "end": 2538.46,
      "text": "Yeah, of course."
    },
    {
      "start": 2538.46,
      "end": 2541.31,
      "text": "Yeah, I thought it was."
    },
    {
      "start": 2541.31,
      "end": 2546.85,
      "text": "I just wanted to be clear."
    },
    {
      "start": 2546.85,
      "end": 2553.7,
      "text": "So yeah, the thing here is, what do you think about what I did"
    },
    {
      "start": 2553.7,
      "end": 2557.54,
      "text": "because my best results about the conversation"
    },
    {
      "start": 2557.54,
      "end": 2560.22,
      "text": "was having two messages."
    },
    {
      "start": 2560.22,
      "end": 2562.78,
      "text": "It's time in two arrays of messages,"
    },
    {
      "start": 2562.78,
      "end": 2566.94,
      "text": "where the first one starts with a human message."
    },
    {
      "start": 2566.94,
      "end": 2572.49,
      "text": "And the second one starts with AI message."
    },
    {
      "start": 2572.49,
      "end": 2582.73,
      "text": "And in this way, the LLM always responds as AI message."
    },
    {
      "start": 2582.73,
      "end": 2589.57,
      "text": "Because if I don't do that, the conversation goes really"
    },
    {
      "start": 2589.57,
      "end": 2590.25,
      "text": "back."
    },
    {
      "start": 2590.25,
      "end": 2595.03,
      "text": "I don't know why."
    },
    {
      "start": 2595.03,
      "end": 2596.15,
      "text": "This had a curiosity."
    },
    {
      "start": 2596.15,
      "end": 2597.35,
      "text": "So I'm going to have to look."
    },
    {
      "start": 2597.35,
      "end": 2598.55,
      "text": "I'm just going to think of this code real quick."
    },
    {
      "start": 2598.55,
      "end": 2599.59,
      "text": "What are these covers?"
    },
    {
      "start": 2599.59,
      "end": 2601.45,
      "text": "What are these?"
    },
    {
      "start": 2601.45,
      "end": 2609.65,
      "text": "Just to call or the text to see better the conversation,"
    },
    {
      "start": 2609.65,
      "end": 2611.42,
      "text": "the chat."
    },
    {
      "start": 2611.42,
      "end": 2615.02,
      "text": "I mean, when I print in, yeah."
    },
    {
      "start": 2615.02,
      "end": 2619.06,
      "text": "Because I print the conversation instead of print the,"
    },
    {
      "start": 2619.06,
      "end": 2620.98,
      "text": "for example, yeah, that's output."
    },
    {
      "start": 2620.98,
      "end": 2624.07,
      "text": "It is some example of that code."
    },
    {
      "start": 2624.07,
      "end": 2631.94,
      "text": "And yeah, at some point, one of the chat"
    },
    {
      "start": 2631.94,
      "end": 2640.88,
      "text": "pot thinks that they are AI a system"
    },
    {
      "start": 2640.88,
      "end": 2646.88,
      "text": "and the conversation goes back."
    },
    {
      "start": 2646.88,
      "end": 2650.89,
      "text": "Just because I was still looking at the code,"
    },
    {
      "start": 2650.89,
      "end": 2653.41,
      "text": "did, how did you sell it?"
    },
    {
      "start": 2653.41,
      "end": 2662.74,
      "text": "You had this on your system, right now, simple."
    },
    {
      "start": 2662.74,
      "end": 2666.98,
      "text": "So when you were running this, you didn't limit it in any way."
    },
    {
      "start": 2666.98,
      "end": 2667.78,
      "text": "You just ran it."
    },
    {
      "start": 2667.78,
      "end": 2670.66,
      "text": "You killed it manually, or is there something"
    },
    {
      "start": 2670.66,
      "end": 2674.34,
      "text": "that I'm worried that you were just conversing?"
    },
    {
      "start": 2674.34,
      "end": 2680.87,
      "text": "Oh, I just press a key to continue the conversation."
    },
    {
      "start": 2680.87,
      "end": 2691.73,
      "text": "All right, that's just OK."
    },
    {
      "start": 2691.73,
      "end": 2697.37,
      "text": "Yeah, and that is my best try because, yeah,"
    },
    {
      "start": 2697.37,
      "end": 2703.01,
      "text": "if I don't do this having two types of messages,"
    },
    {
      "start": 2703.01,
      "end": 2706.25,
      "text": "message A and message B, I only use one."
    },
    {
      "start": 2706.25,
      "end": 2713.74,
      "text": "The dilemmas are a little confused."
    },
    {
      "start": 2713.74,
      "end": 2720.14,
      "text": "I don't have a specific conversation."
    },
    {
      "start": 2720.14,
      "end": 2722.49,
      "text": "I didn't save it."
    },
    {
      "start": 2722.49,
      "end": 2731.32,
      "text": "But they basically repeat what the other LLM say."
    },
    {
      "start": 2731.32,
      "end": 2734.61,
      "text": "But I don't know if this is necessary."
    },
    {
      "start": 2734.61,
      "end": 2743.15,
      "text": "My logic here was having the last message"
    },
    {
      "start": 2743.15,
      "end": 2749.69,
      "text": "as I use as a human message."
    },
    {
      "start": 2749.69,
      "end": 2752.17,
      "text": "For example, as you can see in message A,"
    },
    {
      "start": 2752.17,
      "end": 2753.97,
      "text": "the last message is a human message."
    },
    {
      "start": 2753.97,
      "end": 2758.49,
      "text": "That means that the last person to respond"
    },
    {
      "start": 2758.49,
      "end": 2772.83,
      "text": "will be chat A, that is a CIA message."
    },
    {
      "start": 2772.83,
      "end": 2774.31,
      "text": "Yeah, I'm going to hold on one second."
    },
    {
      "start": 2774.31,
      "end": 2776.5,
      "text": "I'm going to throw this into."
    },
    {
      "start": 2776.5,
      "end": 2799.41,
      "text": "Yep."
    },
    {
      "start": 2799.41,
      "end": 2802.17,
      "text": "I am using the same requirements."
    },
    {
      "start": 2802.17,
      "end": 2824.0,
      "text": "I pull this into the chat, so that's going to be a problem."
    },
    {
      "start": 2824.0,
      "end": 2878.54,
      "text": "And the chat B translate the sense from English to French."
    },
    {
      "start": 2878.54,
      "end": 2881.1,
      "text": "I will put in chat A, chat B."
    },
    {
      "start": 2881.1,
      "end": 2882.26,
      "text": "What did you say, chat A?"
    },
    {
      "start": 2882.26,
      "end": 2885.94,
      "text": "I said, it means I'll have programming in French."
    },
    {
      "start": 2885.94,
      "end": 2895.9,
      "text": "Press to continue the conversation."
    },
    {
      "start": 2895.9,
      "end": 2906.17,
      "text": "So every way, translate the sense A, OK, it's turning the whole."
    },
    {
      "start": 2906.17,
      "end": 2907.65,
      "text": "How do you say, what is it?"
    },
    {
      "start": 2907.65,
      "end": 2910.65,
      "text": "OK, so the chat B is asking, how do you say,"
    },
    {
      "start": 2910.65,
      "end": 2919.59,
      "text": "what is your name in French gives the correct answer?"
    },
    {
      "start": 2919.59,
      "end": 2923.58,
      "text": "Whether languages are you interested in?"
    },
    {
      "start": 2923.58,
      "end": 2933.82,
      "text": "So chat B, jumps, chat B starts asking,"
    },
    {
      "start": 2933.82,
      "end": 2937.54,
      "text": "what are the interesting learning learning languages."
    },
    {
      "start": 2937.54,
      "end": 2943.67,
      "text": "That's great."
    },
    {
      "start": 2943.67,
      "end": 2945.91,
      "text": "How do you plan to practice and improve your language skills"
    },
    {
      "start": 2945.91,
      "end": 2947.94,
      "text": "in these languages?"
    },
    {
      "start": 2947.94,
      "end": 2950.26,
      "text": "I engage in conversation."
    },
    {
      "start": 2950.26,
      "end": 2954.56,
      "text": "So I mean, you've got."
    },
    {
      "start": 2954.56,
      "end": 2956.4,
      "text": "Have you tried using language?"
    },
    {
      "start": 2956.4,
      "end": 2957.64,
      "text": "No, it's not bad at all."
    },
    {
      "start": 2957.64,
      "end": 2964.78,
      "text": "And I'm just looking at just what it's doing."
    },
    {
      "start": 2964.78,
      "end": 2967.01,
      "text": "It's a language exchange project."
    },
    {
      "start": 2967.01,
      "end": 2970.27,
      "text": "I haven't tried using language exchange."
    },
    {
      "start": 2970.27,
      "end": 2974.87,
      "text": "Now I would say, actually, the purpose of this assignment"
    },
    {
      "start": 2974.87,
      "end": 2979.38,
      "text": "was just to kind of get you moving in the course."
    },
    {
      "start": 2979.38,
      "end": 2982.86,
      "text": "Obviously, this is a really simplistic version"
    },
    {
      "start": 2982.86,
      "end": 2985.54,
      "text": "and everything that we're going to be dealing with,"
    },
    {
      "start": 2985.54,
      "end": 2989.66,
      "text": "with useful applications is going to be a lot more complicated."
    },
    {
      "start": 2989.66,
      "end": 2992.34,
      "text": "It's going to involve a lot of prompt engineering."
    },
    {
      "start": 2992.34,
      "end": 2995.82,
      "text": "It's going to involve a lot more moving parts."
    },
    {
      "start": 2995.82,
      "end": 3003.18,
      "text": "But certainly, what LLM do you normally use GPP card?"
    },
    {
      "start": 3003.18,
      "end": 3005.74,
      "text": "What's your go to?"
    },
    {
      "start": 3005.74,
      "end": 3010.91,
      "text": "Yeah, chat GPT, the last version, and club, both of them."
    },
    {
      "start": 3010.91,
      "end": 3011.91,
      "text": "OK."
    },
    {
      "start": 3011.91,
      "end": 3017.71,
      "text": "So because this is using 3.5, when you were running it,"
    },
    {
      "start": 3017.71,
      "end": 3020.41,
      "text": "were you using 3.5 as well?"
    },
    {
      "start": 3020.41,
      "end": 3022.28,
      "text": "Oh, yeah."
    },
    {
      "start": 3022.28,
      "end": 3028.75,
      "text": "What I'm telling you?"
    },
    {
      "start": 3028.75,
      "end": 3029.75,
      "text": "OK."
    },
    {
      "start": 3029.75,
      "end": 3032.75,
      "text": "So there is no magic formula for this."
    },
    {
      "start": 3032.75,
      "end": 3037.31,
      "text": "I mean, I have to modify the prompt"
    },
    {
      "start": 3037.31,
      "end": 3045.33,
      "text": "and tell the LLM how to behave and try it and try it to see"
    },
    {
      "start": 3045.33,
      "end": 3047.34,
      "text": "two results."
    },
    {
      "start": 3047.34,
      "end": 3047.86,
      "text": "Yeah."
    },
    {
      "start": 3047.86,
      "end": 3051.82,
      "text": "And like I said, what we'll get into tomorrow with Lang Smith,"
    },
    {
      "start": 3051.82,
      "end": 3055.98,
      "text": "you'll be able to see a lot more of what's going on inside"
    },
    {
      "start": 3055.98,
      "end": 3059.67,
      "text": "of each of the calls."
    },
    {
      "start": 3059.67,
      "end": 3065.71,
      "text": "And yeah, for something like this, and using 3.5,"
    },
    {
      "start": 3065.71,
      "end": 3078.99,
      "text": "what I was curious about when it changed this to 4.0,"
    },
    {
      "start": 3078.99,
      "end": 3082.3,
      "text": "I am into 4.0."
    },
    {
      "start": 3082.3,
      "end": 3084.02,
      "text": "I'll say we get a temperature."
    },
    {
      "start": 3084.02,
      "end": 3090.02,
      "text": "Let's leave the temperature as is."
    },
    {
      "start": 3090.02,
      "end": 3093.06,
      "text": "Let me just try to run this again."
    },
    {
      "start": 3093.06,
      "end": 3101.66,
      "text": "OK."
    },
    {
      "start": 3101.66,
      "end": 3110.85,
      "text": "I'm just, oh, I see."
    },
    {
      "start": 3110.85,
      "end": 3112.49,
      "text": "How do you say I'm learning a new language?"
    },
    {
      "start": 3112.49,
      "end": 3121.41,
      "text": "OK."
    },
    {
      "start": 3121.41,
      "end": 3122.13,
      "text": "That's interesting."
    },
    {
      "start": 3122.13,
      "end": 3124.18,
      "text": "How do you pronounce J?"
    },
    {
      "start": 3124.18,
      "end": 3126.58,
      "text": "Sure, here's a simple way to break down the pronunciation."
    },
    {
      "start": 3126.58,
      "end": 3129.1,
      "text": "Do you see what kind of better results you're getting?"
    },
    {
      "start": 3129.1,
      "end": 3131.06,
      "text": "You got to think this is 3.5 turbo."
    },
    {
      "start": 3131.06,
      "end": 3132.78,
      "text": "The 3.5 turbo is great."
    },
    {
      "start": 3132.78,
      "end": 3135.22,
      "text": "It's economical for a lot of calls."
    },
    {
      "start": 3135.22,
      "end": 3136.94,
      "text": "You can fine tune it pretty cheap."
    },
    {
      "start": 3136.94,
      "end": 3143.78,
      "text": "But 3.5 turbo came out when it was already 4 came out"
    },
    {
      "start": 3143.78,
      "end": 3147.64,
      "text": "at the beginning of last year."
    },
    {
      "start": 3147.64,
      "end": 3149.72,
      "text": "So when we put a better model into it,"
    },
    {
      "start": 3149.72,
      "end": 3152.52,
      "text": "you're already seeing the difference"
    },
    {
      "start": 3152.52,
      "end": 3158.4,
      "text": "in the outputs that we're getting just by changing the model."
    },
    {
      "start": 3158.4,
      "end": 3160.76,
      "text": "If you were going to say you were going to do something,"
    },
    {
      "start": 3160.76,
      "end": 3165.22,
      "text": "let's obviously we wouldn't be doing something so trivial"
    },
    {
      "start": 3165.22,
      "end": 3165.94,
      "text": "for production."
    },
    {
      "start": 3165.94,
      "end": 3168.62,
      "text": "But let's just say we were doing this in production,"
    },
    {
      "start": 3168.62,
      "end": 3171.42,
      "text": "but we couldn't use 4.0 because it was going to blow our budget up."
    },
    {
      "start": 3171.42,
      "end": 3173.46,
      "text": "So we're going to use 3.5."
    },
    {
      "start": 3173.5,
      "end": 3176.58,
      "text": "Well, there we start thinking about how do we prompt engineer this?"
    },
    {
      "start": 3176.58,
      "end": 3181.3,
      "text": "What can we do internally with and without AI"
    },
    {
      "start": 3181.3,
      "end": 3182.58,
      "text": "to get better results?"
    },
    {
      "start": 3182.58,
      "end": 3185.58,
      "text": "Maybe the hey can we fine tune this model"
    },
    {
      "start": 3185.58,
      "end": 3188.54,
      "text": "and get better results?"
    },
    {
      "start": 3188.54,
      "end": 3191.1,
      "text": "And there's just depending on the task,"
    },
    {
      "start": 3191.1,
      "end": 3193.18,
      "text": "there's so many different models."
    },
    {
      "start": 3193.18,
      "end": 3196.22,
      "text": "There's a ton of models that have already been fine tuned,"
    },
    {
      "start": 3196.22,
      "end": 3200.38,
      "text": "especially the lava models that you can go get a ugly face."
    },
    {
      "start": 3200.38,
      "end": 3202.34,
      "text": "So it really depends on your,"
    },
    {
      "start": 3202.34,
      "end": 3204.62,
      "text": "it's going to depend on your use case."
    },
    {
      "start": 3204.62,
      "end": 3208.88,
      "text": "It's going to depend on what kind of inference time"
    },
    {
      "start": 3208.88,
      "end": 3210.12,
      "text": "you're looking for."
    },
    {
      "start": 3210.12,
      "end": 3213.88,
      "text": "It's going to depend on what kind of budget that she's got."
    },
    {
      "start": 3213.88,
      "end": 3215.72,
      "text": "There's so many variables that go into it,"
    },
    {
      "start": 3215.72,
      "end": 3220.88,
      "text": "but you see, I think that's pretty cool that it was already."
    },
    {
      "start": 3220.88,
      "end": 3222.36,
      "text": "Yeah."
    },
    {
      "start": 3222.36,
      "end": 3227.0,
      "text": "Have a nice lighty and a response."
    },
    {
      "start": 3227.0,
      "end": 3231.28,
      "text": "Yeah, so feel free as well to mess around"
    },
    {
      "start": 3231.28,
      "end": 3239.38,
      "text": "with the model and see the different types of responses"
    },
    {
      "start": 3239.38,
      "end": 3240.34,
      "text": "that you get."
    },
    {
      "start": 3240.34,
      "end": 3242.82,
      "text": "Later on in the class, we'll specifically"
    },
    {
      "start": 3242.82,
      "end": 3249.14,
      "text": "address fine tuning models to be able to get concentrate"
    },
    {
      "start": 3249.14,
      "end": 3251.7,
      "text": "on the type of results exactly that you're looking for."
    },
    {
      "start": 3251.7,
      "end": 3255.7,
      "text": "So you'd how much do you know about fine tuning"
    },
    {
      "start": 3255.7,
      "end": 3260.48,
      "text": "and models in general and whatnot?"
    },
    {
      "start": 3260.48,
      "end": 3269.16,
      "text": "No, basically, I can say nothing, basically."
    },
    {
      "start": 3269.16,
      "end": 3269.92,
      "text": "That's fine."
    },
    {
      "start": 3269.92,
      "end": 3272.6,
      "text": "I'll be, you know, to tell you a little bit about myself,"
    },
    {
      "start": 3272.6,
      "end": 3276.6,
      "text": "I got into, I was originally an epidemiologist."
    },
    {
      "start": 3276.6,
      "end": 3280.72,
      "text": "I got into software engineering seven years ago."
    },
    {
      "start": 3280.72,
      "end": 3283.68,
      "text": "I started out with the MIRN stack."
    },
    {
      "start": 3283.68,
      "end": 3286.48,
      "text": "I moved on and learned Python, I learned C."
    },
    {
      "start": 3286.48,
      "end": 3289.24,
      "text": "I spent about up to now half my time in production"
    },
    {
      "start": 3289.36,
      "end": 3291.8,
      "text": "and about the other half in education,"
    },
    {
      "start": 3291.8,
      "end": 3294.44,
      "text": "doing everything from teaching classes"
    },
    {
      "start": 3294.44,
      "end": 3297.6,
      "text": "to curriculum development."
    },
    {
      "start": 3297.6,
      "end": 3301.08,
      "text": "Right now I work for a startup in production,"
    },
    {
      "start": 3301.08,
      "end": 3302.24,
      "text": "kind of across the stack."
    },
    {
      "start": 3302.24,
      "end": 3305.28,
      "text": "So I'm still working with the front end of the back end,"
    },
    {
      "start": 3305.28,
      "end": 3307.48,
      "text": "but also work with the AI."
    },
    {
      "start": 3307.48,
      "end": 3309.68,
      "text": "I work with the models."
    },
    {
      "start": 3309.68,
      "end": 3312.8,
      "text": "And I've got about a year doing it,"
    },
    {
      "start": 3312.8,
      "end": 3315.64,
      "text": "but this day and age, the way things are changing,"
    },
    {
      "start": 3315.64,
      "end": 3318.04,
      "text": "you know, a year's a long time."
    },
    {
      "start": 3318.04,
      "end": 3320.76,
      "text": "So, I mean, by the time you're done with this course,"
    },
    {
      "start": 3320.76,
      "end": 3322.48,
      "text": "you're going to look back and you're going to,"
    },
    {
      "start": 3322.48,
      "end": 3324.32,
      "text": "after 10, I guess 11 weeks,"
    },
    {
      "start": 3324.32,
      "end": 3327.72,
      "text": "because there's one week off during the July 4th holiday"
    },
    {
      "start": 3327.72,
      "end": 3331.77,
      "text": "for the United States, you're going to look back"
    },
    {
      "start": 3331.77,
      "end": 3334.17,
      "text": "and be like, man, I just learned a ton."
    },
    {
      "start": 3334.17,
      "end": 3337.57,
      "text": "And then looking at the news of everything that comes out"
    },
    {
      "start": 3337.57,
      "end": 3339.89,
      "text": "on how much AI you keep up with,"
    },
    {
      "start": 3339.89,
      "end": 3342.97,
      "text": "but so many new models come out every day."
    },
    {
      "start": 3342.97,
      "end": 3345.52,
      "text": "And a lot of times, so yeah."
    },
    {
      "start": 3345.52,
      "end": 3347.84,
      "text": "I guess with the fine tuning,"
    },
    {
      "start": 3347.84,
      "end": 3350.56,
      "text": "just a 30,000 foot of it,"
    },
    {
      "start": 3350.56,
      "end": 3354.84,
      "text": "you can have a base model that's pretty dang smart,"
    },
    {
      "start": 3354.84,
      "end": 3357.56,
      "text": "but cheat to use, like GPG 3.5."
    },
    {
      "start": 3357.56,
      "end": 3361.2,
      "text": "And what you can do is you can go in with various methods"
    },
    {
      "start": 3361.2,
      "end": 3364.84,
      "text": "and you can train that model more specifically"
    },
    {
      "start": 3364.84,
      "end": 3368.68,
      "text": "on the task that you're looking for it for it to do."
    },
    {
      "start": 3368.68,
      "end": 3372.0,
      "text": "Let's say in the case that prompt engineering didn't work,"
    },
    {
      "start": 3372.0,
      "end": 3377.32,
      "text": "the next step may be to do some degree of fine tuning."
    },
    {
      "start": 3379.24,
      "end": 3382.7,
      "text": "And you can see a lot of times,"
    },
    {
      "start": 3382.7,
      "end": 3386.9,
      "text": "even with some very minimal tuning,"
    },
    {
      "start": 3386.9,
      "end": 3388.34,
      "text": "and a pretty small budget,"
    },
    {
      "start": 3388.34,
      "end": 3392.1,
      "text": "you can really increase the quality of the responses"
    },
    {
      "start": 3392.1,
      "end": 3397.14,
      "text": "that you're getting specific to the goals that you have."
    },
    {
      "start": 3397.38,
      "end": 3401.26,
      "text": "So these big models like GPT 4,"
    },
    {
      "start": 3402.22,
      "end": 3405.1,
      "text": "they're pretty expensive when you start making tens,"
    },
    {
      "start": 3405.1,
      "end": 3408.34,
      "text": "hundreds, thousands, millions of calls to them."
    },
    {
      "start": 3408.34,
      "end": 3409.62,
      "text": "It's some of the smaller models,"
    },
    {
      "start": 3409.62,
      "end": 3410.98,
      "text": "and then another thing,"
    },
    {
      "start": 3410.98,
      "end": 3413.98,
      "text": "have you messed around at all with running models locally?"
    },
    {
      "start": 3417.05,
      "end": 3421.37,
      "text": "I try to run some time."
    },
    {
      "start": 3425.06,
      "end": 3428.26,
      "text": "This image engineering model,"
    },
    {
      "start": 3428.26,
      "end": 3429.42,
      "text": "I forget the name,"
    },
    {
      "start": 3430.42,
      "end": 3434.74,
      "text": "what it is, the stable diffusion."
    },
    {
      "start": 3434.74,
      "end": 3439.74,
      "text": "But yeah, I couldn't in that time and I forget it,"
    },
    {
      "start": 3439.74,
      "end": 3444.37,
      "text": "but I like to run some locally."
    },
    {
      "start": 3444.37,
      "end": 3449.01,
      "text": "That's the depending on your work and where you're working"
    },
    {
      "start": 3449.01,
      "end": 3450.57,
      "text": "and what you need a lot of times,"
    },
    {
      "start": 3450.57,
      "end": 3453.69,
      "text": "you won't be able to make a call to open AI"
    },
    {
      "start": 3453.69,
      "end": 3455.85,
      "text": "because you're dealing with proprietary data,"
    },
    {
      "start": 3455.85,
      "end": 3458.13,
      "text": "you're dealing with sensitive data,"
    },
    {
      "start": 3458.13,
      "end": 3462.17,
      "text": "and in that case, we'll be covering that very shortly,"
    },
    {
      "start": 3462.17,
      "end": 3465.21,
      "text": "ways of interacting in-house with your own data,"
    },
    {
      "start": 3465.21,
      "end": 3470.21,
      "text": "and there are ways of making calls to APIs from the edge"
    },
    {
      "start": 3472.25,
      "end": 3476.09,
      "text": "and then working in-house with your own data"
    },
    {
      "start": 3476.09,
      "end": 3479.82,
      "text": "or you might be using everything totally in-house"
    },
    {
      "start": 3479.82,
      "end": 3480.9,
      "text": "on your own servers,"
    },
    {
      "start": 3480.9,
      "end": 3483.02,
      "text": "it just really depends on the circumstance."
    },
    {
      "start": 3483.02,
      "end": 3484.98,
      "text": "Like here, they set it up,"
    },
    {
      "start": 3484.98,
      "end": 3487.3,
      "text": "I don't think they ever did it last night,"
    },
    {
      "start": 3487.3,
      "end": 3491.59,
      "text": "but you could switch over to,"
    },
    {
      "start": 3491.59,
      "end": 3498.43,
      "text": "let me see, what is it?"
    },
    {
      "start": 3498.43,
      "end": 3501.34,
      "text": "I think it's five, three many."
    },
    {
      "start": 3501.34,
      "end": 3502.34,
      "text": "Okay."
    },
    {
      "start": 3502.34,
      "end": 3505.5,
      "text": "I'm running off of MacBook Air."
    },
    {
      "start": 3505.5,
      "end": 3507.34,
      "text": "Right now, when I need to do anything heavy,"
    },
    {
      "start": 3507.34,
      "end": 3509.7,
      "text": "I use my desktop and Linux,"
    },
    {
      "start": 3509.7,
      "end": 3513.02,
      "text": "but I like my MacBook, but I need to upgrade my notebook"
    },
    {
      "start": 3513.02,
      "end": 3515.14,
      "text": "because I've got a MacBook Air in one"
    },
    {
      "start": 3515.14,
      "end": 3516.42,
      "text": "with eight gigs of memory,"
    },
    {
      "start": 3516.42,
      "end": 3518.5,
      "text": "and if I run that one, a three,"
    },
    {
      "start": 3518.5,
      "end": 3521.38,
      "text": "it will freezes up for a belt."
    },
    {
      "start": 3523.6,
      "end": 3527.2,
      "text": "For about one minute, so let me save that."
    },
    {
      "start": 3527.2,
      "end": 3530.68,
      "text": "Let me run, we'll undo this, too, just fun."
    },
    {
      "start": 3531.76,
      "end": 3542.69,
      "text": "Save it, that's system."
    },
    {
      "start": 3542.69,
      "end": 3544.69,
      "text": "And you can see it still floats."
    },
    {
      "start": 3544.69,
      "end": 3546.89,
      "text": "But this is running via the end of your disk."
    },
    {
      "start": 3546.89,
      "end": 3549.57,
      "text": "This is running completely on my computer."
    },
    {
      "start": 3549.57,
      "end": 3552.69,
      "text": "Open AI, nobody is gathering."
    },
    {
      "start": 3554.02,
      "end": 3555.74,
      "text": "Okay, that's cool."
    },
    {
      "start": 3555.74,
      "end": 3557.5,
      "text": "Wow, it's running slow."
    },
    {
      "start": 3558.02,
      "end": 3559.82,
      "text": "If I put one of three in there,"
    },
    {
      "start": 3559.82,
      "end": 3562.82,
      "text": "it may be like two minutes to run it."
    },
    {
      "start": 3562.82,
      "end": 3568.37,
      "text": "But I think you'll see as the course progresses there,"
    },
    {
      "start": 3568.37,
      "end": 3569.53,
      "text": "and we just got an answer."
    },
    {
      "start": 3569.53,
      "end": 3573.25,
      "text": "So absolutely, let me channel my inner entrepreneurial spirit"
    },
    {
      "start": 3573.25,
      "end": 3576.57,
      "text": "and provide you with the most inspiring options."
    },
    {
      "start": 3576.57,
      "end": 3579.49,
      "text": "Out of socks, a lot, rainbows, anchor, monochrome,"
    },
    {
      "start": 3579.49,
      "end": 3581.29,
      "text": "and not many manufacturers,"
    },
    {
      "start": 3581.29,
      "end": 3583.25,
      "text": "just kidding, let's stick to something catching up,"
    },
    {
      "start": 3583.25,
      "end": 3586.21,
      "text": "powerful, pigmented poetry partners,"
    },
    {
      "start": 3586.21,
      "end": 3587.93,
      "text": "or rainbows, right?"
    },
    {
      "start": 3587.93,
      "end": 3589.93,
      "text": "Hopefully one of these tickles your funny bones"
    },
    {
      "start": 3589.93,
      "end": 3591.85,
      "text": "sparks that business genius."
    },
    {
      "start": 3591.85,
      "end": 3594.93,
      "text": "So even with this tiny Microsoft model"
    },
    {
      "start": 3594.93,
      "end": 3597.69,
      "text": "that I'm running locally,"
    },
    {
      "start": 3597.69,
      "end": 3599.97,
      "text": "you can get some pretty decent results."
    },
    {
      "start": 3599.97,
      "end": 3602.33,
      "text": "What you can do, what's kind of cool,"
    },
    {
      "start": 3602.33,
      "end": 3604.89,
      "text": "is you can take this like this five, three many,"
    },
    {
      "start": 3604.89,
      "end": 3609.31,
      "text": "and you can add retrieval augmented generation,"
    },
    {
      "start": 3609.31,
      "end": 3611.27,
      "text": "like I was talking about RAG,"
    },
    {
      "start": 3611.27,
      "end": 3614.43,
      "text": "where we take this, we combine it"
    },
    {
      "start": 3614.43,
      "end": 3615.99,
      "text": "with some sort of a real prompt"
    },
    {
      "start": 3615.99,
      "end": 3617.99,
      "text": "and not a silly prompt here."
    },
    {
      "start": 3617.99,
      "end": 3619.59,
      "text": "We run some more code."
    },
    {
      "start": 3619.59,
      "end": 3623.25,
      "text": "We connect it to some context, that context,"
    },
    {
      "start": 3623.25,
      "end": 3625.69,
      "text": "let's just say for example, it's proprietary data."
    },
    {
      "start": 3625.69,
      "end": 3628.13,
      "text": "So we've taken, let's say we have a ton of data"
    },
    {
      "start": 3628.13,
      "end": 3631.25,
      "text": "that we don't want to leave, let leave the business."
    },
    {
      "start": 3631.25,
      "end": 3634.01,
      "text": "There's ways, do you know anything about vector databases"
    },
    {
      "start": 3634.01,
      "end": 3637.25,
      "text": "and bad things, there's ways specific"
    },
    {
      "start": 3637.25,
      "end": 3641.09,
      "text": "to our language models that we can save that data"
    },
    {
      "start": 3641.09,
      "end": 3645.88,
      "text": "and then retrieve relative pieces of that data"
    },
    {
      "start": 3645.88,
      "end": 3650.0,
      "text": "and integrate it into our LLM calls."
    },
    {
      "start": 3650.0,
      "end": 3654.26,
      "text": "And with this, it's a small model."
    },
    {
      "start": 3654.26,
      "end": 3658.98,
      "text": "This is a three billion, three billion, I think,"
    },
    {
      "start": 3658.98,
      "end": 3660.34,
      "text": "the country many."
    },
    {
      "start": 3660.34,
      "end": 3663.26,
      "text": "They've got a seven and a 14 out now."
    },
    {
      "start": 3663.26,
      "end": 3666.42,
      "text": "But if you mix this with retrieval augmented generation"
    },
    {
      "start": 3666.42,
      "end": 3668.7,
      "text": "and run it locally with Obama,"
    },
    {
      "start": 3668.7,
      "end": 3672.86,
      "text": "you can get pretty darn good results just with that"
    },
    {
      "start": 3672.86,
      "end": 3674.22,
      "text": "and with not a lot of program."
    },
    {
      "start": 3674.22,
      "end": 3680.75,
      "text": "I wrote one with this, with maybe 200 lines of code"
    },
    {
      "start": 3682.27,
      "end": 3686.27,
      "text": "at most, maybe 100 lines of code at most, I forget."
    },
    {
      "start": 3686.27,
      "end": 3688.47,
      "text": "And was getting really good results with it."
    },
    {
      "start": 3688.47,
      "end": 3689.83,
      "text": "So like I said, you'll see a lot."
    },
    {
      "start": 3689.83,
      "end": 3691.79,
      "text": "I think a lot of the questions that you may have"
    },
    {
      "start": 3691.79,
      "end": 3696.59,
      "text": "in your mind, you'll see answer to in the coming lessons"
    },
    {
      "start": 3696.59,
      "end": 3698.35,
      "text": "and you'll build on that."
    },
    {
      "start": 3698.35,
      "end": 3700.35,
      "text": "And then a few weeks they're gonna start hitting"
    },
    {
      "start": 3700.35,
      "end": 3702.35,
      "text": "really hard with the agents and things"
    },
    {
      "start": 3702.47,
      "end": 3706.65,
      "text": "that are gonna get super interesting, super complex"
    },
    {
      "start": 3706.65,
      "end": 3708.49,
      "text": "and super useful as being able to,"
    },
    {
      "start": 3708.49,
      "end": 3711.69,
      "text": "hey, I can actually build something useful."
    },
    {
      "start": 3711.69,
      "end": 3715.8,
      "text": "But right now, like the first lesson like Julian pointed out,"
    },
    {
      "start": 3715.8,
      "end": 3718.88,
      "text": "we wanted to make sure everybody was on the same page"
    },
    {
      "start": 3718.88,
      "end": 3722.8,
      "text": "and tomorrow is going to start diving into line Smith."
    },
    {
      "start": 3722.8,
      "end": 3726.08,
      "text": "But like for example, what you did with your homework,"
    },
    {
      "start": 3726.08,
      "end": 3728.28,
      "text": "if you had just posted that in the channel,"
    },
    {
      "start": 3728.28,
      "end": 3729.92,
      "text": "that asked, hey, can I get some feedback"
    },
    {
      "start": 3729.92,
      "end": 3731.4,
      "text": "out of throwing that into my system?"
    },
    {
      "start": 3731.76,
      "end": 3736.32,
      "text": "And I thought, hey man, you've done with a very minimal"
    },
    {
      "start": 3736.32,
      "end": 3738.72,
      "text": "amount of code, a very minimal amount of experience"
    },
    {
      "start": 3738.72,
      "end": 3740.8,
      "text": "and very minimal instructions."
    },
    {
      "start": 3740.8,
      "end": 3744.12,
      "text": "You're getting a good response even with three, five turbo."
    },
    {
      "start": 3744.12,
      "end": 3746.0,
      "text": "You could probably even throw something local"
    },
    {
      "start": 3746.0,
      "end": 3748.64,
      "text": "in like this five, three many."
    },
    {
      "start": 3748.64,
      "end": 3751.6,
      "text": "I'm not sure if it how well it deals with foreign languages"
    },
    {
      "start": 3751.6,
      "end": 3756.36,
      "text": "or not, but it should shift away from foreign languages"
    },
    {
      "start": 3756.36,
      "end": 3759.32,
      "text": "and ask it something else, but you saw when we moved up"
    },
    {
      "start": 3759.32,
      "end": 3763.55,
      "text": "to 40, the quality, how it increased."
    },
    {
      "start": 3763.55,
      "end": 3767.01,
      "text": "So I don't really think there was any problem"
    },
    {
      "start": 3767.01,
      "end": 3768.33,
      "text": "as much with your code."
    },
    {
      "start": 3768.33,
      "end": 3772.01,
      "text": "It was just, and like I said, I was asking what you use,"
    },
    {
      "start": 3772.01,
      "end": 3772.85,
      "text": "nothing's perfect."
    },
    {
      "start": 3772.85,
      "end": 3777.21,
      "text": "These are not deterministic machines."
    },
    {
      "start": 3777.21,
      "end": 3780.61,
      "text": "They're very indeterministic."
    },
    {
      "start": 3780.61,
      "end": 3783.89,
      "text": "So, you know, you put one thing into it one minute"
    },
    {
      "start": 3783.89,
      "end": 3785.01,
      "text": "and it gives you one response"
    },
    {
      "start": 3785.01,
      "end": 3786.81,
      "text": "and you put the same thing into it."
    },
    {
      "start": 3786.81,
      "end": 3788.49,
      "text": "You get a totally different response,"
    },
    {
      "start": 3788.49,
      "end": 3791.53,
      "text": "they can hallucinate, they can give you false info."
    },
    {
      "start": 3791.53,
      "end": 3795.49,
      "text": "They're certainly not something 100% trustable"
    },
    {
      "start": 3795.49,
      "end": 3798.53,
      "text": "that you don't want a human looking over"
    },
    {
      "start": 3798.53,
      "end": 3800.13,
      "text": "for anything important."
    },
    {
      "start": 3802.93,
      "end": 3803.89,
      "text": "Yeah."
    },
    {
      "start": 3803.89,
      "end": 3804.89,
      "text": "Cool."
    },
    {
      "start": 3804.89,
      "end": 3807.69,
      "text": "I would post it in the channel."
    },
    {
      "start": 3807.69,
      "end": 3809.25,
      "text": "Yeah, as far as your code goes,"
    },
    {
      "start": 3809.25,
      "end": 3810.73,
      "text": "you know, when I was looking over it"
    },
    {
      "start": 3810.73,
      "end": 3812.89,
      "text": "and then I took a look at it."
    },
    {
      "start": 3812.89,
      "end": 3814.45,
      "text": "It's like, well, it's a good setup."
    },
    {
      "start": 3814.45,
      "end": 3818.85,
      "text": "I like how you set up the two separate instances"
    },
    {
      "start": 3818.85,
      "end": 3821.29,
      "text": "of the chat."
    },
    {
      "start": 3821.29,
      "end": 3825.61,
      "text": "And I remember you asking a question about this invoke"
    },
    {
      "start": 3825.61,
      "end": 3831.22,
      "text": "and this is just earlier and totally bouncing just off subject."
    },
    {
      "start": 3831.22,
      "end": 3833.26,
      "text": "But back to that, that's just line chain."
    },
    {
      "start": 3833.26,
      "end": 3836.38,
      "text": "I think Julian may have mentioned it in class before"
    },
    {
      "start": 3836.38,
      "end": 3838.06,
      "text": "in the past, it was something different."
    },
    {
      "start": 3838.06,
      "end": 3840.06,
      "text": "I think it was chat, but that's just its way"
    },
    {
      "start": 3840.06,
      "end": 3843.38,
      "text": "of calling the LOM in the end."
    },
    {
      "start": 3844.34,
      "end": 3845.58,
      "text": "But I was looking at your code."
    },
    {
      "start": 3845.58,
      "end": 3847.58,
      "text": "I was like, well, this looks at pretty solid code"
    },
    {
      "start": 3847.58,
      "end": 3848.66,
      "text": "for what they've got."
    },
    {
      "start": 3848.66,
      "end": 3852.52,
      "text": "What's been, then we started looking at it."
    },
    {
      "start": 3852.52,
      "end": 3854.84,
      "text": "You know, I think you hit the nail on the head"
    },
    {
      "start": 3854.84,
      "end": 3860.22,
      "text": "and did you have any more questions"
    },
    {
      "start": 3860.22,
      "end": 3861.82,
      "text": "about anything from the class,"
    },
    {
      "start": 3861.82,
      "end": 3865.54,
      "text": "anything that's coming, anything about the homework,"
    },
    {
      "start": 3865.54,
      "end": 3870.06,
      "text": "anything I've added about tonight or just AI in general?"
    },
    {
      "start": 3872.8,
      "end": 3874.8,
      "text": "No, there's a lot of information today"
    },
    {
      "start": 3874.8,
      "end": 3879.8,
      "text": "so I need to process and maybe the next class or, yeah,"
    },
    {
      "start": 3882.54,
      "end": 3885.22,
      "text": "right now I don't have questions."
    },
    {
      "start": 3885.22,
      "end": 3889.97,
      "text": "Okay, you know, if you ever call out,"
    },
    {
      "start": 3889.97,
      "end": 3892.89,
      "text": "I'll talk to you to anybody that is in here."
    },
    {
      "start": 3892.89,
      "end": 3894.81,
      "text": "I'll always say the same thing."
    },
    {
      "start": 3894.81,
      "end": 3900.61,
      "text": "If you ever have any questions, feel free to post them in Slack."
    },
    {
      "start": 3902.05,
      "end": 3905.05,
      "text": "Feel free to, I'm gonna create a thread every week"
    },
    {
      "start": 3905.05,
      "end": 3907.53,
      "text": "like I did in the channel for homework"
    },
    {
      "start": 3907.53,
      "end": 3910.37,
      "text": "if you want to post your homework there"
    },
    {
      "start": 3910.37,
      "end": 3912.41,
      "text": "to get any feedback."
    },
    {
      "start": 3912.45,
      "end": 3916.7,
      "text": "If you've got, if you've got anything particular"
    },
    {
      "start": 3916.7,
      "end": 3917.82,
      "text": "that made me for some reason,"
    },
    {
      "start": 3917.82,
      "end": 3920.18,
      "text": "you don't want to put in the channel, you could DM me."
    },
    {
      "start": 3920.18,
      "end": 3923.5,
      "text": "I can't guarantee, I'll fast, I'll get back to it."
    },
    {
      "start": 3923.5,
      "end": 3925.38,
      "text": "I will get back to it."
    },
    {
      "start": 3925.38,
      "end": 3928.66,
      "text": "But this is something I'm doing at night for them."
    },
    {
      "start": 3928.66,
      "end": 3931.26,
      "text": "So I've gotten a regular day job and things going on."
    },
    {
      "start": 3931.26,
      "end": 3934.9,
      "text": "But yeah, I'm in this salon too."
    },
    {
      "start": 3934.9,
      "end": 3937.26,
      "text": "Like I said, I've only been really big"
    },
    {
      "start": 3937.26,
      "end": 3941.74,
      "text": "into working with ML and AI for the last year."
    },
    {
      "start": 3941.74,
      "end": 3944.82,
      "text": "So every day, I'm learning something new as well."
    },
    {
      "start": 3944.82,
      "end": 3948.66,
      "text": "So any question how they're basic or how they're advanced,"
    },
    {
      "start": 3948.66,
      "end": 3950.58,
      "text": "it would be, I'd be happy to answer it."
    },
    {
      "start": 3950.58,
      "end": 3953.62,
      "text": "If I don't know the answer, I'll go to the professor."
    },
    {
      "start": 3953.62,
      "end": 3955.66,
      "text": "We can, you know, in a small setting like this"
    },
    {
      "start": 3955.66,
      "end": 3958.3,
      "text": "or if there's a few people, we can kind of brainstorm it"
    },
    {
      "start": 3958.3,
      "end": 3960.42,
      "text": "through together if that's what everybody wants to do"
    },
    {
      "start": 3960.42,
      "end": 3961.58,
      "text": "and go on a high."
    },
    {
      "start": 3961.58,
      "end": 3964.26,
      "text": "And, you know, one thing I would suggest"
    },
    {
      "start": 3964.26,
      "end": 3967.74,
      "text": "is you're working through this."
    },
    {
      "start": 3968.66,
      "end": 3970.66,
      "text": "Do you use any sort of co-pilot"
    },
    {
      "start": 3970.66,
      "end": 3972.14,
      "text": "with your work?"
    },
    {
      "start": 3974.66,
      "end": 3977.5,
      "text": "You mean, for example, the co-pilot?"
    },
    {
      "start": 3979.59,
      "end": 3982.83,
      "text": "Ah, yeah, I always, yeah, you always co-pilot"
    },
    {
      "start": 3982.83,
      "end": 3985.67,
      "text": "in being a co-pilot, yeah."
    },
    {
      "start": 3985.67,
      "end": 3986.67,
      "text": "Yeah."
    },
    {
      "start": 3986.67,
      "end": 3987.87,
      "text": "So yeah, co-pilot's good."
    },
    {
      "start": 3987.87,
      "end": 3990.75,
      "text": "I'm assuming you put things in the L and the times"
    },
    {
      "start": 3990.75,
      "end": 3994.59,
      "text": "when you're working on codes."
    },
    {
      "start": 3994.59,
      "end": 3998.1,
      "text": "And you know, they're good."
    },
    {
      "start": 3998.1,
      "end": 4000.5,
      "text": "I'm sure as you know, never press 100%"
    },
    {
      "start": 4000.5,
      "end": 4001.5,
      "text": "there out."
    },
    {
      "start": 4001.5,
      "end": 4002.5,
      "text": "Yeah."
    },
    {
      "start": 4002.5,
      "end": 4003.5,
      "text": "Yeah, exactly."
    },
    {
      "start": 4003.5,
      "end": 4004.98,
      "text": "Like, for example, one thing I discovered,"
    },
    {
      "start": 4004.98,
      "end": 4009.26,
      "text": "the first time I messed with anything related to an L.O.L.M."
    },
    {
      "start": 4009.26,
      "end": 4011.5,
      "text": "I was doing curriculum development."
    },
    {
      "start": 4011.5,
      "end": 4014.86,
      "text": "And I think it was December 1st when GPT-4 came out,"
    },
    {
      "start": 4014.86,
      "end": 4017.82,
      "text": "my boss said a message, like, bro, look at this."
    },
    {
      "start": 4017.82,
      "end": 4019.46,
      "text": "This is really awesome."
    },
    {
      "start": 4019.46,
      "end": 4021.98,
      "text": "Start messing with it, you know, don't abuse it"
    },
    {
      "start": 4021.98,
      "end": 4025.3,
      "text": "but use it to help out while you're working."
    },
    {
      "start": 4025.3,
      "end": 4026.14,
      "text": "This is cool."
    },
    {
      "start": 4026.14,
      "end": 4029.54,
      "text": "I messed a little bit with chatbots, with my friends,"
    },
    {
      "start": 4029.54,
      "end": 4032.78,
      "text": "years of co-pilot had never seen anything like that."
    },
    {
      "start": 4032.78,
      "end": 4034.94,
      "text": "And, you know, just kind of went from there"
    },
    {
      "start": 4034.94,
      "end": 4038.42,
      "text": "and I started, it was GPT-3 that you could fine tune"
    },
    {
      "start": 4038.42,
      "end": 4041.38,
      "text": "and I was fine tuning a little bit for the company."
    },
    {
      "start": 4041.38,
      "end": 4045.14,
      "text": "And then when I saw that, hey, I was going to take my job"
    },
    {
      "start": 4045.14,
      "end": 4049.54,
      "text": "and decided to start studying AI and AI and D took my job."
    },
    {
      "start": 4049.54,
      "end": 4050.54,
      "text": "Oh, yeah."
    },
    {
      "start": 4050.54,
      "end": 4051.54,
      "text": "Yeah."
    },
    {
      "start": 4051.54,
      "end": 4053.66,
      "text": "Fortunately, I was able to pick something up"
    },
    {
      "start": 4053.66,
      "end": 4055.5,
      "text": "for that after a little bit of time."
    },
    {
      "start": 4055.5,
      "end": 4059.45,
      "text": "But, you know, any, like, for example,"
    },
    {
      "start": 4059.45,
      "end": 4063.01,
      "text": "something I discovered quick, I never had to mess with Regettes."
    },
    {
      "start": 4063.01,
      "end": 4064.77,
      "text": "Again, I'd put something into it."
    },
    {
      "start": 4064.77,
      "end": 4066.29,
      "text": "It would put out a pretty good answer"
    },
    {
      "start": 4066.29,
      "end": 4068.97,
      "text": "and, you know, maybe you'd have to tank her with it a little bit"
    },
    {
      "start": 4068.97,
      "end": 4070.53,
      "text": "but it's great with something like that."
    },
    {
      "start": 4070.53,
      "end": 4074.17,
      "text": "It's still horrible with complex problem-solving"
    },
    {
      "start": 4074.17,
      "end": 4075.93,
      "text": "and complex math the other day."
    },
    {
      "start": 4075.93,
      "end": 4079.65,
      "text": "I saw somebody, the layers of a neural network,"
    },
    {
      "start": 4079.65,
      "end": 4084.37,
      "text": "they were working in pie torch and they tried to calculate"
    },
    {
      "start": 4084.37,
      "end": 4091.48,
      "text": "the layers using GPT-4 and it put out the most ridiculous nonsense."
    },
    {
      "start": 4091.48,
      "end": 4096.24,
      "text": "You can imagine it because it couldn't do the math"
    },
    {
      "start": 4096.24,
      "end": 4098.96,
      "text": "and the winner of your algebra was all thrown off"
    },
    {
      "start": 4098.96,
      "end": 4102.8,
      "text": "and it was just throwing errors left and right in pie torch."
    },
    {
      "start": 4102.8,
      "end": 4103.8,
      "text": "Yeah."
    },
    {
      "start": 4103.8,
      "end": 4108.0,
      "text": "It's using co-pilot and use L-insport and cans."
    },
    {
      "start": 4108.0,
      "end": 4111.58,
      "text": "And like I said, I think you'll see"
    },
    {
      "start": 4111.58,
      "end": 4114.78,
      "text": "that your knowledge and expertise"
    },
    {
      "start": 4114.78,
      "end": 4118.78,
      "text": "and this flies up dramatically in the weeks to come"
    },
    {
      "start": 4118.78,
      "end": 4122.54,
      "text": "and certainly over the next 11 weeks."
    },
    {
      "start": 4122.54,
      "end": 4125.78,
      "text": "If you don't have anything else, I'm going to end up recording."
    },
    {
      "start": 4125.78,
      "end": 4128.5,
      "text": "Uh, I'm going to have to go something in."
    },
    {
      "start": 4128.5,
      "end": 4135.94,
      "text": "Yeah, I'm going to go ahead and I'm not sharing the screen anymore."
    }
  ],
  "metadata": {},
  "file_size_mb": 121.61085224151611,
  "model_used": "base",
  "timestamp": "2025-08-14 14:54:58"
}