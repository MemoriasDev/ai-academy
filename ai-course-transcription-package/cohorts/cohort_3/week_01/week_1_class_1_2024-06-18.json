{
  "source_url": "https://aitra-main.s3.us-east-2.amazonaws.com/afdp_cohort_3_recordings/week_1_class_1_2024-06-18.mp4?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA6ELKOKYDDOCGTW4H%2F20250814%2Fus-east-2%2Fs3%2Faws4_request&X-Amz-Date=20250814T184219Z&X-Amz-Expires=3600&X-Amz-Signature=2139f2597bdf38a90b4454b49d036325ac605035d1ef25e1172bce1da4a4edb8&X-Amz-SignedHeaders=host&x-amz-checksum-mode=ENABLED&x-id=GetObject",
  "duration": 4926.912,
  "language": "en",
  "processing_time": 322.7398898601532,
  "segments_count": 1090,
  "transcript_text": "[00:03 - 00:10] Let's get this chat starting. All right folks, welcome. There's a few of you still arriving in so kind of in a few minutes late\n[00:10 - 00:16] That's quite all right. Generally, I try to start as close to on the hour as possible because really we've got quite a\n[00:16 - 00:22] Like a lot of these concepts for a lot of these classes. We're going to have a lot to go over within a\n[00:23 - 00:30] Really short period of time. So with that said, I just want to say hello. I'm looking forward to working with you over the next\n[00:30 - 00:37] eight to ten weeks. My name is Julien Okano. I'm coming I'm coming in from Washington, DC, and I know that we've got\n[00:38 - 00:42] Eupholics coming in from all over the world. I'm really excited to see and\n[00:43 - 00:47] Let's see some of the projects get to learn from you get to learn from like some of them that you're building\n[00:47 - 00:52] But if you're here there is probably a great deal of\n[00:54 - 00:56] background experience and\n[00:56 - 01:01] Interests that you're bringing to the table. So a couple of ground rules that I just wanted to talk about. I am\n[01:02 - 01:07] Pretty I'm pretty much working with three different screens, but that does mean that\n[01:08 - 01:14] When it comes to actually seeing your questions. There's a couple of ways that I generally recommend that you ask questions\n[01:16 - 01:23] For starters, I'm going to have a slack channel as you can see Jonathan Brunt has already posted a blockers thread\n[01:23 - 01:32] Essentially any issue that you may be running into please feel free to ask questions there. I'm also going to set another thread up on slack.\n[01:32 - 01:34] So you have any questions\n[01:34 - 01:37] Please post there\n[01:37 - 01:44] That is the best place for me to kind of generally see the questions as they're coming\n[01:45 - 01:52] The other way that I generally request you ask questions is we're a relative we're probably going to be a relatively small group\n[01:52 - 01:53] I'm not seeing more than 20 people here\n[01:53 - 02:00] So far may go up to 30 but gets any much bigger than that will change our approach, but if you raise your hand on slack\n[02:01 - 02:07] Or on zoom. I am more than happy to kind of stop what I'm saying and just address the question you may have\n[02:07 - 02:13] That feels if you feel too shy to like come off the camera or come off of the mute. That's all right\n[02:13 - 02:16] Please feel free to ask any questions on slack\n[02:16 - 02:22] With that said, I want to start off this session at today's session with a question for you\n[02:22 - 02:29] So if you can navigate over to the slack channel for tonight. My question to you is what if any\n[02:33 - 02:35] I love AI tools\n[02:35 - 02:38] Have you already built\n[02:38 - 02:45] One of the things I like about this class is that it is very much a dynamic class a lot of the way we're going to be working is\n[02:46 - 02:50] Essentially with your capstone project in mind a lot of you have a joy that this program\n[02:50 - 02:57] Knowing that there is perhaps your company is asking you to put together a specific project your company is probably asking you\n[02:57 - 03:01] To build a specific tool for the betterment of the company itself\n[03:01 - 03:06] So if you are you have something in mind. I want to know about it\n[03:06 - 03:08] So if you go ahead and answer infrared\n[03:08 - 03:14] I want to know what you're looking to know. I want to see what kind of experience you've already had a chance to\n[03:15 - 03:23] Begin creating so that I know that what we are teaching here that what we are going over is stuff that you know is not necessarily\n[03:24 - 03:35] Too basic or perhaps not too intense for you perhaps so if you've done none if you are new to this if AI stuff is completely brilliant. That's great\n[03:35 - 03:38] We're going to make sure that you feel comfortable in that in that space\n[03:38 - 03:40] So with that said welcome\n[03:40 - 03:44] Today, we're just going to talk a little bit about the groundwork for water classes are going to look like\n[03:45 - 03:48] What are assignments are going to look like?\n[03:49 - 03:55] You're going to get to know me you're going to get to know a little bit about how we do education and teaching here at\n[03:57 - 04:02] App blue tech and we'll actually end the class having built a very simple script\n[04:02 - 04:11] Very simple open AI base script. That's why you have the open AI API key up on the Slack channel and also introduce a local\n[04:11 - 04:16] Language a large language model that you can use within your local framework\n[04:17 - 04:19] So with that said\n[04:20 - 04:28] Let's get on it. Hi, and welcome. This is yeah, generally I go for a little bit more of a longer introduction\n[04:28 - 04:33] But to be fair like with our time frame. I want to make sure that we cover everything that we need to be covered\n[04:33 - 04:40] So that's that here's our silvets and here on the right you can actually I recommend you take a moment to like read through that content first\n[04:41 - 04:44] We'll download and access some of the content together\n[04:44 - 04:46] But first I want to make sure that you're in the right place\n[04:46 - 04:49] This is why I probably have a question of hey, what have you built so far?\n[04:49 - 04:57] What are you interested in building because every so often you may have a student or someone who comes into the classes that has already built\n[04:57 - 05:05] Perhaps some level of a multi-agent system or has already gone through a lot of curriculum and as much as I like taking people's money\n[05:05 - 05:09] I make sure that I like to take people's money and affair and\n[05:09 - 05:18] And a fair and like trustful way, so I'm not gonna be have you sit here for eight weeks if you've already if you're already familiar with this concept\n[05:19 - 05:24] But yeah, a lot of what we're doing especially today and this week may seem a little bit introductory\n[05:25 - 05:29] It is going to be fast paced but starting next week and starting\n[05:30 - 05:35] the next couple of weeks we're diving right into a rag and based components\n[05:36 - 05:44] The general framework that we're going to be using is a Python package called Lang chain\n[05:44 - 05:50] And as soon as this Thursday, we're actually going to be diving into another component of Lang chains called Lang Smith\n[05:50 - 05:58] Essentially, how we're going to be accessing a lot of the data using the Lang chain framework is going to be based off of the Lang Smith\n[05:58 - 06:00] uh\n[06:00 - 06:05] Website and its own servers. I think we're actually using a couple of different components\n[06:05 - 06:10] Just so that you have access to a lot of different tools as part of the class session\n[06:10 - 06:14] Weeks three and four we talk a little bit about how to change some of the different\n[06:15 - 06:17] Components that we've built so far when I talk about\n[06:18 - 06:25] All rag what are really what I'm referring to is how and how a large language model and connect to outside documents\n[06:26 - 06:32] Learn from those outside documents and bring it into the knowledge basis that it has for a particular\n[06:33 - 06:35] Code or text-based invocation\n[06:36 - 06:44] Um we start building very simple tools or very simple agents as early as week three and four again\n[06:44 - 06:52] That's going to be depending on the pace your feedback your comments how you're feeling with the content experience so far and\n[06:52 - 06:54] the general\n[06:55 - 07:01] Build out or the general architecture for the agents. We're going to be using starts towards the latter end of the class now\n[07:01 - 07:04] I didn't say this is a 10-week course\n[07:05 - 07:09] So I do want to point out that as part of this 10-week course\n[07:09 - 07:12] You're going to be doing a lot of kind of many daily projects\n[07:13 - 07:18] So if you're someone who has quite a bit of experience or conceptually understands what AI\n[07:19 - 07:22] Or what gen AI packages can potentially do it\n[07:22 - 07:28] You've never really built something to kind of showcase that this is a this is actually a very good setup\n[07:28 - 07:34] And a very good environment for you because what we encourage you to do here is learn through building learn through troubleshooting\n[07:35 - 07:38] So as part of the projects that you'll will be assigning\n[07:38 - 07:44] Through your bloom tech portal, which you haven't had access to that yet highly recommend you reach out that ash\n[07:45 - 07:48] Who you've probably seen a little south bar and\n[07:50 - 07:52] Yeah, make sure you have access to the bloom tech\n[07:53 - 07:58] area to like turn in your assignment because that's yeah, you're going to have some homework\n[07:58 - 08:00] And so\n[08:00 - 08:04] These guided projects you'll have a little bit of time on your own to work on this\n[08:05 - 08:07] Outside of class\n[08:07 - 08:10] But the way we do our sessions here is\n[08:11 - 08:15] We spend a good chunk of Tuesdays talk about the conceptual elements\n[08:15 - 08:18] We see some easy examples of the packages we're working with\n[08:19 - 08:25] And eventually we build a simple project together in class. It's less of a code along and more of\n[08:26 - 08:32] Essential like hey, this is what this code does and here's how this builds into the package itself\n[08:33 - 08:36] And you'll be able to follow along, but it's not going to be as tedious as some\n[08:37 - 08:41] Bootcamp level code along sessions that you may have seen in the past\n[08:41 - 08:45] So just want to give you the heads up of what to expect\n[08:46 - 08:52] Each of these projects as you submit them is rated on a pass fail and a lot of ways what we're looking for is whatever you submit is\n[08:53 - 08:57] Returned with a little bit of feedback of like hey, this is really cool. Good job. Here's how you can improve it\n[08:58 - 09:04] Generally, it's hard to fail one of these projects because I assume and I'm presuming that most of you are\n[09:04 - 09:09] Fairly professional and fairly adept at this type of content. So if you've built something that you're comfortable submitting\n[09:10 - 09:15] It's fairly it's fairly likely that it's brought that it's very much likely something good\n[09:15 - 09:20] And of course, we're going to make sure that we offer you some stretch goals concepts and content that you can\n[09:21 - 09:25] Let your creativity shine and go beyond like any minimum requirements that we have\n[09:26 - 09:28] Before I keep talking\n[09:28 - 09:31] Any questions that may have come up so far\n[09:35 - 09:40] This is pretty much just the hello at the first start, but I do want to make sure\n[09:42 - 09:44] We are\n[09:44 - 09:50] Checking in a see about seven replies as to what if any type of ATI tools you have already built\n[09:50 - 09:56] Again, recommend that you submit your feedback like what I want to know is what you've built so far\n[09:56 - 09:58] I want to know what you've had a chance to\n[09:59 - 10:05] Create on your own and honestly, I see some of you hey, I've had no experience with AI\n[10:05 - 10:08] That's great like I want to encourage you to come to that's excellent\n[10:08 - 10:12] I want to hear no experience. I want you to feel comfortable learning and again\n[10:13 - 10:18] One of the components of this class is that hey if I'm going too fast if there's something that's whoa\n[10:18 - 10:21] That was way out of left field that I've never seen this before\n[10:21 - 10:23] This is also a safe space to chime in and let us know\n[10:24 - 10:26] But yeah, we want to make sure that you're\n[10:26 - 10:34] You're comfortable and helps build helps helps you build on what you're looking to build for your particular company\n[10:35 - 10:37] Okay\n[10:37 - 10:39] So with that said\n[10:39 - 10:44] The biggest project or the biggest reason you're probably here we're probably here is because you heard that hey\n[10:44 - 10:46] I had this is going to make you on\n[10:47 - 10:49] AI developer in a very\n[10:49 - 10:56] In a very expert level way or perhaps maybe your company your boss was like I want you to become an AI developer in a very expert\n[10:56 - 11:01] Professional way and I want you to be able to showcase this in a caps like with a capstone now\n[11:02 - 11:08] A lot of the feedback I've heard from both ash from your employers from you as part of\n[11:08 - 11:13] Your interest in this company is that you want to be able to build something that you can bring back to work\n[11:13 - 11:17] So this is going to be my second question that I want to post on slack\n[11:18 - 11:20] What if anything?\n[11:21 - 11:24] Do you have in mind?\n[11:24 - 11:27] For the capstone\n[11:27 - 11:30] That's okay if you don't have anything in mind maybe you'll\n[11:31 - 11:34] Find some inspiration along the way\n[11:34 - 11:37] But if you have already have something in mind, I want to know about it\n[11:37 - 11:39] And I think you've this question hasn't posed to you before\n[11:40 - 11:44] That's great. I want to make sure to hear from it as well because what we are\n[11:45 - 11:52] hoping that you get out of this class is essentially a working architecture for a multi-agent\n[11:52 - 11:53] uh\n[11:53 - 11:55] multi-agent model that essentially\n[11:56 - 11:58] Does what you set yourself up to do\n[11:58 - 12:02] So the last two weeks of the course we're building this capstone project with a\n[12:04 - 12:05] The mindset that it's going to be\n[12:05 - 12:10] Implemented directly into your company's needs or perhaps your team at the company\n[12:10 - 12:12] um\n[12:12 - 12:18] And if you know if we're not hitting the marks in our class sessions then that's something we want to be able to adjust\n[12:19 - 12:22] One of the things I like about working with Blumtech with working with Ash Jonathan\n[12:22 - 12:26] a lot of the other teams uh as part of these uh\n[12:26 - 12:33] A.I.C. Awards is that we are solid at uh pivoting when we need to pivot on the content\n[12:33 - 12:38] And we are solid at making sure to deliver exactly what it is that you might be looking for\n[12:38 - 12:40] So if we're not doing that\n[12:40 - 12:42] Well make sure to reach out to Ash and let us know\n[12:43 - 12:46] But in order to do that a few things that we want to make sure that we are addressing\n[12:46 - 12:49] We want to make sure that you're able to identify the task\n[12:49 - 12:52] Make sure to identify the problem that you're setting yourself up to build\n[12:53 - 12:54] um\n[12:54 - 12:56] That we are able to\n[12:56 - 13:00] Together design a specific solution for that problem\n[13:01 - 13:06] Uh coded together or coded on our own or even coded a very simple\n[13:06 - 13:12] Version of what we're trying to build that you can walk away from this class after 10 weeks with a\n[13:13 - 13:15] codeotype that you can potentially test\n[13:15 - 13:20] Now the idea behind all of this is not to make sure that it works after 10 weeks\n[13:20 - 13:21] That's not the expectation\n[13:21 - 13:26] I wouldn't expect that out of having to learn a package from the\n[13:26 - 13:28] You know it's like from the very first time\n[13:28 - 13:31] What we are looking for is a minimum viable product\n[13:31 - 13:34] And that's generally the best we can start off with\n[13:34 - 13:37] You're working with a team you bring the minimum viable product\n[13:37 - 13:39] You get inspiration from someone else on your team\n[13:39 - 13:42] You help build your content out from there\n[13:43 - 13:47] So let's see I'm seeing some other some some of your comments video\n[13:47 - 13:52] Video and analysis code based insights great how to correct your code excellent better task managing\n[13:53 - 13:55] Excellent. Oh\n[13:55 - 13:57] The input animations to the reads email. Yes, I think\n[13:58 - 14:04] Believe we build something like that towards the end especially when we start talking about the multi agent system\n[14:04 - 14:08] So we'll build up to that that list that is excellent\n[14:08 - 14:10] That's that's an excellent sentence\n[14:11 - 14:13] All right folks\n[14:13 - 14:16] So with that said\n[14:16 - 14:20] I do want to talk a little bit about the schedule is going to look like\n[14:20 - 14:26] This is also a little bit about how we're going to do where I'm going to do a little bit of the brief\n[14:26 - 14:28] Introductions for us today. So\n[14:30 - 14:34] Here you can see the schedule for this week and the next couple of weeks\n[14:35 - 14:43] We don't have classes two weeks from now. That is July 4th. So that entire week you are off enjoy the time enjoy the holiday\n[14:44 - 14:51] But it's also one of the things that's not noted here on this schedule is that tomorrow at least the United States\n[14:51 - 14:57] It is a major federal holiday and most of our office hours is going to be done on Wednesday\n[14:57 - 14:59] So with that I would like to introduce our\n[15:00 - 15:02] excellent\n[15:03 - 15:10] Teaching instructor what's the what's the what's the what's the what's the what's the title is it's learning learning instructor learning assistant\n[15:11 - 15:13] Teaching assistant\n[15:13 - 15:16] Yeah, if it's some play on teaching assistant learning assistants\n[15:18 - 15:23] Whatever it may be yeah, well Jonathan. Please say hello to everyone. Say hello to Jonathan\n[15:25 - 15:27] Hey, everybody. I'm John\n[15:27 - 15:31] I will be the learning assistant for this course\n[15:31 - 15:37] I'll be holding an optional office hours on Wednesdays at the same time at PM eastern\n[15:37 - 15:40] The office hours are completely optional\n[15:41 - 15:44] Anyone and everyone is encouraged to come during the office hours\n[15:44 - 15:51] We'll spend the first 10 to 20 minutes in general going over a solution to the homework that you had the previous day\n[15:51 - 15:57] And the rest of the time is for you to bring questions that you have about the content that we've been covering\n[15:57 - 16:04] And the course in general and AI in general so this week we do have a holiday\n[16:05 - 16:09] I will be holding the office hours as regular this week tomorrow\n[16:10 - 16:18] For anyone that wants to come also I understand that we have quite a few people from India and Thailand from the other side of the world in this course\n[16:19 - 16:23] Ash spoke with me earlier and we're going to set something up starting next week\n[16:23 - 16:28] We're going to have an alternate office hours. It's more conducive to that time zone\n[16:28 - 16:29] I believe\n[16:29 - 16:34] Then Ash is going to set up a poll to see what the best time would be but more on that later\n[16:35 - 16:36] but\n[16:36 - 16:38] Aside from that if you've got any questions\n[16:39 - 16:43] I try to keep up with my slack when I can so feel free to reach out to me and slack\n[16:44 - 16:48] And I'll get back with you when I can and I look forward to working with everybody\n[16:52 - 16:54] so\n[16:54 - 16:57] And as for myself my name like I said earlier is Julio and Okendo\n[16:57 - 16:58] I am\n[16:59 - 17:01] Originally a data science\n[17:01 - 17:06] Professional nowadays these days I have more refer to myself as an automation consultant\n[17:07 - 17:12] Sometimes I use the AI consultant when it comes to some of the companies I work with but I tend to work with\n[17:13 - 17:18] Individuals who are typically a lot more scared of AI. Oh my gosh AI is taking my job\n[17:18 - 17:23] So a lot of times having the term a automation consultant is a little bit more of from the interface\n[17:23 - 17:28] A lot of my work is done with nonprofits and small businesses in the Washington DC area\n[17:28 - 17:31] So I do the type of work where I walk into\n[17:33 - 17:39] I walk into a specific small business and help them automate a lot of their daily processes or some of the management\n[17:40 - 17:42] some of the management\n[17:43 - 17:44] tedious tasks\n[17:44 - 17:47] I think a lot of you have talked a little bit about like how do I\n[17:48 - 17:52] Organize my more tedious tasks into something a little bit more straightforward\n[17:52 - 17:55] So a lot of the type of work that I do even before\n[17:56 - 17:59] The big open AI chat GBT\n[18:00 - 18:05] Breakthrough of last year was essentially showing and teaching people how to make their lives\n[18:06 - 18:10] A lot more efficient a lot more succinct along with the same lines\n[18:10 - 18:13] I also teach data science as a side\n[18:14 - 18:17] Career path if so if there's anyone here who wants to talk statistics\n[18:18 - 18:25] Please by all means hit me up. This is sort of this is the sort of conversation that I I'd love to dive into\n[18:26 - 18:28] But with that being said what we're doing today\n[18:29 - 18:35] Is essentially going to be diving deep into some of our first large language models\n[18:35 - 18:40] Let me go ahead and take a brief moment to pause. I just want to make sure we're all on the same page\n[18:41 - 18:44] Now I asked you all earlier just to make sure\n[18:45 - 18:50] That we had a chance to fork and clone the specific repo is there anyone who\n[18:51 - 18:56] Hasn't had a chance to do that yet. Give me a thumbs down if you haven't had a chance to do that if that's okay\n[18:56 - 18:58] Let's go ahead and do that. I\n[18:59 - 19:02] Said yeah, yeah, like this is this is what we're here for so\n[19:03 - 19:07] I think I shared the link to that on the Slack channel. Yeah, so go ahead\n[19:08 - 19:11] I'll go ahead and share that once again\n[19:12 - 19:13] I'm\n[19:13 - 19:21] Just in case but what we're going to do and what I'm going to walk us through is simply the process of these instructions listed below the idea is\n[19:22 - 19:25] But I want to be able to make sure to create a virtual environment in my local computer\n[19:25 - 19:30] Because I'm going to be installing especially over the next few weeks a lot of different packages some of these packages\n[19:31 - 19:37] Might play well together, but some of them might not and it's certainly a good habit to make sure that all of this is all of these\n[19:38 - 19:42] Different installations are done in a virtual environment. So\n[19:43 - 19:48] With that said there's a couple of different ways you can do this if you have a preferred way to do that\n[19:48 - 19:51] Highly recommend you take that approach\n[19:51 - 19:56] I'm simply going to go over the approach that's listed on the GitHub repository\n[19:57 - 19:58] um\n[19:58 - 20:02] The grab my terminal. That's where did you go?\n[20:03 - 20:05] Problem with having of a Jillian screens\n[20:07 - 20:09] So you never know where you put things\n[20:17 - 20:19] Yeah, here we go\n[20:20 - 20:22] There get that out of the way\n[20:22 - 20:26] Let's set this up\n[20:26 - 20:29] All right, so here's what we're going to need to do\n[20:29 - 20:33] This is the first repository for our class today\n[20:34 - 20:38] But we're going to have pretty much a whole lot of other repositories down the road\n[20:38 - 20:42] So I want to make sure that you have your files relatively organized\n[20:43 - 20:44] so\n[20:44 - 20:50] Most of you should have a documents folder. I'm going to recommend that you create a directory on your local like on your local computer to\n[20:51 - 20:54] Clown and save all of these repositories for today\n[20:55 - 20:59] I'm simply going to navigate over to my documents folder\n[20:59 - 21:01] um\n[21:01 - 21:03] Inside that folder. This is where I'm going to\n[21:05 - 21:07] clone my\n[21:08 - 21:09] Oh my files um\n[21:11 - 21:14] Let's create a folder called let's say\n[21:14 - 21:16] AI dev class\n[21:18 - 21:22] And that's where I'm going to make sure that all of my content is located. So\n[21:23 - 21:25] What I'm doing right now just in case\n[21:26 - 21:28] For anyone who hasn't seen this before just\n[21:29 - 21:34] Happy to reach out like if this is not something you've seen before if you're not familiar with this stick around after class\n[21:34 - 21:39] I will talk a little bit about navigating the the command line in that case\n[21:39 - 21:43] But all I'm doing is creating a folder to you clone\n[21:44 - 21:49] All of the content that's coming from this repository and you can even see the slides located in there as well. So\n[21:50 - 21:52] I am going to\n[21:52 - 21:54] found\n[21:54 - 21:56] The repository that I've just\n[21:57 - 21:59] loaded into this file\n[22:02 - 22:04] And while I'm here\n[22:08 - 22:11] Now let's just go ahead and create the environment right inside\n[22:12 - 22:14] That's folder\n[22:14 - 22:19] So I just want to navigate into the folder and create my environment there. So\n[22:20 - 22:24] I'm presuming that pipe you've had a chance to install Python if you haven't\n[22:25 - 22:26] Okay\n[22:26 - 22:31] Stick around watch the recording get the concepts and then we'll and we'll we'll troubleshoot those questions\n[22:32 - 22:35] Towards the last 30 minutes of the session\n[22:36 - 22:41] But the idea is that I want to create a virtual environment like man for this is pretty standard\n[22:41 - 22:46] It's Python 3 or Python regular as long as you have Python 3.6\n[22:46 - 22:48] You should be good to go\n[22:48 - 22:52] I'm going to create a virtual environment. I'm just going to call this one\n[22:53 - 22:55] You can call this whatever you want. I'm going to call this one\n[22:56 - 22:58] Let's say\n[22:58 - 23:00] AI school\n[23:00 - 23:06] So Python 3.mve and v.ai school that dot\n[23:07 - 23:08] Creates this as a hidden file\n[23:11 - 23:15] It's going to be a little bit of a brief delay and I like to showcase and make sure that my files are working\n[23:15 - 23:19] By just showcasing lsa. There's AI school\n[23:19 - 23:21] Along with all of the other\n[23:21 - 23:23] content that was\n[23:23 - 23:25] brought in from that well-being\n[23:26 - 23:28] From the repo that we just formed\n[23:29 - 23:34] And what I want to make sure is I just want to make sure that this is running in a virtual environment\n[23:34 - 23:36] And we simply activate that with source\n[23:38 - 23:40] dot AI school\n[23:41 - 23:43] Then\n[23:43 - 23:44] Activate\n[23:44 - 23:46] If you're on windows. I think the command\n[23:47 - 23:50] Is scripts activate\n[23:52 - 23:57] So that's my fair warning if you're on windows. It's AI school then scripts activate\n[23:59 - 24:01] And at least on my side you can see that like\n[24:01 - 24:05] This dot AI school at the very beginning needs that I have activated those scripts\n[24:08 - 24:12] Now here's the fun part in order to get some of this code to work\n[24:13 - 24:18] And if it doesn't work for you if there's some if for whatever reason there's a path issue on your local computer\n[24:18 - 24:22] We have alternatives to get that API key to work for your computer\n[24:23 - 24:28] But I need to be able to install some of the files in this requirements folder\n[24:29 - 24:32] So let me just talk a little bit about what's in that requirements folder\n[24:34 - 24:38] I like to showcase. I just used a nano to open up the folder itself\n[24:38 - 24:42] I'm going to I want to showcase that these are the packages. I'm going to be installing it's\n[24:42 - 24:47] Langshane, Langshane Core, OpenAI, and Langshane Community\n[24:49 - 24:51] Actually missing one of the packages here\n[24:51 - 24:55] Alamo, I believe, but we'll see if we get to that towards the end of the class\n[24:56 - 25:01] But the idea is that I want to be able to install all of these requirements in one go\n[25:02 - 25:05] So I'm simply going to use a pip install\n[25:05 - 25:07] Again, if you have a different method for using this that's fine\n[25:08 - 25:11] I just want to install the files from that requirements dot the xt\n[25:12 - 25:14] In this virtual environment\n[25:19 - 25:22] See how we're doing\n[25:22 - 25:25] Again, if we've already gone past\n[25:26 - 25:29] Where you're feeling comfortable with that's okay stick around\n[25:29 - 25:34] After our lecture portion of the pass and we'll talk this through\n[25:35 - 25:40] I just want to make sure that this is one of the steps that we are comfortable with\n[25:41 - 25:44] Ah, it's always one of those things that I really need to get from that creating\n[25:46 - 25:48] All right, so\n[25:48 - 25:55] After that long line of code what I want to do is this part is kind of an optional if you know what you're doing\n[25:55 - 26:00] That's okay, but what I want to do is create a dot environment folder\n[26:00 - 26:02] That we can use to\n[26:03 - 26:05] either access\n[26:05 - 26:06] With\n[26:06 - 26:10] Directly through the pie file or we can access directly from the environment that we're working with\n[26:11 - 26:15] So I'm going to copy the dot EMV file\n[26:15 - 26:18] So this is a file folder that's all that already exists\n[26:19 - 26:21] From the files that we cloned so\n[26:22 - 26:24] Not EMV I just want to showcase\n[26:24 - 26:27] This is the open API key that we've included created earlier\n[26:29 - 26:34] So you can actually add the key to that at this point there\n[26:36 - 26:40] Or you can simply copy the structure of the of the sample\n[26:41 - 26:48] By creating a dot EMV of your again, all of this is up again of repo. I'm simply going to ending this step by step\n[26:49 - 26:53] So now we have both a dot EMV and a dot EMV sample\n[26:54 - 26:56] I want to update\n[26:56 - 27:03] That key value that we created so I'm just going to nano into that environment folder and I'm going to replace\n[27:04 - 27:06] the super secret key text\n[27:10 - 27:13] With the API key that we've just shared\n[27:16 - 27:19] Still you shared that on the slack channel\n[27:23 - 27:27] So I'm going to copy that and include that here\n[27:30 - 27:32] Just make sure that the key name\n[27:32 - 27:37] Or that the environment variable name is exactly open at AI-APA\n[27:38 - 27:42] Excuse me underscore API underscore key\n[27:43 - 27:46] And with that I'm going to save the files\n[27:48 - 27:51] And then just to be on the safest side\n[27:52 - 27:55] Honestly, this part is a little bit overkill\n[27:55 - 28:01] But if you look at the environment, I'm going to kind of embarrass myself a little bit by showing some of the\n[28:04 - 28:08] Terrible work I've done with keeping my environment in a consistent format\n[28:09 - 28:11] With some of the home room content\n[28:11 - 28:16] But one of the things I want to be able to do is I want to be able to\n[28:16 - 28:20] Export the open AI key that we've just created\n[28:20 - 28:25] So I'm showcasing all of the content that is previously in this virtual environment\n[28:25 - 28:28] I want to export\n[28:28 - 28:29] One more variable\n[28:29 - 28:32] Open AI underscore API key\n[28:35 - 28:37] And this is again just overkill\n[28:38 - 28:42] Where I simply I'm creating the API key into my local environment\n[28:44 - 28:48] So when I've heard that environment now I have it in two different locations\n[28:48 - 28:49] Again, this is simply\n[28:50 - 28:54] Being I'm simply playing it safe to be on the safe side\n[28:55 - 28:59] You can do one tool or the other you can essentially take one step or the other\n[29:00 - 29:04] But the idea behind this is that when we start running commands\n[29:07 - 29:09] If you have it to run commands in the command line\n[29:09 - 29:12] We're going to meet we can essentially easily access them\n[29:13 - 29:15] Uh\n[29:15 - 29:19] We could easily access those uh the key itself. So hopefully\n[29:22 - 29:25] Directly from the command line it should work to just use\n[29:26 - 29:29] 5 on 3 in class example and let's say\n[29:30 - 29:31] I have a file called\n[29:33 - 29:34] Uh\n[29:34 - 29:36] What's the first file called\n[29:37 - 29:40] And get simple check history\n[29:40 - 29:42] It's one of them that should work on itself\n[29:46 - 29:47] And if that file runs\n[29:47 - 29:49] Yeah\n[29:49 - 30:00] We're on the right track\n[30:00 - 30:03] I was in here just for a moment to see if any questions might have come up\n[30:05 - 30:07] It's a matter of uh struggling with the\n[30:07 - 30:08] Environment keys\n[30:08 - 30:13] That's fine\n[30:13 - 30:16] If even if you know you did everything correctly and it didn't quite work\n[30:16 - 30:18] The other tool that we can use\n[30:19 - 30:22] The other tool that I can recommend you use is that right here\n[30:22 - 30:24] While you're in this virtual environment\n[30:24 - 30:27] You can also pip install Python\n[30:28 - 30:29] Um\n[30:29 - 30:31] That ENV\n[30:31 - 30:33] Which is the other package\n[30:34 - 30:36] That you can install that you can uh import\n[30:38 - 30:40] As a tool\n[30:40 - 30:42] Into your script to access\n[30:44 - 30:47] The environments that we created\n[30:47 - 30:55] So that's another option\n[30:55 - 30:57] Any questions so far\n[30:57 - 31:00] Cool okay feeling good all right basic stuff\n[31:00 - 31:02] Keeping it basic so\n[31:02 - 31:04] Um\n[31:04 - 31:06] A little bit behind schedule but that's okay\n[31:06 - 31:10] The next bit of the components is actually pretty straightforward\n[31:10 - 31:13] He does at this point I'm getting the sense that you have uh some familiarity with this\n[31:14 - 31:16] But what we're going to be doing is\n[31:16 - 31:19] Essentially talking a little bit about large how large language models work\n[31:19 - 31:22] And in order to understand it we need to know what a large language model is\n[31:22 - 31:25] If you have a large language model is nothing more than just a\n[31:26 - 31:28] Tool that allows you to\n[31:30 - 31:35] Create content or create essentially the structure of one word based on the previous word that exist\n[31:35 - 31:37] There's a little bit of a statistical\n[31:37 - 31:40] Physical properties that are happening behind the scenes that we're going to talk about at just a point\n[31:41 - 31:46] But if you're here it's generally and probably because you are very familiar with what a large language model\n[31:47 - 31:51] Does I'd like to just for the sake of uh\n[31:52 - 31:58] Making sure that we're fairly comfortable with the concepts we are going to discuss a little bit about what Markov chains look like and what they do\n[31:59 - 32:02] But in order to understand how these large language models work\n[32:02 - 32:05] I also want to be able to address how these large language models operate\n[32:05 - 32:07] So we're going to talk a little bit about what these messages\n[32:08 - 32:11] What large language models messages look like when you're\n[32:12 - 32:16] Creating them which how you're programming them from the very beginning for talk a little bit about what\n[32:16 - 32:22] Prompting may look like specifically one zero one shot and few shot prompting may look like specifically\n[32:22 - 32:24] We're going to be addressing more of the prompting components\n[32:25 - 32:27] Later this week when we integrate them with\n[32:27 - 32:30] Mike Smith and for blank views and\n[32:30 - 32:38] Part of our major conversation is just how the prompts and the messages that we use inform our models behaviors\n[32:39 - 32:44] Finally, we're going to be able to end the class with a little bit about a little bit\n[32:44 - 32:46] I'll talk about how chat histories\n[32:47 - 32:53] Essentially help our model remember what has been written out so far\n[32:54 - 33:01] And if we have time, I'll make sure to kind of address how you can install a lot of if you haven't done so already\n[33:02 - 33:06] But it's an excellent fun tool to just have on your\n[33:07 - 33:11] On your local on your local computer if you haven't had a chance to do that all\n[33:13 - 33:14] So with that said\n[33:14 - 33:17] Let's talk a little bit about the intuition behind the map alone\n[33:18 - 33:20] And generally speaking, this is simply a\n[33:21 - 33:23] ask that has a\n[33:23 - 33:24] series of\n[33:24 - 33:30] Understanding what words existed beforehand in order to determine what the next word is going to be\n[33:30 - 33:37] So it's essentially just a token predictor it essentially books at a specific statistic one analysis based on like\n[33:37 - 33:39] simple mark off chain where we are\n[33:40 - 33:44] Determining based on probability what the next word in a sentence might look like\n[33:45 - 33:50] So this is not essentially going to be a magical tool in any way that gives you all the answers to any of the problems\n[33:50 - 33:53] We are going to be engineering these tools\n[33:53 - 33:54] To\n[33:54 - 34:02] To address what like to kind of address and improve what the next word in our chain or our\n[34:03 - 34:07] In our structured language might look like I'm going to share and showcase\n[34:09 - 34:12] This particular notebook we're just going to\n[34:12 - 34:14] Roll over to\n[34:14 - 34:16] Google call that for just a hot second\n[34:17 - 34:19] Because I\n[34:19 - 34:22] Find these mark off chains to be really exciting for a second\n[34:25 - 34:26] And\n[34:32 - 34:34] Okay, well, let's just open that up\n[34:34 - 34:47] All right, and if you aren't following along on the slides\n[34:48 - 34:50] Here's the link if this is something you might want to play on your own\n[34:57 - 35:03] And the idea behind this notebook is just to kind of showcase what how I'm not going to spend too much time on this because\n[35:03 - 35:06] This should be pretty straightforward to all of you\n[35:06 - 35:12] But this is a mark off chain class or we are just manually creating a simple mark off chain\n[35:13 - 35:15] That follows the same properties as the\n[35:17 - 35:20] As the tool we've built so far now mark off chain\n[35:20 - 35:22] named after abrade marga\n[35:23 - 35:24] essentially just\n[35:24 - 35:28] A mathematician from like the early from the\n[35:28 - 35:32] May 1800s kind of love this guy you look there look at that beard just kind of\n[35:33 - 35:38] Kind of that epic work just screams intense Russian mathematician that essentially just\n[35:39 - 35:45] Got really successful at age 19 and essentially started creating these types of tools that we're still using to get\n[35:45 - 35:49] So I don't I just like showing this picture cuz seriously look at that beard\n[35:50 - 35:56] But what a mark off chain is essentially doing is essentially a tool that looks at a particular object\n[35:56 - 35:58] As it exists and based on a probability\n[35:59 - 36:02] On the probability of that particular object. It's going to\n[36:02 - 36:05] Determine whether that object goes in one direction or the other\n[36:06 - 36:13] Essentially in a in a very simplistic term all we're doing with a mark up chain is we're assigning the probability that it's going to be\n[36:14 - 36:19] That it's either going to stay within its own value or shift into the next word\n[36:20 - 36:25] So the way it works behind the scenes just run this anyway\n[36:26 - 36:33] Is that if you have a specific set of words and that class is run first\n[36:34 - 36:36] What this mark off chain does\n[36:38 - 36:39] Is that we are going to build\n[36:40 - 36:45] Want a specific statement so if we have this statement that says this is a test of the mark off chain\n[36:45 - 36:54] What a mark off chain does is it's going to tell you what is the probability that if I have one given word that I'm going to return the next word\n[36:55 - 37:02] And in this small example we have a very structured sentence this is one sentence and the word\n[37:03 - 37:07] And the order of this word is problem like the probability that the next word\n[37:07 - 37:09] is pretty well defined\n[37:10 - 37:11] Given on\n[37:11 - 37:16] How those words were essentially structured so this is a test of the mark on chain\n[37:18 - 37:25] And if you're asking any specific mark off chain to generate text based on any particular word or any starting word\n[37:26 - 37:32] We can expect that the word is going to follow the order of how it was originally created\n[37:34 - 37:35] If you're with me so far\n[37:35 - 37:38] You've probably heard this at some point in this profession obviously\n[37:39 - 37:46] Now as any large language model user knows this is not enough data to actually give us something useful\n[37:47 - 37:52] Any large language model is based on tons and tons of data and tons and tons of information\n[37:52 - 37:56] Scrape from all over the internet depending on what type of fully working with\n[37:57 - 37:59] Open AI of course\n[37:59 - 38:03] Very well known has essentially scraped pretty much everything you could potentially scrape from the internet itself\n[38:04 - 38:07] So what I'm doing here is I'm essentially pulling from the Gutenberg\n[38:08 - 38:11] Text essentially the entire text from the Alice in Wonderland book\n[38:12 - 38:14] And I'm recreating\n[38:14 - 38:17] From that text file that same mark of chain file\n[38:18 - 38:21] And now we have a different set of tools\n[38:21 - 38:27] In this case the next word reference that may come after we work with the word Alice\n[38:28 - 38:30] Is now a little bit\n[38:30 - 38:38] More vague it's there's not just one word that always follows the word Alice in this text Alice is followed by Alice things\n[38:38 - 38:47] Started Alice after there's now a probability that the word Alice is going to be followed by any one of these potential words\n[38:48 - 38:51] And when we see that built into a larger\n[38:52 - 38:55] Generation or a gentler a larger function\n[38:55 - 38:59] We can essentially begin to create or generate language\n[38:59 - 39:01] based on the probability\n[39:01 - 39:04] Of the word Alice starting\n[39:05 - 39:09] To see what is going to occur next so here's the next text that we can expect\n[39:12 - 39:17] And so on and so forth after every word there's a probability that that word is going to be followed by a different word\n[39:18 - 39:26] This is also another example. We essentially start with the word the and we'll see another example of how those words are followed\n[39:27 - 39:35] So this is just a kind of a simple example just to go over the basics of how mark off chains work because it's important to have the context behind scenes\n[39:37 - 39:39] But with that in mind\n[39:39 - 39:46] What we are building is essentially a structure or creating the structure for our llm\n[39:46 - 39:48] Operate smoothly\n[39:48 - 39:51] But in order to do that we need to be able to give it direction\n[39:51 - 39:54] It's not just a matter of giving it a specific text\n[39:55 - 40:02] It's not just a matter of giving it all of the data and seeing where it seems where it goes because if we just allow it to operate in its own\n[40:03 - 40:07] Kind of a just randomly assigned structure like you're going to have\n[40:08 - 40:10] The type of generated text where\n[40:11 - 40:17] Doesn't necessarily make sense. Well, it's getting she generally gave herself up and I can talk at her full size by this time\n[40:17 - 40:22] We are essentially randomly assigning words to follow one word after the other\n[40:23 - 40:25] So we need to be able to give that direction\n[40:26 - 40:32] We need to be able to give it almost a specific prompt or a specific structure or a specific set of instructions\n[40:33 - 40:35] So what I want to talk a little bit now\n[40:36 - 40:41] Well, what I want to talk about here is what is the what are the existing structures\n[40:42 - 40:44] that are already built into\n[40:44 - 40:46] Python packages\n[40:46 - 40:48] because we don't really have to rebuild\n[40:49 - 40:52] probabilistic nature of open AI to\n[40:53 - 40:55] generate a\n[40:55 - 40:57] sentence that makes sense\n[40:57 - 41:03] But we want to be able to structure and build a an llm that has a specific\n[41:05 - 41:07] set of instructions for us to work with\n[41:08 - 41:09] so\n[41:09 - 41:11] Here's what I'm going to do you can look over this code\n[41:12 - 41:15] The code is actually also on your\n[41:16 - 41:18] It's also on your\n[41:19 - 41:24] On the repo we just downloaded, but I want to talk a little bit about the type of messages that we can build into\n[41:26 - 41:28] Langchains\n[41:28 - 41:30] Open AI model\n[41:30 - 41:32] to essentially\n[41:32 - 41:37] Direct how that chat or how that open AI model is going to respond\n[41:38 - 41:41] So there's a few terms that I want to refer to and we're going to have like user messages\n[41:42 - 41:44] Sometimes they're referred to as human messages\n[41:45 - 41:49] These are any text or this is the text that we are asking\n[41:50 - 41:54] Open AI specifically to invoke this is essentially what the human asks\n[41:55 - 42:00] The system message is the message that we as developers prompt the machine to respond\n[42:01 - 42:08] In other words, this is what has the most weight because we are giving\n[42:08 - 42:13] We are putting the guard rails on the open AI language or the open AI model\n[42:14 - 42:19] In other words, if we ask to build a large language model that doesn't curse\n[42:19 - 42:23] We incorporate that into the system message because that way whenever a human prompts\n[42:24 - 42:26] A specific response\n[42:26 - 42:28] We can essentially\n[42:28 - 42:30] Create a model that hey\n[42:31 - 42:34] Make sure that you respond to any of your answers in a way that's not\n[42:35 - 42:37] cursing\n[42:37 - 42:43] There's other tools we can essentially ex there's other tools like AI messages which essentially\n[42:44 - 42:53] Refer to the response from the open AI after or from the LLM after a human has essentially prompted a question\n[42:53 - 43:00] So the reason for this is that we want to be able to influence the behaviors of these packages and we essentially want to be able to\n[43:01 - 43:10] Combine both types of messages to create much more accurate or create LLMs that are much more responsive to what we want to do\n[43:11 - 43:19] So this is an example specifically from the documentation. I'm going to be working in this case in Microsoft VS Code\n[43:19 - 43:23] There's no requirement in this class as to which IDE you might be more interested in working with\n[43:24 - 43:28] You can do this at Jupyter Notebook if you feel the most comfortable you can do this on the command line\n[43:28 - 43:30] But what I'm going to be going over\n[43:31 - 43:34] Is just going over this specific code\n[43:34 - 43:43] I'm talking a little bit about how these specific messages work within the framework of our chat open AI model\n[43:44 - 43:46] I'm going to go ahead and run this within terminal\n[43:47 - 43:51] Here we go. That one's already set up. This is an environment I had set up earlier\n[43:53 - 43:55] But here's here's what we're going so\n[43:56 - 44:00] This is going to come from three of the packages these other packages. We just installed this part of our environment\n[44:02 - 44:05] You have any problem running any of these cells at any point\n[44:06 - 44:09] I'm not going to necessarily use a llama in this case\n[44:10 - 44:12] So I'm going to just comment that out for right now\n[44:13 - 44:18] But just off of chat a open AI. I'm going to be incorporating a system message and a human message\n[44:18 - 44:21] I'm just going to showcase both of these tools together\n[44:22 - 44:23] Chat open AI\n[44:23 - 44:25] I'm just going to be sticking to\n[44:26 - 44:28] 3.5 turdable model to be fair\n[44:28 - 44:31] This one's already a little bit out of date at this point if you're using that\n[44:31 - 44:34] Chat like gpt 4.0 you're more than welcome to\n[44:35 - 44:36] It's a little bit more pricey at this point\n[44:36 - 44:38] But the\n[44:38 - 44:41] I think most of chat you could be right now has moved on to\n[44:42 - 44:44] version 4.0 in both\n[44:45 - 44:48] API and in the chat\n[44:48 - 44:50] It's in the\n[44:50 - 44:52] Like the website we all access\n[44:52 - 44:55] But I'm going to be calling this to a to an object called lm\n[44:56 - 44:59] And I'm going to be calling this as a tool\n[45:01 - 45:05] That asks the question what would be a good company name for company that makes colorful sucks\n[45:07 - 45:11] So the first component of langshade is that I'm going to be building\n[45:12 - 45:15] Calling the open AI not directly from the open AI website\n[45:15 - 45:19] But from the framework that exists within langshade\n[45:19 - 45:24] I would think langshade they already have a specific class called human message\n[45:24 - 45:28] That allows you to specify what that human message is going to look like\n[45:28 - 45:30] So this is a human message that's coming from us the user\n[45:32 - 45:36] So my first step I'm simply going to invoke this particular message in a call\n[45:37 - 45:40] By the way if you're if the last time you work with\n[45:41 - 45:44] Langshade was about 10 months ago. They did change up a little bit\n[45:46 - 45:49] The tools that you use to invoke or prompt messages\n[45:49 - 45:51] So it's no longer\n[45:52 - 45:54] Chat I believe was the old\n[45:55 - 45:57] command of four\n[45:57 - 45:58] prompting messages\n[45:58 - 46:00] so\n[46:00 - 46:02] As part of the tool itself\n[46:02 - 46:08] I'm going to go ahead and run my package python in class example. This is system\n[46:08 - 46:10] versus user.py\n[46:15 - 46:20] So that's my first prompt. I've essentially just had one of those brief conversations with\n[46:21 - 46:23] With chat jbt of those guys\n[46:24 - 46:28] One quick thing to note specifically because I didn't notice that there's a few of you who'd have mentioned that you are\n[46:29 - 46:31] Hey, I'm brand new to\n[46:32 - 46:36] Open AI and I'm brand new to AI. Here's a term that I want to be familiar with\n[46:37 - 46:39] Because this will come up again down the road\n[46:40 - 46:44] And it's one of the arguments or one of the attributes that exists within chat open AI\n[46:44 - 46:48] And you can actually expect to see this at any of the models that we work with whether it's a llama and throbic\n[46:49 - 46:52] Whatever you want to use one of the concept one of the set one of the\n[46:52 - 46:55] Attributes that you should be familiar with is temperature\n[46:56 - 46:58] Temperature specifically refers to\n[46:59 - 47:02] Literally how hot or cold you want your model to run\n[47:03 - 47:05] Terrible choke. I'll never say that one again\n[47:05 - 47:11] But essentially within an argument of zero to one you can determine how\n[47:13 - 47:15] Random do you want your\n[47:16 - 47:18] Variables to look like how much\n[47:19 - 47:22] How much risk do you want it to take when it comes to\n[47:23 - 47:27] What type of terminology you want the open AI to be\n[47:28 - 47:32] sending out or or or or or reflecting so in other words\n[47:33 - 47:39] From the documentation itself if you take a look at the temperature itself. You I think the default is set to\n[47:40 - 47:46] Incorrect to me if I'm wrong. I think the fault is set to 0.07 which is actually fairly high\n[47:46 - 47:50] A temperature of 0.07 means that it's just a little bit above a median\n[47:51 - 47:53] amount where it's going to come up with\n[47:55 - 47:57] not super structured\n[47:58 - 48:00] Language\n[48:00 - 48:08] So just to sort of showcase the set as an example if I have a temperature of 0.01 and I run that same code\n[48:10 - 48:12] Gonna run this code a couple of times here\n[48:12 - 48:15] It's going to return rainbow sock.co\n[48:16 - 48:18] It's gonna return again rainbow sock.co\n[48:18 - 48:23] It's essentially saying that I want to take less risks with what type of terminology\n[48:24 - 48:29] Is sent out like the probability that the word I'm going to be showcased is going to be\n[48:30 - 48:32] Less risky\n[48:32 - 48:37] Now if I increase that temperature, let's take this up all the way to 0.95\n[48:37 - 48:39] It means that now my chat\n[48:40 - 48:41] This chat open AI\n[48:42 - 48:47] Model from GPT 3.5 is going to be a little bit more creative\n[48:49 - 48:50] Cool. This one's rainbow footwear\n[48:51 - 48:56] This next one is again rainbow footwear. Let me try this again. Say if I keep getting into rainbow sock creations\n[48:56 - 48:59] Okay, getting a little bit of differences here\n[48:59 - 49:02] So it's essentially if you want to expect different results\n[49:02 - 49:05] You increase the temperature on the model itself\n[49:06 - 49:08] So we have a little bit\n[49:09 - 49:13] So then the question ends up being I'm probably going to go a little bit over time my apologies\n[49:13 - 49:15] But the question ends up being\n[49:16 - 49:20] What are some components and this is a question for you. I do you want to hear your voices because I've been talking for about\n[49:21 - 49:23] Too long right now\n[49:23 - 49:29] What is a scenario where you might want to have a higher temperature? What kind of output would you want?\n[49:37 - 49:43] I suppose anything where you have a bias for creativity versus just you know producing like a static answer from a static reset\n[49:43 - 49:47] Yeah, perfect. Hey, have you ever written a model a novel\n[49:48 - 49:52] This is how you do it increase the temperature. I do not recommend that you write novels with chat GPT\n[49:53 - 49:58] Technically, there's you know, I'll leave the ethical part of that. I'm to you. That's not my business\n[49:58 - 50:00] So on the flip side\n[50:00 - 50:07] Why would I want my temperature to be much lower? What what kind of work might you you be doing that requires a lower temperature\n[50:09 - 50:14] Probably like the opposite like legal conformity or something like that where you're researching laws or codes\n[50:15 - 50:18] Exactly or perhaps another example is something like\n[50:19 - 50:25] SQL if you want to make sure that you're writing a sequel query you want your temperature to be a much lower scale because you want that\n[50:26 - 50:28] structure of that same taxity much more\n[50:29 - 50:31] Composed\n[50:31 - 50:37] So that's just kind of a general simple overview of like what you can expect from those simple arguments\n[50:37 - 50:42] But the other component the other attribute that you might want to keep in mind is top E that's another way to\n[50:42 - 50:49] Kind of control the probability that your your model is going to select the word based on a certain amount of probability\n[50:49 - 50:56] All in almost like a threshold as to one probability to determine which word to assign to a specific value\n[50:57 - 51:04] Now that's not quite prompting, but it is something that you need to be familiar with when it comes to prompting a lot of what we're talking about is\n[51:04 - 51:11] What is the structure that we are giving or one of the instructions that we are giving are of an AI model in this case\n[51:12 - 51:16] So in this case we started off with just the human message the human prompt\n[51:16 - 51:21] But what I want to do next is I want to add a little bit\n[51:22 - 51:28] Of added context, so I'm going to comment out this step three and I'm going to add some system text\n[51:29 - 51:32] And I've got the set to a sarcastic bond that gives helpful\n[51:33 - 51:35] advice\n[51:35 - 51:40] Sometimes I change this so that it's you know the voice of Madonna singing this like\n[51:40 - 51:46] Some to use pick a pop singer if you want to but as you can see when we add this one of the things that we do\n[51:46 - 51:48] I'm going to go ahead and kind of doubt this context\n[51:50 - 51:54] Is that the messages now includes a system message and a human message\n[51:55 - 51:57] So now I've essentially given the\n[51:57 - 52:01] Model some framework to operate behind the scenes and in this case, it's going to be a sarcastic bond\n[52:02 - 52:05] And when I run that code\n[52:05 - 52:11] The model returns some additional content. Oh wow. What are you going to do? How that's sarcastic? Whatever it says there\n[52:12 - 52:14] Again, I'm still operating on a high temperature here\n[52:16 - 52:18] But it is big now. It's giving me a sarcastic answer\n[52:19 - 52:25] So simply put when we're building our models we want to keep in mind what kind of\n[52:26 - 52:29] Specific prompts are being built into the model itself\n[52:31 - 52:36] Any questions or comments about this before I move onto the next bit\n[52:53 - 52:57] So that's pretty much what a system proper look like the other type of prompts\n[52:57 - 53:04] I'm not going to code this part too much because it's relatively straightforward and we're going to be doing a lot this a more a lot more of this off third step\n[53:05 - 53:10] But the other type of prompt that you're going to inform your model is essentially showcasing\n[53:10 - 53:17] Hey, it's essentially telling any model the more information or any type of like any other type of model the more information\n[53:17 - 53:24] You give the model the better it's trained the better type of information of the better type of output you can expect\n[53:24 - 53:30] From a model that has been provided additional prompting additional data additional language to work with\n[53:31 - 53:32] So the other\n[53:32 - 53:39] Example that you can expect to see or the some of the terminology that you can expect to hear is the difference between zero shot and view shot prompting\n[53:40 - 53:46] In other words, the zero shot prompting is any situation where you're just asking the model to operate with just\n[53:47 - 53:53] Oh with with essentially no learning background. So this zero shot is essentially saying like read a response to the following review\n[53:54 - 54:03] But you don't actually have any reviews for the model to kind of compare against a few shot type prompt here on the right essentially just says\n[54:03 - 54:05] Hey, here's a couple of different types of\n[54:08 - 54:13] Reviews that you can expect to see and here is the response that we've created in the past\n[54:13 - 54:20] So this is where that combination of system message human message system message human message that's shared it before\n[54:21 - 54:24] And so with this set of\n[54:25 - 54:29] Previous prompting we've essentially told the model behind the scenes\n[54:29 - 54:32] What it can explain like how we can expect\n[54:32 - 54:34] What kind of\n[54:34 - 54:38] Gaugard rails were proposing that the model operates with\n[54:39 - 54:41] So this responding from feedback\n[54:41 - 54:46] We're glad to hear you had a great experience and then this bottom is sorry didn't hear those product and meet your expectation\n[54:46 - 54:51] That kind of prompting allows the model to start finding a specific set of patterns to follow\n[54:52 - 54:59] So not going to spend too much time on this type of prompting quite yet because we're going to talk more about this from Thursday\n[55:00 - 55:02] Um\n[55:02 - 55:06] Just take a look and I'd rather talk about chat history. So I'm just kind of gonna\n[55:07 - 55:09] Handwave you over this next slide\n[55:09 - 55:14] But if you want in a lot of ways what's happening behind the scenes is that we are\n[55:15 - 55:20] Specifically defining within the prompt itself some additional context\n[55:20 - 55:27] So you've probably seen this when you're working with chat GPT on its own that when you create a specific set of instructions\n[55:27 - 55:32] The model operates a lot more in a lot more in a much more straightforward way\n[55:32 - 55:37] So any system prompt where you say as a technical support specialist who want to provide clear and concise instructions\n[55:38 - 55:43] To the user's problem you also want to be able to define what the unique problem is looking\n[55:43 - 55:46] So this would be what the human looks tries to respond\n[55:46 - 55:48] Might want to respond as\n[55:48 - 55:54] But you're going to want to give the system prompt a little bit of additional context behind the scenes\n[55:55 - 55:58] Another example of how you can decorate your prompt\n[55:58 - 56:02] Is essentially giving even much more structured card rails\n[56:03 - 56:10] So if you have a prompt with a schema essentially want the answers to be returned as a almost SQL schema table\n[56:11 - 56:16] If you have a table where it's defined as employee table you would want the columns to be specified as\n[56:17 - 56:24] Or creating a specific type of column that's defined with with data type of the particular column you're created\n[56:25 - 56:27] And this is going to be important when it comes to\n[56:28 - 56:30] How to how should we say if we have any type of\n[56:31 - 56:34] input automation that reads emails slack or geratacres\n[56:35 - 56:39] One of the components that you're going to want to keep in mind is how to create a specific setup prompts\n[56:39 - 56:41] That follows this general structure\n[56:43 - 56:45] Won't go into this too much for right now\n[56:46 - 56:53] Because what's the most important for our particular purposes these days is going to be how to understand chat history\n[56:54 - 56:57] Now when you ask a chat to be asked a question within that one page\n[56:57 - 56:59] It's pretty good about keeping track of\n[57:01 - 57:03] most\n[57:03 - 57:07] Some or maybe just like two or three questions behind it's pretty good at keeping track\n[57:07 - 57:09] We're keeping the memory of the specific conversation\n[57:10 - 57:13] Now that's not you know, that's still one of those things that we are continuous\n[57:15 - 57:18] I think programmers are all of the first trying to improve wherever they can like\n[57:19 - 57:22] hallucinations and things like that are still going to be a general problem\n[57:23 - 57:28] But what I'd like to talk about is how do we essentially build a smaller structure of this on our\n[57:29 - 57:31] That we know will still work\n[57:32 - 57:35] Within the model itself where in this case the model for chat you can see will\n[57:36 - 57:40] Will at least be able to collect some amount of the memory itself\n[57:40 - 57:43] And we do that using the same components that we've used here so far\n[57:44 - 57:46] If we're creating a set of messages\n[57:46 - 57:50] Or if we've created that set of messages with an human message and an AI message\n[57:50 - 57:54] We've essentially built and prompted a specific set of conversations to\n[57:55 - 57:57] For our model to operate\n[57:57 - 57:59] And that means that when we invoke that conversation\n[58:00 - 58:04] This is a pretty manual way we're going to be seeing some other tools within the lang chain framework\n[58:04 - 58:08] But I do want you to have something to start working with down the road\n[58:09 - 58:14] So this code is also um built into your\n[58:14 - 58:19] Built into the repo so it's exactly what you're seeing in the in the\n[58:20 - 58:23] In the code for simple chat history\n[58:23 - 58:26] I'm using the message in order to showcase what a conversational look like\n[58:27 - 58:30] Says the chat prompt template but I hope we actually used\n[58:31 - 58:34] Oh, no, I'm using an old one hang on. This is not going to be the second code you add\n[58:37 - 58:39] Let me make sure that's correct\n[58:40 - 58:42] It's yeah, that should still work\n[58:43 - 58:44] All right, so that\n[58:44 - 58:46] Using an outdated\n[58:46 - 58:51] All that work this afternoon Jonathan and I still el-gunned up the note the wrong note book\n[58:53 - 58:58] Awesome, okay, so I'm gonna be calling from AI message human message chat open\n[58:59 - 59:01] So I'm calling out this conversation\n[59:02 - 59:03] AI\n[59:03 - 59:05] Essentially, this is going to be a tool that\n[59:06 - 59:08] Changes the conversation from\n[59:08 - 59:10] English to French love programming\n[59:11 - 59:16] And it says what did you just say so when we have it invoke the messages itself the idea is\n[59:17 - 59:20] That it should return\n[59:20 - 59:22] The message that we've just\n[59:22 - 59:25] Respond into that message should be\n[59:26 - 59:28] printed into the conversation\n[59:28 - 59:33] Hopefully we'll be able to see that and we'll be able to create a tool that looks at the message\n[59:34 - 59:39] Throughout at least see oh, I think I knew what I think I know what I mean wrong here\n[59:40 - 59:42] So if I take my Python 3 we'll just\n[59:44 - 59:46] Write that overall\n[59:47 - 59:50] This might be messy. Yeah, I'm just gonna fix this up\n[59:51 - 59:53] You can do gonna add\n[59:55 - 59:57] content\n[59:57 - 01:00:00] here\n[01:00:00 - 01:00:02] I'm going to add\n[01:00:03 - 01:00:05] Content there\n[01:00:07 - 01:00:10] Let's give that helps this up all right, so\n[01:00:11 - 01:00:14] Well, since we're here one thing to know all of the managed data\n[01:00:14 - 01:00:16] You can still actually see our results as part of the metadata\n[01:00:16 - 01:00:21] But it also gives you a lot of additional information if that's something that's relevant to you\n[01:00:21 - 01:00:25] With chain framework, we're going to be seeing down the road the string out parser\n[01:00:25 - 01:00:27] But you can also just call\n[01:00:28 - 01:00:34] And limit the content you're pulling from the model itself with the dot content\n[01:00:35 - 01:00:36] Uh\n[01:00:36 - 01:00:38] property here, so let me say that let me try that again\n[01:00:41 - 01:00:43] See try that one more time\n[01:00:45 - 01:00:48] That's a little better\n[01:00:48 - 01:00:50] All right, so\n[01:00:50 - 01:00:51] Let's\n[01:00:51 - 01:00:55] Let me just leave this up from that one more time so that it's a little bit more\n[01:00:56 - 01:00:58] straightforward\n[01:00:58 - 01:01:06] And here we have a conversation so this was the first prompt that I shared that I asked share translate the sentence from English to French\n[01:01:07 - 01:01:10] The the prompt that I asked the AI message to\n[01:01:11 - 01:01:15] Share with us. This was something that I included was to say I adore the programation\n[01:01:16 - 01:01:21] And the response we included that we asked into a vote was what did you just say and there's the memory working\n[01:01:22 - 01:01:28] Behind the scenes and it said I said I adore the programation, which I means I love programming in French\n[01:01:29 - 01:01:34] Not kind of like that because it essentially is telling the model itself that it's operating and still looking back\n[01:01:34 - 01:01:37] That information from\n[01:01:37 - 01:01:39] Previously up from previously included\n[01:01:39 - 01:01:42] So I have the ability to prompt from this particular\n[01:01:44 - 01:01:48] From this particular point on model itself is essentially working so that it\n[01:01:50 - 01:01:57] Adds another human message the human message I'm about to include back into this specific list of\n[01:01:58 - 01:02:03] Human message AI messaging problems. So now it says can you try that in Spanish\n[01:02:05 - 01:02:08] For example, I like to show this prompt because\n[01:02:09 - 01:02:12] Once I asked to say can you try that in Spanish it understands\n[01:02:13 - 01:02:20] The previous amount of language that I created earlier which is essentially just I want to translate this sentence from English to French\n[01:02:20 - 01:02:23] That earlier sentence was I love programming\n[01:02:23 - 01:02:25] This is how the memory begins to be built\n[01:02:26 - 01:02:32] Because the otherwise the other problem you may be fit like that you might be facing if you're just calling or invoking a specific\n[01:02:34 - 01:02:40] A specific prompt from that from open AI is that you'll notice that otherwise it forgets the information quickly\n[01:02:40 - 01:02:43] So for example, let's see how far back this goes\n[01:02:45 - 01:02:50] What was my first question to you\n[01:02:52 - 01:02:53] Let's see how well it does\n[01:02:55 - 01:02:59] This is where I pose that question still keeps all the information from translating\n[01:02:59 - 01:03:01] This first\n[01:03:02 - 01:03:07] This first sentence translate the sentence from English to Spanish. I love programming. That's what I said\n[01:03:07 - 01:03:09] That was my first statement\n[01:03:10 - 01:03:13] But technically speaking my first question to it was indeed\n[01:03:14 - 01:03:18] What did you just say? So your first question was indeed. What did you just say?\n[01:03:19 - 01:03:22] So the memory behind the model itself seems to be\n[01:03:24 - 01:03:32] Working fairly well\n[01:03:32 - 01:03:37] Question there go for it. Why didn't you rap man Conta program are in an AI message?\n[01:03:37 - 01:03:42] Like you did in the previous one when you fed it back into the history say that I'm so sorry say that again\n[01:03:42 - 01:03:43] I kind of missed a little bit of that there\n[01:03:45 - 01:03:51] Why didn't you you when you feed everything back into the history you rap all the all the prompts into human messages\n[01:03:52 - 01:03:58] But you don't wrap the responses back into AI messages. Why not? Oh, so what\n[01:04:00 - 01:04:03] So why did that why did I not wrap the oh so the AI\n[01:04:04 - 01:04:05] message\n[01:04:05 - 01:04:07] Let me make sure I understand\n[01:04:07 - 01:04:12] But use it to do the right AI message. Yeah, I said a human message. Can you\n[01:04:12 - 01:04:13] Think of them?\n[01:04:13 - 01:04:15] You true\n[01:04:15 - 01:04:16] Why did I not wrap it?\n[01:04:16 - 01:04:18] Good question. Why did I not wrap it?\n[01:04:19 - 01:04:21] Probably because I forgot\n[01:04:21 - 01:04:26] This looks like kind of an error on my part of like not having wrapped it up as part of a human message component there\n[01:04:28 - 01:04:31] Let's see\n[01:04:31 - 01:04:33] But I miss there human message responses\n[01:04:34 - 01:04:37] Although I think\n[01:04:37 - 01:04:39] Let me just make sure\n[01:04:39 - 01:04:45] You've been message problem with this question to the human message. Yeah, I guess I just forgot to actually specify that I wanted it to\n[01:04:47 - 01:04:49] Yeah, and that would actually create now. You're absolutely right\n[01:04:49 - 01:04:53] I did forget to like wrap that up as an AI message like literally just forgot\n[01:04:54 - 01:04:57] Essentially and the problem that might cause down the road is that\n[01:04:58 - 01:05:03] That's actually try that out. Thank you for calling that out. I'll make sure that's fixed up for the next session\n[01:05:03 - 01:05:06] but let's try\n[01:05:06 - 01:05:09] What that might end up\n[01:05:09 - 01:05:10] Happening\n[01:05:10 - 01:05:13] Is that let's say in try that in Spanish\n[01:05:16 - 01:05:17] Do-do-do\n[01:05:17 - 01:05:19] Says human who can try that mean come to that we're gonna show\n[01:05:20 - 01:05:22] Let's say what was the last thing\n[01:05:25 - 01:05:28] You wrote\n[01:05:28 - 01:05:31] I mean\n[01:05:31 - 01:05:33] Okay, it still recognizes that it's\n[01:05:34 - 01:05:37] I think that might be a little bit of a flip, but I believe that with additional contents that\n[01:05:38 - 01:05:43] That AI message might actually start to hallucinate fairly quickly after giving enough information\n[01:05:43 - 01:05:46] So for example if I were to try what was\n[01:05:48 - 01:05:50] The second thing you wrote\n[01:05:51 - 01:05:54] See if it keeps track of that\n[01:05:54 - 01:05:56] Okay, still okay for the most part\n[01:05:57 - 01:06:04] So I don't want to say that it's not necessary, but I do want to say that it's it like it's probably it's a good practice to make sure to specify that we're possible\n[01:06:05 - 01:06:12] So to make sure I'm following Doug's point we want to wrap the messages append on 16 and 28 with the AI message like\n[01:06:13 - 01:06:15] In situation situation correct where possible and this is just something that\n[01:06:16 - 01:06:18] That we did that I that\n[01:06:18 - 01:06:25] Passion I didn't properly build into the into that while statement. It's a good call out. We'll fix that next time right Jonathan\n[01:06:27 - 01:06:29] Yeah\n[01:06:30 - 01:06:32] Good call out I was like that\n[01:06:33 - 01:06:35] In the end very much\n[01:06:35 - 01:06:37] Fairly soon. This is just like\n[01:06:38 - 01:06:42] Here's what I like about this session. This is very basic and from here\n[01:06:42 - 01:06:49] You can essentially already take this and start to implement this at some of the programming like any programming tool that you might be interested in a lot of ways\n[01:06:50 - 01:07:00] This is kind of your base level needs for a lot of different components if all your need is an easy access to chat gpt's to open a eyes chat gpt's\n[01:07:02 - 01:07:05] 3.5 model. You've got something to get started with\n[01:07:06 - 01:07:11] um and so the idea behind this is again just kind of\n[01:07:12 - 01:07:14] Understanding how\n[01:07:15 - 01:07:23] It like how easily it fits into kind of a Python framework as we move forward starting next Thursday or two days from now\n[01:07:23 - 01:07:28] A lot of how we frame and these type of messages are you know\n[01:07:28 - 01:07:34] It's not going to be good practice for us to kind of store this on a local computer or store this and kind of a temporary variable\n[01:07:35 - 01:07:42] You're probably more likely and not going to be pulling this information from an online rack or like you're going to be pulling from a specific document\n[01:07:42 - 01:07:50] This information is going to be existing on AWS the cloud langs that whatever you want to like wherever you're going to be pulling this information from you're going to expect to have a standing\n[01:07:51 - 01:07:56] But I want to make sure that you're familiar with essentially the step-by-step process that was happened across the states\n[01:07:59 - 01:08:01] So with that said the other tool that you have access to\n[01:08:02 - 01:08:05] If you're not interested in keeping together a\n[01:08:07 - 01:08:09] ongoing\n[01:08:10 - 01:08:15] An ongoing connection to open AI is that wow I am so pastime my apologies\n[01:08:15 - 01:08:22] Is that a llama is super easy to download except for the fact that it's something like six gigabytes of data\n[01:08:23 - 01:08:29] But the framework I know but the framework is fairly simple with a llama and with llama you can essentially\n[01:08:30 - 01:08:37] Create a specific tool that runs on your local computer it runs a little bit slower than most but it is another option\n[01:08:38 - 01:08:40] or\n[01:08:42 - 01:08:47] A specific chatbot if that's what you're invoking questions from a specific chatbot\n[01:08:48 - 01:08:56] But in a lot of ways the only thing that you need to keep in mind is that the structure of those system messages might vary depending on the type of model you're working with\n[01:08:57 - 01:09:04] So keep that in mind like the documentation for that in my chain is actually pretty like is somewhat standardize\n[01:09:04 - 01:09:06] But it's the sort of thing that you might want to be\n[01:09:06 - 01:09:11] Aware of as you're building as as you're building or choosing a model to work with\n[01:09:14 - 01:09:15] um so with that said\n[01:09:17 - 01:09:24] If you want to have a bit something to practice or something to build you act like with what we've covered here so far honestly\n[01:09:26 - 01:09:31] With just this kind of context and with a different model of your choice\n[01:09:32 - 01:09:38] Um, and if it's not a llama you can also use you know create an API key with something like a throbic or any of the other\n[01:09:39 - 01:09:46] Models that are existing within the language and framework you can create a specific tool of you know how models talk to each other\n[01:09:47 - 01:09:49] Which is pretty straightforward\n[01:09:49 - 01:09:56] But if it's something that you want to practice after having gone and seen these tools as we work on them together\n[01:09:57 - 01:10:04] Today you can essentially try to see how well you can keep that chat history going generally speaking like if you get it\n[01:10:05 - 01:10:09] Remember content from three or four levels in you've done pretty well\n[01:10:12 - 01:10:15] Um\n[01:10:15 - 01:10:18] That having been said I am over time\n[01:10:18 - 01:10:23] So I do not expect you to hang out any longer than you have already thank you for your patience\n[01:10:24 - 01:10:26] With that said you are three to go\n[01:10:26 - 01:10:32] But if you have questions I'm here for another 15-20 minutes and happy to answer any questions whether it's about like hey\n[01:10:32 - 01:10:38] What are we doing again or I'm sorry my computer just laid on fire the second I tried to run an environment variable\n[01:10:39 - 01:10:47] We're here for either one of those purposes, but I just want to say thank you or first aid boss great question done\n[01:10:48 - 01:10:51] Something always always being proven content\n[01:10:51 - 01:10:52] Uh\n[01:10:53 - 01:10:56] Yeah, and I'm by the way. Yeah, always call me out if you see messy count\n[01:10:57 - 01:10:59] It's gonna be uh, it's gonna be a thing\n[01:11:00 - 01:11:01] um\n[01:11:01 - 01:11:03] Yeah, if you have any questions\n[01:11:03 - 01:11:08] Feel free to stick around to otherwise have a wonderful evening if you're on the United States enjoy the holiday tomorrow\n[01:11:08 - 01:11:15] Thank you for your time\n[01:11:15 - 01:11:22] Thank you. Thank you. Thank you. Thank you. Thank you. Good night. Have a good night. Thank you. Thank you\n[01:11:25 - 01:11:26] You\n[01:11:27 - 01:11:37] You too. Thanks folks. Oh just in case there are questions. I'm gonna go ahead and pull up the syllabus here\n[01:11:49 - 01:11:50] Sorry\n[01:11:50 - 01:11:56] All right, can you go to the last um the last one? Yeah\n[01:11:57 - 01:12:02] Sorry, you know the last um how do you say it like the last slide? Yeah, yeah, exactly sorry\n[01:12:04 - 01:12:05] Um\n[01:12:05 - 01:12:10] The one of the homework. Yeah, well when when where where it says consider approaches such as summarization\n[01:12:11 - 01:12:19] Practice content context. I don't know if you already said it, but um what would be summarization and proactive context management\n[01:12:19 - 01:12:21] Yeah, let's start let\n[01:12:22 - 01:12:26] Yeah, since we stuck around let's talk about props there a little bit so\n[01:12:27 - 01:12:31] Here's our systems. Okay. Yeah, this is a great example. So\n[01:12:32 - 01:12:36] Let me just go ahead and share this code on the slack channel\n[01:12:37 - 01:12:40] extra\n[01:12:40 - 01:12:42] So extra code for prompts\n[01:12:43 - 01:12:45] So i'm going to share on the slack channel\n[01:12:47 - 01:12:50] this code\n[01:12:50 - 01:12:53] And i'm simply going to work with a couple of those steps here\n[01:12:54 - 01:12:56] You'll see that there's a little bit of things commented out\n[01:12:56 - 01:13:01] So i don't know what i would ask you to do is just create a pie file called prompts.py\n[01:13:02 - 01:13:04] and uh\n[01:13:04 - 01:13:11] What we'll do here is essentially showcase what that summarization look like so um\n[01:13:13 - 01:13:15] Okay, so\n[01:13:15 - 01:13:19] This is pretty much the standard stuff you have uh if you're if you want to use a llama\n[01:13:19 - 01:13:23] You can actually import that from along itself and using chat to be t. Here's my system message\n[01:13:24 - 01:13:27] And the system message like i said earlier. It's like i'm telling the open AI. Hey\n[01:13:28 - 01:13:30] I want you to work like a technical support specialist\n[01:13:30 - 01:13:34] I want you to predict like provide clear and concise instruction, right?\n[01:13:35 - 01:13:39] And what i want the human to respond is hey my computer's running slow freezing properly\n[01:13:39 - 01:13:41] That's what should i do response\n[01:13:41 - 01:13:42] This is what i'm going to have it in vogue\n[01:13:43 - 01:13:44] And\n[01:13:44 - 01:13:48] We actually don't need to worry about step two for right now. So i'm going to leave that commented out\n[01:13:48 - 01:13:51] So if we run this code\n[01:13:51 - 01:13:55] close that up\n[01:13:55 - 01:13:56] Close that up\n[01:13:56 - 01:13:58] Let's clear that out\n[01:13:58 - 01:14:00] So this is going to be python three prompts\n[01:14:02 - 01:14:04] In class example prompts pie\n[01:14:05 - 01:14:08] It's going to take a second and i just want to showcase what that means\n[01:14:12 - 01:14:16] So oh i forgot the included content here. Oh my gosh\n[01:14:16 - 01:14:18] I wonder what i did there so in vogue\n[01:14:19 - 01:14:24] include contents\n[01:14:24 - 01:14:26] Contents\n[01:14:26 - 01:14:28] Let's just make us not messy\n[01:14:28 - 01:14:35] Let's give that a second\n[01:14:36 - 01:14:40] All right that looks a little bit better. All right, so when i asked it for this prompt\n[01:14:41 - 01:14:45] What did it do? Dress your computer running slow freezing frequently. You can you you\n[01:14:46 - 01:14:51] You can try the following steps. Here's step one step two all of these are folded values\n[01:14:52 - 01:14:54] Step three step four step five. Okay\n[01:14:54 - 01:14:56] Cool\n[01:14:56 - 01:14:58] But let's say that i want to try something like\n[01:15:00 - 01:15:03] Hey, i don't want 10 steps here\n[01:15:04 - 01:15:06] What of the components you can do is\n[01:15:06 - 01:15:14] Essentially think about how you would return instructions and i'm like you want instructions in a much more concise manner for example\n[01:15:14 - 01:15:16] So let's say keep your\n[01:15:17 - 01:15:20] or\n[01:15:20 - 01:15:22] respond\n[01:15:22 - 01:15:24] Two questions\n[01:15:24 - 01:15:26] With only one step\n[01:15:27 - 01:15:33] So here i'm adding a little bit of added like context to the what to the system prompt itself\n[01:15:34 - 01:15:36] And it's as simple as that so\n[01:15:36 - 01:15:38] That means that if i'm going to now try\n[01:15:39 - 01:15:41] the same\n[01:15:41 - 01:15:42] question\n[01:15:42 - 01:15:47] Let's see if it works\n[01:15:47 - 01:15:52] Instead of nine steps. I essentially it's just on a trip performing a discipline up to free up space in your computer and group its performance\n[01:15:54 - 01:15:57] So i think i could be like oh sorry, no go ahead\n[01:15:58 - 01:16:04] I was thinking the idea would be like how we can tailor down our own prompt so that like there is we can actually use the\n[01:16:04 - 01:16:06] response as they input for\n[01:16:06 - 01:16:08] For sending the message to the other\n[01:16:09 - 01:16:13] Exactly so the idea exactly so the idea is like\n[01:16:14 - 01:16:22] Uh\n[01:16:22 - 01:16:28] You want to get really weird like you can just like essentially like the answer like the answer is essentially just\n[01:16:29 - 01:16:33] How do you how do you just like how do you create the response in a way that like\n[01:16:34 - 01:16:39] It's your specific needs and if it's in the if it's in the format that essentially is made to\n[01:16:40 - 01:16:44] That you need it to like respond to your to your LLM in a much more\n[01:16:45 - 01:16:48] like\n[01:16:48 - 01:16:50] in uh in a\n[01:16:51 - 01:16:57] In an AI or human message you do want to be able to like like recognize that these messages need to be\n[01:16:57 - 01:17:03] Delivered to the other AI and uh kind of in a structure that the other AI would be able to like understand it\n[01:17:05 - 01:17:11] Great question\n[01:17:11 - 01:17:16] And when you say in order to avoid the conversation going stable be like\n[01:17:17 - 01:17:19] not staying in terms of\n[01:17:21 - 01:17:28] Like hanging is just like diverging into something that it's completely different to what we were actually looking for\n[01:17:28 - 01:17:36] So it's less about so part of what you can expect and this is one of those hard things to avoid with the tools that we're using here so far\n[01:17:36 - 01:17:38] Unless you're connected to like\n[01:17:38 - 01:17:42] Um a lot of data like and especially as we go further in the classes\n[01:17:42 - 01:17:45] Becomes like it like we'll learn ways to prevent this\n[01:17:45 - 01:17:49] But if you're just kind of keeping this simple to like some of the simple\n[01:17:50 - 01:17:53] Like some of these simple steps one of the things are going to notice is that\n[01:17:54 - 01:17:58] Um, let's say that we have two LLM's talking about video games\n[01:17:58 - 01:18:01] What happens what starts to happen is that the\n[01:18:02 - 01:18:06] Conversation start repeating the set themselves the messages start a start to\n[01:18:07 - 01:18:09] Be almost repetitive\n[01:18:09 - 01:18:13] And so like when it repeats itself, it sort of like gets echoed back\n[01:18:13 - 01:18:19] To the LLM and I had an example that I was going to try to show this and like I couldn't get it to work because\n[01:18:20 - 01:18:21] Just didn't have time unfortunately\n[01:18:21 - 01:18:27] But the idea is that what you're going to start to notice is that if you try to create a conversation with these two LLMs\n[01:18:27 - 01:18:29] You could expect to see a very\n[01:18:30 - 01:18:31] Um\n[01:18:31 - 01:18:35] Yeah, like we're essentially a quest to start to repeat themselves over and over again\n[01:18:36 - 01:18:38] Unless you prompted to like stop\n[01:18:41 - 01:18:43] Doesn't work because I was working with a different library\n[01:18:44 - 01:18:46] Sorry, yeah\n[01:18:46 - 01:18:47] Excellent\n[01:18:47 - 01:18:49] No, thank you\n[01:18:49 - 01:18:52] Thank you. So I'll I'll go away then\n[01:18:52 - 01:18:55] No, thank you. You're all right. You've got me for another 10 minutes. So\n[01:18:55 - 01:18:58] You have another process for him, but otherwise have a wonderful evening\n[01:19:00 - 01:19:02] Wonder if full evening for you two guys\n[01:19:04 - 01:19:10] Like\n[01:19:10 - 01:19:13] I kind of like a general question like about the course\n[01:19:14 - 01:19:15] um\n[01:19:15 - 01:19:16] so\n[01:19:16 - 01:19:20] Like right now I'm on my personal computer and I know the capstone project like\n[01:19:20 - 01:19:25] The goal is to do something like for my company for my team. Is it okay? Like is this course\n[01:19:26 - 01:19:29] Fine if I just do the entire thing on my personal computer\n[01:19:30 - 01:19:35] Okay, cool. Oh, yeah. No, like I'm doing this off of a MacBook. Okay. Cool. Yeah\n[01:19:37 - 01:19:40] Yeah, I think the only like the generally the things you might want to be\n[01:19:42 - 01:19:44] Concern on it if you're doing this on your personal computer\n[01:19:44 - 01:19:47] And you're using sensitive data from your company that might not be\n[01:19:48 - 01:19:50] Yeah, definitely not. Yeah\n[01:19:52 - 01:20:05] Cool. Okay. Thank you. Thank you. That was cool about the history. I'd never\n[01:20:06 - 01:20:10] thought like keeping the history like that. I'd never\n[01:20:10 - 01:20:15] Because I've experimented before but it was always like a one prompt kind of thing and I didn't even know\n[01:20:16 - 01:20:18] I was doing\n[01:20:18 - 01:20:25] The few shot prompts. I just like like I gave it a few examples to base off like the generator response off of\n[01:20:26 - 01:20:32] But I was curious like how to make it more of like a historical context to if I wanted to continue asking that same questions\n[01:20:32 - 01:20:36] That was cool. Yeah. Yeah. Yeah. No, I mean and again, this is\n[01:20:37 - 01:20:40] This is just using like we're not even using all of\n[01:20:42 - 01:20:46] We're not even using the the line chain framework yet for that. This is just\n[01:20:47 - 01:20:52] Sticking with Python. Yeah, basic content. So yeah, it gets better\n[01:20:53 - 01:21:05] Cool. Yeah, sweet. And then it's tomorrow\n[01:21:06 - 01:21:07] like\n[01:21:07 - 01:21:10] I think is it just like office hours kind of deal\n[01:21:14 - 01:21:16] Yeah, tomorrow the\n[01:21:16 - 01:21:19] The office hours will be at the same time 80 Stern\n[01:21:20 - 01:21:24] And like I was saying earlier we'll go over the first 10 to 20 minutes\n[01:21:25 - 01:21:29] Solution to the homework and the rest of it will basically\n[01:21:29 - 01:21:32] The bring your questions and okay\n[01:21:33 - 01:21:38] Completely optional, but definitely encourage for anybody that then wants to come\n[01:21:40 - 01:21:43] Sweet\n[01:21:43 - 01:21:47] All right, awesome. That was it for me. Thank you guys very much\n[01:21:48 - 01:21:50] Yeah, have a good night. Yeah, you too",
  "transcript_segments": [
    {
      "start": 3.54,
      "end": 10.46,
      "text": "Let's get this chat starting. All right folks, welcome. There's a few of you still arriving in so kind of in a few minutes late"
    },
    {
      "start": 10.46,
      "end": 16.98,
      "text": "That's quite all right. Generally, I try to start as close to on the hour as possible because really we've got quite a"
    },
    {
      "start": 16.98,
      "end": 22.3,
      "text": "Like a lot of these concepts for a lot of these classes. We're going to have a lot to go over within a"
    },
    {
      "start": 23.62,
      "end": 30.26,
      "text": "Really short period of time. So with that said, I just want to say hello. I'm looking forward to working with you over the next"
    },
    {
      "start": 30.26,
      "end": 37.74,
      "text": "eight to ten weeks. My name is Julien Okano. I'm coming I'm coming in from Washington, DC, and I know that we've got"
    },
    {
      "start": 38.26,
      "end": 42.42,
      "text": "Eupholics coming in from all over the world. I'm really excited to see and"
    },
    {
      "start": 43.06,
      "end": 47.58,
      "text": "Let's see some of the projects get to learn from you get to learn from like some of them that you're building"
    },
    {
      "start": 47.9,
      "end": 52.66,
      "text": "But if you're here there is probably a great deal of"
    },
    {
      "start": 54.14,
      "end": 56.14,
      "text": "background experience and"
    },
    {
      "start": 56.14,
      "end": 61.86,
      "text": "Interests that you're bringing to the table. So a couple of ground rules that I just wanted to talk about. I am"
    },
    {
      "start": 62.98,
      "end": 67.14,
      "text": "Pretty I'm pretty much working with three different screens, but that does mean that"
    },
    {
      "start": 68.02,
      "end": 74.86,
      "text": "When it comes to actually seeing your questions. There's a couple of ways that I generally recommend that you ask questions"
    },
    {
      "start": 76.38,
      "end": 83.74,
      "text": "For starters, I'm going to have a slack channel as you can see Jonathan Brunt has already posted a blockers thread"
    },
    {
      "start": 83.86,
      "end": 92.1,
      "text": "Essentially any issue that you may be running into please feel free to ask questions there. I'm also going to set another thread up on slack."
    },
    {
      "start": 92.7,
      "end": 94.7,
      "text": "So you have any questions"
    },
    {
      "start": 94.7,
      "end": 97.37,
      "text": "Please post there"
    },
    {
      "start": 97.37,
      "end": 104.64,
      "text": "That is the best place for me to kind of generally see the questions as they're coming"
    },
    {
      "start": 105.36,
      "end": 112.16,
      "text": "The other way that I generally request you ask questions is we're a relative we're probably going to be a relatively small group"
    },
    {
      "start": 112.16,
      "end": 113.88,
      "text": "I'm not seeing more than 20 people here"
    },
    {
      "start": 113.88,
      "end": 120.52,
      "text": "So far may go up to 30 but gets any much bigger than that will change our approach, but if you raise your hand on slack"
    },
    {
      "start": 121.16,
      "end": 127.12,
      "text": "Or on zoom. I am more than happy to kind of stop what I'm saying and just address the question you may have"
    },
    {
      "start": 127.88,
      "end": 133.2,
      "text": "That feels if you feel too shy to like come off the camera or come off of the mute. That's all right"
    },
    {
      "start": 133.2,
      "end": 136.04,
      "text": "Please feel free to ask any questions on slack"
    },
    {
      "start": 136.84,
      "end": 142.6,
      "text": "With that said, I want to start off this session at today's session with a question for you"
    },
    {
      "start": 142.6,
      "end": 149.16,
      "text": "So if you can navigate over to the slack channel for tonight. My question to you is what if any"
    },
    {
      "start": 153.12,
      "end": 155.12,
      "text": "I love AI tools"
    },
    {
      "start": 155.44,
      "end": 158.05,
      "text": "Have you already built"
    },
    {
      "start": 158.05,
      "end": 165.78,
      "text": "One of the things I like about this class is that it is very much a dynamic class a lot of the way we're going to be working is"
    },
    {
      "start": 166.34,
      "end": 170.66,
      "text": "Essentially with your capstone project in mind a lot of you have a joy that this program"
    },
    {
      "start": 170.94,
      "end": 177.16,
      "text": "Knowing that there is perhaps your company is asking you to put together a specific project your company is probably asking you"
    },
    {
      "start": 177.46,
      "end": 181.58,
      "text": "To build a specific tool for the betterment of the company itself"
    },
    {
      "start": 181.78,
      "end": 186.98,
      "text": "So if you are you have something in mind. I want to know about it"
    },
    {
      "start": 186.98,
      "end": 188.78,
      "text": "So if you go ahead and answer infrared"
    },
    {
      "start": 188.78,
      "end": 194.72,
      "text": "I want to know what you're looking to know. I want to see what kind of experience you've already had a chance to"
    },
    {
      "start": 195.66,
      "end": 203.32,
      "text": "Begin creating so that I know that what we are teaching here that what we are going over is stuff that you know is not necessarily"
    },
    {
      "start": 204.32,
      "end": 215.08,
      "text": "Too basic or perhaps not too intense for you perhaps so if you've done none if you are new to this if AI stuff is completely brilliant. That's great"
    },
    {
      "start": 215.08,
      "end": 218.08,
      "text": "We're going to make sure that you feel comfortable in that in that space"
    },
    {
      "start": 218.56,
      "end": 220.56,
      "text": "So with that said welcome"
    },
    {
      "start": 220.8,
      "end": 224.96,
      "text": "Today, we're just going to talk a little bit about the groundwork for water classes are going to look like"
    },
    {
      "start": 225.64,
      "end": 228.56,
      "text": "What are assignments are going to look like?"
    },
    {
      "start": 229.28,
      "end": 235.76,
      "text": "You're going to get to know me you're going to get to know a little bit about how we do education and teaching here at"
    },
    {
      "start": 237.56,
      "end": 242.28,
      "text": "App blue tech and we'll actually end the class having built a very simple script"
    },
    {
      "start": 242.96,
      "end": 251.28,
      "text": "Very simple open AI base script. That's why you have the open AI API key up on the Slack channel and also introduce a local"
    },
    {
      "start": 251.88,
      "end": 256.64,
      "text": "Language a large language model that you can use within your local framework"
    },
    {
      "start": 257.12,
      "end": 259.12,
      "text": "So with that said"
    },
    {
      "start": 260.47,
      "end": 268.02,
      "text": "Let's get on it. Hi, and welcome. This is yeah, generally I go for a little bit more of a longer introduction"
    },
    {
      "start": 268.02,
      "end": 273.46,
      "text": "But to be fair like with our time frame. I want to make sure that we cover everything that we need to be covered"
    },
    {
      "start": 273.82,
      "end": 280.82,
      "text": "So that's that here's our silvets and here on the right you can actually I recommend you take a moment to like read through that content first"
    },
    {
      "start": 281.26,
      "end": 284.1,
      "text": "We'll download and access some of the content together"
    },
    {
      "start": 284.1,
      "end": 286.5,
      "text": "But first I want to make sure that you're in the right place"
    },
    {
      "start": 286.5,
      "end": 289.58,
      "text": "This is why I probably have a question of hey, what have you built so far?"
    },
    {
      "start": 289.86,
      "end": 297.22,
      "text": "What are you interested in building because every so often you may have a student or someone who comes into the classes that has already built"
    },
    {
      "start": 297.7,
      "end": 305.78,
      "text": "Perhaps some level of a multi-agent system or has already gone through a lot of curriculum and as much as I like taking people's money"
    },
    {
      "start": 305.78,
      "end": 309.3,
      "text": "I make sure that I like to take people's money and affair and"
    },
    {
      "start": 309.3,
      "end": 318.9,
      "text": "And a fair and like trustful way, so I'm not gonna be have you sit here for eight weeks if you've already if you're already familiar with this concept"
    },
    {
      "start": 319.14,
      "end": 324.74,
      "text": "But yeah, a lot of what we're doing especially today and this week may seem a little bit introductory"
    },
    {
      "start": 325.3,
      "end": 329.82,
      "text": "It is going to be fast paced but starting next week and starting"
    },
    {
      "start": 330.46,
      "end": 335.9,
      "text": "the next couple of weeks we're diving right into a rag and based components"
    },
    {
      "start": 336.38,
      "end": 344.14,
      "text": "The general framework that we're going to be using is a Python package called Lang chain"
    },
    {
      "start": 344.78,
      "end": 350.38,
      "text": "And as soon as this Thursday, we're actually going to be diving into another component of Lang chains called Lang Smith"
    },
    {
      "start": 350.38,
      "end": 358.14,
      "text": "Essentially, how we're going to be accessing a lot of the data using the Lang chain framework is going to be based off of the Lang Smith"
    },
    {
      "start": 358.62,
      "end": 360.72,
      "text": "uh"
    },
    {
      "start": 360.72,
      "end": 365.6,
      "text": "Website and its own servers. I think we're actually using a couple of different components"
    },
    {
      "start": 365.6,
      "end": 370.24,
      "text": "Just so that you have access to a lot of different tools as part of the class session"
    },
    {
      "start": 370.88,
      "end": 374.16,
      "text": "Weeks three and four we talk a little bit about how to change some of the different"
    },
    {
      "start": 375.28,
      "end": 377.68,
      "text": "Components that we've built so far when I talk about"
    },
    {
      "start": 378.24,
      "end": 385.92,
      "text": "All rag what are really what I'm referring to is how and how a large language model and connect to outside documents"
    },
    {
      "start": 386.4,
      "end": 392.64,
      "text": "Learn from those outside documents and bring it into the knowledge basis that it has for a particular"
    },
    {
      "start": 393.2,
      "end": 395.2,
      "text": "Code or text-based invocation"
    },
    {
      "start": 396.0,
      "end": 404.24,
      "text": "Um we start building very simple tools or very simple agents as early as week three and four again"
    },
    {
      "start": 404.24,
      "end": 412.16,
      "text": "That's going to be depending on the pace your feedback your comments how you're feeling with the content experience so far and"
    },
    {
      "start": 412.64,
      "end": 414.64,
      "text": "the general"
    },
    {
      "start": 415.76,
      "end": 421.52,
      "text": "Build out or the general architecture for the agents. We're going to be using starts towards the latter end of the class now"
    },
    {
      "start": 421.84,
      "end": 424.56,
      "text": "I didn't say this is a 10-week course"
    },
    {
      "start": 425.12,
      "end": 429.28,
      "text": "So I do want to point out that as part of this 10-week course"
    },
    {
      "start": 429.6,
      "end": 432.8,
      "text": "You're going to be doing a lot of kind of many daily projects"
    },
    {
      "start": 433.2,
      "end": 438.24,
      "text": "So if you're someone who has quite a bit of experience or conceptually understands what AI"
    },
    {
      "start": 439.2,
      "end": 442.08,
      "text": "Or what gen AI packages can potentially do it"
    },
    {
      "start": 442.08,
      "end": 448.16,
      "text": "You've never really built something to kind of showcase that this is a this is actually a very good setup"
    },
    {
      "start": 448.16,
      "end": 454.8,
      "text": "And a very good environment for you because what we encourage you to do here is learn through building learn through troubleshooting"
    },
    {
      "start": 455.28,
      "end": 458.24,
      "text": "So as part of the projects that you'll will be assigning"
    },
    {
      "start": 458.88,
      "end": 464.72,
      "text": "Through your bloom tech portal, which you haven't had access to that yet highly recommend you reach out that ash"
    },
    {
      "start": 465.36,
      "end": 468.56,
      "text": "Who you've probably seen a little south bar and"
    },
    {
      "start": 470.16,
      "end": 472.64,
      "text": "Yeah, make sure you have access to the bloom tech"
    },
    {
      "start": 473.36,
      "end": 478.32,
      "text": "area to like turn in your assignment because that's yeah, you're going to have some homework"
    },
    {
      "start": 478.96,
      "end": 480.4,
      "text": "And so"
    },
    {
      "start": 480.4,
      "end": 484.4,
      "text": "These guided projects you'll have a little bit of time on your own to work on this"
    },
    {
      "start": 485.76,
      "end": 487.76,
      "text": "Outside of class"
    },
    {
      "start": 487.76,
      "end": 490.96,
      "text": "But the way we do our sessions here is"
    },
    {
      "start": 491.76,
      "end": 495.36,
      "text": "We spend a good chunk of Tuesdays talk about the conceptual elements"
    },
    {
      "start": 495.36,
      "end": 498.48,
      "text": "We see some easy examples of the packages we're working with"
    },
    {
      "start": 499.12,
      "end": 505.36,
      "text": "And eventually we build a simple project together in class. It's less of a code along and more of"
    },
    {
      "start": 506.32,
      "end": 512.64,
      "text": "Essential like hey, this is what this code does and here's how this builds into the package itself"
    },
    {
      "start": 513.2,
      "end": 516.24,
      "text": "And you'll be able to follow along, but it's not going to be as tedious as some"
    },
    {
      "start": 517.2,
      "end": 521.36,
      "text": "Bootcamp level code along sessions that you may have seen in the past"
    },
    {
      "start": 521.92,
      "end": 525.28,
      "text": "So just want to give you the heads up of what to expect"
    },
    {
      "start": 526.0,
      "end": 532.24,
      "text": "Each of these projects as you submit them is rated on a pass fail and a lot of ways what we're looking for is whatever you submit is"
    },
    {
      "start": 533.12,
      "end": 537.84,
      "text": "Returned with a little bit of feedback of like hey, this is really cool. Good job. Here's how you can improve it"
    },
    {
      "start": 538.8,
      "end": 544.24,
      "text": "Generally, it's hard to fail one of these projects because I assume and I'm presuming that most of you are"
    },
    {
      "start": 544.72,
      "end": 549.84,
      "text": "Fairly professional and fairly adept at this type of content. So if you've built something that you're comfortable submitting"
    },
    {
      "start": 550.48,
      "end": 555.28,
      "text": "It's fairly it's fairly likely that it's brought that it's very much likely something good"
    },
    {
      "start": 555.92,
      "end": 560.24,
      "text": "And of course, we're going to make sure that we offer you some stretch goals concepts and content that you can"
    },
    {
      "start": 561.04,
      "end": 565.12,
      "text": "Let your creativity shine and go beyond like any minimum requirements that we have"
    },
    {
      "start": 566.4,
      "end": 568.78,
      "text": "Before I keep talking"
    },
    {
      "start": 568.94,
      "end": 571.66,
      "text": "Any questions that may have come up so far"
    },
    {
      "start": 575.31,
      "end": 580.11,
      "text": "This is pretty much just the hello at the first start, but I do want to make sure"
    },
    {
      "start": 582.91,
      "end": 584.28,
      "text": "We are"
    },
    {
      "start": 584.28,
      "end": 590.04,
      "text": "Checking in a see about seven replies as to what if any type of ATI tools you have already built"
    },
    {
      "start": 590.68,
      "end": 596.36,
      "text": "Again, recommend that you submit your feedback like what I want to know is what you've built so far"
    },
    {
      "start": 596.44,
      "end": 598.92,
      "text": "I want to know what you've had a chance to"
    },
    {
      "start": 599.56,
      "end": 605.4,
      "text": "Create on your own and honestly, I see some of you hey, I've had no experience with AI"
    },
    {
      "start": 605.48,
      "end": 608.84,
      "text": "That's great like I want to encourage you to come to that's excellent"
    },
    {
      "start": 608.84,
      "end": 612.76,
      "text": "I want to hear no experience. I want you to feel comfortable learning and again"
    },
    {
      "start": 613.4,
      "end": 618.28,
      "text": "One of the components of this class is that hey if I'm going too fast if there's something that's whoa"
    },
    {
      "start": 618.28,
      "end": 621.08,
      "text": "That was way out of left field that I've never seen this before"
    },
    {
      "start": 621.4,
      "end": 623.88,
      "text": "This is also a safe space to chime in and let us know"
    },
    {
      "start": 624.68,
      "end": 626.52,
      "text": "But yeah, we want to make sure that you're"
    },
    {
      "start": 626.68,
      "end": 634.2,
      "text": "You're comfortable and helps build helps helps you build on what you're looking to build for your particular company"
    },
    {
      "start": 635.66,
      "end": 637.08,
      "text": "Okay"
    },
    {
      "start": 637.08,
      "end": 639.08,
      "text": "So with that said"
    },
    {
      "start": 639.08,
      "end": 644.36,
      "text": "The biggest project or the biggest reason you're probably here we're probably here is because you heard that hey"
    },
    {
      "start": 644.52,
      "end": 646.52,
      "text": "I had this is going to make you on"
    },
    {
      "start": 647.0,
      "end": 649.0,
      "text": "AI developer in a very"
    },
    {
      "start": 649.0,
      "end": 656.44,
      "text": "In a very expert level way or perhaps maybe your company your boss was like I want you to become an AI developer in a very expert"
    },
    {
      "start": 656.68,
      "end": 661.88,
      "text": "Professional way and I want you to be able to showcase this in a caps like with a capstone now"
    },
    {
      "start": 662.52,
      "end": 668.04,
      "text": "A lot of the feedback I've heard from both ash from your employers from you as part of"
    },
    {
      "start": 668.84,
      "end": 673.4,
      "text": "Your interest in this company is that you want to be able to build something that you can bring back to work"
    },
    {
      "start": 673.96,
      "end": 677.48,
      "text": "So this is going to be my second question that I want to post on slack"
    },
    {
      "start": 678.83,
      "end": 680.83,
      "text": "What if anything?"
    },
    {
      "start": 681.76,
      "end": 684.73,
      "text": "Do you have in mind?"
    },
    {
      "start": 684.81,
      "end": 687.66,
      "text": "For the capstone"
    },
    {
      "start": 687.66,
      "end": 690.86,
      "text": "That's okay if you don't have anything in mind maybe you'll"
    },
    {
      "start": 691.9,
      "end": 694.14,
      "text": "Find some inspiration along the way"
    },
    {
      "start": 694.14,
      "end": 697.02,
      "text": "But if you have already have something in mind, I want to know about it"
    },
    {
      "start": 697.02,
      "end": 699.5,
      "text": "And I think you've this question hasn't posed to you before"
    },
    {
      "start": 700.14,
      "end": 704.38,
      "text": "That's great. I want to make sure to hear from it as well because what we are"
    },
    {
      "start": 705.02,
      "end": 712.06,
      "text": "hoping that you get out of this class is essentially a working architecture for a multi-agent"
    },
    {
      "start": 712.7,
      "end": 713.58,
      "text": "uh"
    },
    {
      "start": 713.58,
      "end": 715.5,
      "text": "multi-agent model that essentially"
    },
    {
      "start": 716.22,
      "end": 718.22,
      "text": "Does what you set yourself up to do"
    },
    {
      "start": 718.7,
      "end": 722.78,
      "text": "So the last two weeks of the course we're building this capstone project with a"
    },
    {
      "start": 724.14,
      "end": 725.98,
      "text": "The mindset that it's going to be"
    },
    {
      "start": 725.98,
      "end": 730.3,
      "text": "Implemented directly into your company's needs or perhaps your team at the company"
    },
    {
      "start": 730.86,
      "end": 732.06,
      "text": "um"
    },
    {
      "start": 732.06,
      "end": 738.94,
      "text": "And if you know if we're not hitting the marks in our class sessions then that's something we want to be able to adjust"
    },
    {
      "start": 739.02,
      "end": 742.7,
      "text": "One of the things I like about working with Blumtech with working with Ash Jonathan"
    },
    {
      "start": 742.7,
      "end": 746.54,
      "text": "a lot of the other teams uh as part of these uh"
    },
    {
      "start": 746.54,
      "end": 753.02,
      "text": "A.I.C. Awards is that we are solid at uh pivoting when we need to pivot on the content"
    },
    {
      "start": 753.02,
      "end": 758.54,
      "text": "And we are solid at making sure to deliver exactly what it is that you might be looking for"
    },
    {
      "start": 758.54,
      "end": 760.14,
      "text": "So if we're not doing that"
    },
    {
      "start": 760.14,
      "end": 762.78,
      "text": "Well make sure to reach out to Ash and let us know"
    },
    {
      "start": 763.26,
      "end": 766.78,
      "text": "But in order to do that a few things that we want to make sure that we are addressing"
    },
    {
      "start": 766.86,
      "end": 769.02,
      "text": "We want to make sure that you're able to identify the task"
    },
    {
      "start": 769.58,
      "end": 772.86,
      "text": "Make sure to identify the problem that you're setting yourself up to build"
    },
    {
      "start": 773.5,
      "end": 774.7,
      "text": "um"
    },
    {
      "start": 774.7,
      "end": 776.62,
      "text": "That we are able to"
    },
    {
      "start": 776.62,
      "end": 780.86,
      "text": "Together design a specific solution for that problem"
    },
    {
      "start": 781.34,
      "end": 786.46,
      "text": "Uh coded together or coded on our own or even coded a very simple"
    },
    {
      "start": 786.46,
      "end": 792.22,
      "text": "Version of what we're trying to build that you can walk away from this class after 10 weeks with a"
    },
    {
      "start": 793.18,
      "end": 795.42,
      "text": "codeotype that you can potentially test"
    },
    {
      "start": 795.42,
      "end": 800.14,
      "text": "Now the idea behind all of this is not to make sure that it works after 10 weeks"
    },
    {
      "start": 800.14,
      "end": 801.74,
      "text": "That's not the expectation"
    },
    {
      "start": 801.74,
      "end": 806.22,
      "text": "I wouldn't expect that out of having to learn a package from the"
    },
    {
      "start": 806.78,
      "end": 808.94,
      "text": "You know it's like from the very first time"
    },
    {
      "start": 808.94,
      "end": 811.42,
      "text": "What we are looking for is a minimum viable product"
    },
    {
      "start": 811.98,
      "end": 814.86,
      "text": "And that's generally the best we can start off with"
    },
    {
      "start": 814.86,
      "end": 817.5,
      "text": "You're working with a team you bring the minimum viable product"
    },
    {
      "start": 817.5,
      "end": 819.98,
      "text": "You get inspiration from someone else on your team"
    },
    {
      "start": 819.98,
      "end": 822.46,
      "text": "You help build your content out from there"
    },
    {
      "start": 823.42,
      "end": 827.02,
      "text": "So let's see I'm seeing some other some some of your comments video"
    },
    {
      "start": 827.02,
      "end": 832.62,
      "text": "Video and analysis code based insights great how to correct your code excellent better task managing"
    },
    {
      "start": 833.18,
      "end": 835.18,
      "text": "Excellent. Oh"
    },
    {
      "start": 835.18,
      "end": 837.66,
      "text": "The input animations to the reads email. Yes, I think"
    },
    {
      "start": 838.3,
      "end": 844.3,
      "text": "Believe we build something like that towards the end especially when we start talking about the multi agent system"
    },
    {
      "start": 844.3,
      "end": 848.06,
      "text": "So we'll build up to that that list that is excellent"
    },
    {
      "start": 848.62,
      "end": 850.62,
      "text": "That's that's an excellent sentence"
    },
    {
      "start": 851.82,
      "end": 853.92,
      "text": "All right folks"
    },
    {
      "start": 853.92,
      "end": 856.16,
      "text": "So with that said"
    },
    {
      "start": 856.16,
      "end": 860.8,
      "text": "I do want to talk a little bit about the schedule is going to look like"
    },
    {
      "start": 860.8,
      "end": 866.0,
      "text": "This is also a little bit about how we're going to do where I'm going to do a little bit of the brief"
    },
    {
      "start": 866.64,
      "end": 868.64,
      "text": "Introductions for us today. So"
    },
    {
      "start": 870.16,
      "end": 874.08,
      "text": "Here you can see the schedule for this week and the next couple of weeks"
    },
    {
      "start": 875.28,
      "end": 883.04,
      "text": "We don't have classes two weeks from now. That is July 4th. So that entire week you are off enjoy the time enjoy the holiday"
    },
    {
      "start": 884.24,
      "end": 891.52,
      "text": "But it's also one of the things that's not noted here on this schedule is that tomorrow at least the United States"
    },
    {
      "start": 891.52,
      "end": 897.04,
      "text": "It is a major federal holiday and most of our office hours is going to be done on Wednesday"
    },
    {
      "start": 897.04,
      "end": 899.92,
      "text": "So with that I would like to introduce our"
    },
    {
      "start": 900.96,
      "end": 902.96,
      "text": "excellent"
    },
    {
      "start": 903.76,
      "end": 910.96,
      "text": "Teaching instructor what's the what's the what's the what's the what's the what's the title is it's learning learning instructor learning assistant"
    },
    {
      "start": 911.28,
      "end": 913.28,
      "text": "Teaching assistant"
    },
    {
      "start": 913.28,
      "end": 916.56,
      "text": "Yeah, if it's some play on teaching assistant learning assistants"
    },
    {
      "start": 918.4,
      "end": 923.36,
      "text": "Whatever it may be yeah, well Jonathan. Please say hello to everyone. Say hello to Jonathan"
    },
    {
      "start": 925.32,
      "end": 927.32,
      "text": "Hey, everybody. I'm John"
    },
    {
      "start": 927.56,
      "end": 931.0,
      "text": "I will be the learning assistant for this course"
    },
    {
      "start": 931.48,
      "end": 937.32,
      "text": "I'll be holding an optional office hours on Wednesdays at the same time at PM eastern"
    },
    {
      "start": 937.88,
      "end": 940.36,
      "text": "The office hours are completely optional"
    },
    {
      "start": 941.23,
      "end": 944.91,
      "text": "Anyone and everyone is encouraged to come during the office hours"
    },
    {
      "start": 944.91,
      "end": 951.39,
      "text": "We'll spend the first 10 to 20 minutes in general going over a solution to the homework that you had the previous day"
    },
    {
      "start": 951.95,
      "end": 957.07,
      "text": "And the rest of the time is for you to bring questions that you have about the content that we've been covering"
    },
    {
      "start": 957.79,
      "end": 964.43,
      "text": "And the course in general and AI in general so this week we do have a holiday"
    },
    {
      "start": 965.31,
      "end": 969.39,
      "text": "I will be holding the office hours as regular this week tomorrow"
    },
    {
      "start": 970.19,
      "end": 978.67,
      "text": "For anyone that wants to come also I understand that we have quite a few people from India and Thailand from the other side of the world in this course"
    },
    {
      "start": 979.39,
      "end": 983.63,
      "text": "Ash spoke with me earlier and we're going to set something up starting next week"
    },
    {
      "start": 983.63,
      "end": 988.35,
      "text": "We're going to have an alternate office hours. It's more conducive to that time zone"
    },
    {
      "start": 988.51,
      "end": 989.79,
      "text": "I believe"
    },
    {
      "start": 989.79,
      "end": 994.35,
      "text": "Then Ash is going to set up a poll to see what the best time would be but more on that later"
    },
    {
      "start": 995.32,
      "end": 996.65,
      "text": "but"
    },
    {
      "start": 996.65,
      "end": 998.65,
      "text": "Aside from that if you've got any questions"
    },
    {
      "start": 999.53,
      "end": 1003.85,
      "text": "I try to keep up with my slack when I can so feel free to reach out to me and slack"
    },
    {
      "start": 1004.41,
      "end": 1008.33,
      "text": "And I'll get back with you when I can and I look forward to working with everybody"
    },
    {
      "start": 1012.75,
      "end": 1014.46,
      "text": "so"
    },
    {
      "start": 1014.46,
      "end": 1017.26,
      "text": "And as for myself my name like I said earlier is Julio and Okendo"
    },
    {
      "start": 1017.26,
      "end": 1018.06,
      "text": "I am"
    },
    {
      "start": 1019.18,
      "end": 1021.18,
      "text": "Originally a data science"
    },
    {
      "start": 1021.74,
      "end": 1026.54,
      "text": "Professional nowadays these days I have more refer to myself as an automation consultant"
    },
    {
      "start": 1027.18,
      "end": 1032.38,
      "text": "Sometimes I use the AI consultant when it comes to some of the companies I work with but I tend to work with"
    },
    {
      "start": 1033.1,
      "end": 1038.06,
      "text": "Individuals who are typically a lot more scared of AI. Oh my gosh AI is taking my job"
    },
    {
      "start": 1038.06,
      "end": 1043.58,
      "text": "So a lot of times having the term a automation consultant is a little bit more of from the interface"
    },
    {
      "start": 1043.58,
      "end": 1048.38,
      "text": "A lot of my work is done with nonprofits and small businesses in the Washington DC area"
    },
    {
      "start": 1048.38,
      "end": 1051.98,
      "text": "So I do the type of work where I walk into"
    },
    {
      "start": 1053.18,
      "end": 1059.74,
      "text": "I walk into a specific small business and help them automate a lot of their daily processes or some of the management"
    },
    {
      "start": 1060.78,
      "end": 1062.78,
      "text": "some of the management"
    },
    {
      "start": 1063.26,
      "end": 1064.3,
      "text": "tedious tasks"
    },
    {
      "start": 1064.3,
      "end": 1067.58,
      "text": "I think a lot of you have talked a little bit about like how do I"
    },
    {
      "start": 1068.14,
      "end": 1072.46,
      "text": "Organize my more tedious tasks into something a little bit more straightforward"
    },
    {
      "start": 1072.46,
      "end": 1075.5,
      "text": "So a lot of the type of work that I do even before"
    },
    {
      "start": 1076.46,
      "end": 1079.42,
      "text": "The big open AI chat GBT"
    },
    {
      "start": 1080.62,
      "end": 1085.82,
      "text": "Breakthrough of last year was essentially showing and teaching people how to make their lives"
    },
    {
      "start": 1086.3,
      "end": 1090.54,
      "text": "A lot more efficient a lot more succinct along with the same lines"
    },
    {
      "start": 1090.54,
      "end": 1093.42,
      "text": "I also teach data science as a side"
    },
    {
      "start": 1094.06,
      "end": 1097.74,
      "text": "Career path if so if there's anyone here who wants to talk statistics"
    },
    {
      "start": 1098.38,
      "end": 1105.42,
      "text": "Please by all means hit me up. This is sort of this is the sort of conversation that I I'd love to dive into"
    },
    {
      "start": 1106.22,
      "end": 1108.94,
      "text": "But with that being said what we're doing today"
    },
    {
      "start": 1109.9,
      "end": 1115.1,
      "text": "Is essentially going to be diving deep into some of our first large language models"
    },
    {
      "start": 1115.5,
      "end": 1120.7,
      "text": "Let me go ahead and take a brief moment to pause. I just want to make sure we're all on the same page"
    },
    {
      "start": 1121.26,
      "end": 1124.94,
      "text": "Now I asked you all earlier just to make sure"
    },
    {
      "start": 1125.82,
      "end": 1130.78,
      "text": "That we had a chance to fork and clone the specific repo is there anyone who"
    },
    {
      "start": 1131.34,
      "end": 1136.46,
      "text": "Hasn't had a chance to do that yet. Give me a thumbs down if you haven't had a chance to do that if that's okay"
    },
    {
      "start": 1136.78,
      "end": 1138.78,
      "text": "Let's go ahead and do that. I"
    },
    {
      "start": 1139.66,
      "end": 1142.54,
      "text": "Said yeah, yeah, like this is this is what we're here for so"
    },
    {
      "start": 1143.42,
      "end": 1147.98,
      "text": "I think I shared the link to that on the Slack channel. Yeah, so go ahead"
    },
    {
      "start": 1148.7,
      "end": 1151.82,
      "text": "I'll go ahead and share that once again"
    },
    {
      "start": 1152.3,
      "end": 1153.26,
      "text": "I'm"
    },
    {
      "start": 1153.26,
      "end": 1161.82,
      "text": "Just in case but what we're going to do and what I'm going to walk us through is simply the process of these instructions listed below the idea is"
    },
    {
      "start": 1162.14,
      "end": 1165.42,
      "text": "But I want to be able to make sure to create a virtual environment in my local computer"
    },
    {
      "start": 1165.58,
      "end": 1170.7,
      "text": "Because I'm going to be installing especially over the next few weeks a lot of different packages some of these packages"
    },
    {
      "start": 1171.5,
      "end": 1177.9,
      "text": "Might play well together, but some of them might not and it's certainly a good habit to make sure that all of this is all of these"
    },
    {
      "start": 1178.46,
      "end": 1182.22,
      "text": "Different installations are done in a virtual environment. So"
    },
    {
      "start": 1183.1,
      "end": 1188.06,
      "text": "With that said there's a couple of different ways you can do this if you have a preferred way to do that"
    },
    {
      "start": 1188.54,
      "end": 1191.1,
      "text": "Highly recommend you take that approach"
    },
    {
      "start": 1191.58,
      "end": 1196.14,
      "text": "I'm simply going to go over the approach that's listed on the GitHub repository"
    },
    {
      "start": 1197.1,
      "end": 1198.68,
      "text": "um"
    },
    {
      "start": 1198.68,
      "end": 1202.2,
      "text": "The grab my terminal. That's where did you go?"
    },
    {
      "start": 1203.39,
      "end": 1205.87,
      "text": "Problem with having of a Jillian screens"
    },
    {
      "start": 1207.07,
      "end": 1209.07,
      "text": "So you never know where you put things"
    },
    {
      "start": 1217.24,
      "end": 1219.24,
      "text": "Yeah, here we go"
    },
    {
      "start": 1220.12,
      "end": 1222.12,
      "text": "There get that out of the way"
    },
    {
      "start": 1222.44,
      "end": 1226.46,
      "text": "Let's set this up"
    },
    {
      "start": 1226.46,
      "end": 1229.18,
      "text": "All right, so here's what we're going to need to do"
    },
    {
      "start": 1229.82,
      "end": 1233.98,
      "text": "This is the first repository for our class today"
    },
    {
      "start": 1234.94,
      "end": 1238.54,
      "text": "But we're going to have pretty much a whole lot of other repositories down the road"
    },
    {
      "start": 1238.54,
      "end": 1242.7,
      "text": "So I want to make sure that you have your files relatively organized"
    },
    {
      "start": 1243.26,
      "end": 1244.46,
      "text": "so"
    },
    {
      "start": 1244.46,
      "end": 1250.78,
      "text": "Most of you should have a documents folder. I'm going to recommend that you create a directory on your local like on your local computer to"
    },
    {
      "start": 1251.58,
      "end": 1254.94,
      "text": "Clown and save all of these repositories for today"
    },
    {
      "start": 1255.66,
      "end": 1259.42,
      "text": "I'm simply going to navigate over to my documents folder"
    },
    {
      "start": 1259.9,
      "end": 1261.26,
      "text": "um"
    },
    {
      "start": 1261.26,
      "end": 1263.9,
      "text": "Inside that folder. This is where I'm going to"
    },
    {
      "start": 1265.1,
      "end": 1267.1,
      "text": "clone my"
    },
    {
      "start": 1268.25,
      "end": 1269.85,
      "text": "Oh my files um"
    },
    {
      "start": 1271.21,
      "end": 1274.41,
      "text": "Let's create a folder called let's say"
    },
    {
      "start": 1274.97,
      "end": 1276.41,
      "text": "AI dev class"
    },
    {
      "start": 1278.3,
      "end": 1282.62,
      "text": "And that's where I'm going to make sure that all of my content is located. So"
    },
    {
      "start": 1283.5,
      "end": 1285.34,
      "text": "What I'm doing right now just in case"
    },
    {
      "start": 1286.46,
      "end": 1288.62,
      "text": "For anyone who hasn't seen this before just"
    },
    {
      "start": 1289.34,
      "end": 1294.46,
      "text": "Happy to reach out like if this is not something you've seen before if you're not familiar with this stick around after class"
    },
    {
      "start": 1294.46,
      "end": 1299.34,
      "text": "I will talk a little bit about navigating the the command line in that case"
    },
    {
      "start": 1299.5,
      "end": 1303.5,
      "text": "But all I'm doing is creating a folder to you clone"
    },
    {
      "start": 1304.22,
      "end": 1309.82,
      "text": "All of the content that's coming from this repository and you can even see the slides located in there as well. So"
    },
    {
      "start": 1310.62,
      "end": 1312.62,
      "text": "I am going to"
    },
    {
      "start": 1312.62,
      "end": 1314.01,
      "text": "found"
    },
    {
      "start": 1314.01,
      "end": 1316.81,
      "text": "The repository that I've just"
    },
    {
      "start": 1317.85,
      "end": 1319.85,
      "text": "loaded into this file"
    },
    {
      "start": 1322.17,
      "end": 1324.33,
      "text": "And while I'm here"
    },
    {
      "start": 1328.08,
      "end": 1331.28,
      "text": "Now let's just go ahead and create the environment right inside"
    },
    {
      "start": 1332.62,
      "end": 1334.62,
      "text": "That's folder"
    },
    {
      "start": 1334.62,
      "end": 1339.9,
      "text": "So I just want to navigate into the folder and create my environment there. So"
    },
    {
      "start": 1340.7,
      "end": 1344.46,
      "text": "I'm presuming that pipe you've had a chance to install Python if you haven't"
    },
    {
      "start": 1345.02,
      "end": 1346.22,
      "text": "Okay"
    },
    {
      "start": 1346.22,
      "end": 1351.74,
      "text": "Stick around watch the recording get the concepts and then we'll and we'll we'll troubleshoot those questions"
    },
    {
      "start": 1352.7,
      "end": 1355.74,
      "text": "Towards the last 30 minutes of the session"
    },
    {
      "start": 1356.14,
      "end": 1361.9,
      "text": "But the idea is that I want to create a virtual environment like man for this is pretty standard"
    },
    {
      "start": 1361.9,
      "end": 1366.06,
      "text": "It's Python 3 or Python regular as long as you have Python 3.6"
    },
    {
      "start": 1366.54,
      "end": 1368.54,
      "text": "You should be good to go"
    },
    {
      "start": 1368.54,
      "end": 1372.06,
      "text": "I'm going to create a virtual environment. I'm just going to call this one"
    },
    {
      "start": 1373.37,
      "end": 1375.61,
      "text": "You can call this whatever you want. I'm going to call this one"
    },
    {
      "start": 1376.97,
      "end": 1378.49,
      "text": "Let's say"
    },
    {
      "start": 1378.49,
      "end": 1380.44,
      "text": "AI school"
    },
    {
      "start": 1380.44,
      "end": 1386.44,
      "text": "So Python 3.mve and v.ai school that dot"
    },
    {
      "start": 1387.08,
      "end": 1388.6,
      "text": "Creates this as a hidden file"
    },
    {
      "start": 1391.28,
      "end": 1395.52,
      "text": "It's going to be a little bit of a brief delay and I like to showcase and make sure that my files are working"
    },
    {
      "start": 1395.52,
      "end": 1399.12,
      "text": "By just showcasing lsa. There's AI school"
    },
    {
      "start": 1399.52,
      "end": 1401.52,
      "text": "Along with all of the other"
    },
    {
      "start": 1401.6,
      "end": 1403.04,
      "text": "content that was"
    },
    {
      "start": 1403.04,
      "end": 1405.52,
      "text": "brought in from that well-being"
    },
    {
      "start": 1406.08,
      "end": 1408.32,
      "text": "From the repo that we just formed"
    },
    {
      "start": 1409.58,
      "end": 1414.22,
      "text": "And what I want to make sure is I just want to make sure that this is running in a virtual environment"
    },
    {
      "start": 1414.22,
      "end": 1416.94,
      "text": "And we simply activate that with source"
    },
    {
      "start": 1418.6,
      "end": 1420.6,
      "text": "dot AI school"
    },
    {
      "start": 1421.4,
      "end": 1423.05,
      "text": "Then"
    },
    {
      "start": 1423.05,
      "end": 1424.54,
      "text": "Activate"
    },
    {
      "start": 1424.54,
      "end": 1426.54,
      "text": "If you're on windows. I think the command"
    },
    {
      "start": 1427.92,
      "end": 1430.0,
      "text": "Is scripts activate"
    },
    {
      "start": 1432.49,
      "end": 1437.53,
      "text": "So that's my fair warning if you're on windows. It's AI school then scripts activate"
    },
    {
      "start": 1439.32,
      "end": 1441.8,
      "text": "And at least on my side you can see that like"
    },
    {
      "start": 1441.8,
      "end": 1445.8,
      "text": "This dot AI school at the very beginning needs that I have activated those scripts"
    },
    {
      "start": 1448.04,
      "end": 1452.44,
      "text": "Now here's the fun part in order to get some of this code to work"
    },
    {
      "start": 1453.47,
      "end": 1458.51,
      "text": "And if it doesn't work for you if there's some if for whatever reason there's a path issue on your local computer"
    },
    {
      "start": 1458.59,
      "end": 1462.75,
      "text": "We have alternatives to get that API key to work for your computer"
    },
    {
      "start": 1463.74,
      "end": 1468.3,
      "text": "But I need to be able to install some of the files in this requirements folder"
    },
    {
      "start": 1469.18,
      "end": 1472.94,
      "text": "So let me just talk a little bit about what's in that requirements folder"
    },
    {
      "start": 1474.57,
      "end": 1478.17,
      "text": "I like to showcase. I just used a nano to open up the folder itself"
    },
    {
      "start": 1478.73,
      "end": 1482.25,
      "text": "I'm going to I want to showcase that these are the packages. I'm going to be installing it's"
    },
    {
      "start": 1482.81,
      "end": 1487.45,
      "text": "Langshane, Langshane Core, OpenAI, and Langshane Community"
    },
    {
      "start": 1489.29,
      "end": 1491.05,
      "text": "Actually missing one of the packages here"
    },
    {
      "start": 1491.45,
      "end": 1495.69,
      "text": "Alamo, I believe, but we'll see if we get to that towards the end of the class"
    },
    {
      "start": 1496.33,
      "end": 1501.61,
      "text": "But the idea is that I want to be able to install all of these requirements in one go"
    },
    {
      "start": 1502.49,
      "end": 1505.05,
      "text": "So I'm simply going to use a pip install"
    },
    {
      "start": 1505.05,
      "end": 1507.85,
      "text": "Again, if you have a different method for using this that's fine"
    },
    {
      "start": 1508.49,
      "end": 1511.77,
      "text": "I just want to install the files from that requirements dot the xt"
    },
    {
      "start": 1512.72,
      "end": 1514.72,
      "text": "In this virtual environment"
    },
    {
      "start": 1519.4,
      "end": 1522.33,
      "text": "See how we're doing"
    },
    {
      "start": 1522.65,
      "end": 1525.69,
      "text": "Again, if we've already gone past"
    },
    {
      "start": 1526.65,
      "end": 1529.69,
      "text": "Where you're feeling comfortable with that's okay stick around"
    },
    {
      "start": 1529.69,
      "end": 1534.09,
      "text": "After our lecture portion of the pass and we'll talk this through"
    },
    {
      "start": 1535.23,
      "end": 1540.11,
      "text": "I just want to make sure that this is one of the steps that we are comfortable with"
    },
    {
      "start": 1541.12,
      "end": 1544.8,
      "text": "Ah, it's always one of those things that I really need to get from that creating"
    },
    {
      "start": 1546.16,
      "end": 1548.54,
      "text": "All right, so"
    },
    {
      "start": 1548.54,
      "end": 1555.1,
      "text": "After that long line of code what I want to do is this part is kind of an optional if you know what you're doing"
    },
    {
      "start": 1555.82,
      "end": 1560.14,
      "text": "That's okay, but what I want to do is create a dot environment folder"
    },
    {
      "start": 1560.86,
      "end": 1562.86,
      "text": "That we can use to"
    },
    {
      "start": 1563.18,
      "end": 1565.18,
      "text": "either access"
    },
    {
      "start": 1565.18,
      "end": 1566.7,
      "text": "With"
    },
    {
      "start": 1566.7,
      "end": 1570.62,
      "text": "Directly through the pie file or we can access directly from the environment that we're working with"
    },
    {
      "start": 1571.18,
      "end": 1575.56,
      "text": "So I'm going to copy the dot EMV file"
    },
    {
      "start": 1575.56,
      "end": 1578.52,
      "text": "So this is a file folder that's all that already exists"
    },
    {
      "start": 1579.4,
      "end": 1581.8,
      "text": "From the files that we cloned so"
    },
    {
      "start": 1582.52,
      "end": 1584.52,
      "text": "Not EMV I just want to showcase"
    },
    {
      "start": 1584.52,
      "end": 1587.88,
      "text": "This is the open API key that we've included created earlier"
    },
    {
      "start": 1589.98,
      "end": 1594.38,
      "text": "So you can actually add the key to that at this point there"
    },
    {
      "start": 1596.17,
      "end": 1600.65,
      "text": "Or you can simply copy the structure of the of the sample"
    },
    {
      "start": 1601.45,
      "end": 1608.09,
      "text": "By creating a dot EMV of your again, all of this is up again of repo. I'm simply going to ending this step by step"
    },
    {
      "start": 1609.82,
      "end": 1613.02,
      "text": "So now we have both a dot EMV and a dot EMV sample"
    },
    {
      "start": 1614.06,
      "end": 1616.06,
      "text": "I want to update"
    },
    {
      "start": 1616.91,
      "end": 1623.63,
      "text": "That key value that we created so I'm just going to nano into that environment folder and I'm going to replace"
    },
    {
      "start": 1624.67,
      "end": 1626.43,
      "text": "the super secret key text"
    },
    {
      "start": 1630.04,
      "end": 1633.24,
      "text": "With the API key that we've just shared"
    },
    {
      "start": 1636.86,
      "end": 1639.98,
      "text": "Still you shared that on the slack channel"
    },
    {
      "start": 1643.37,
      "end": 1647.45,
      "text": "So I'm going to copy that and include that here"
    },
    {
      "start": 1650.52,
      "end": 1652.2,
      "text": "Just make sure that the key name"
    },
    {
      "start": 1652.68,
      "end": 1657.96,
      "text": "Or that the environment variable name is exactly open at AI-APA"
    },
    {
      "start": 1658.84,
      "end": 1662.36,
      "text": "Excuse me underscore API underscore key"
    },
    {
      "start": 1663.8,
      "end": 1666.6,
      "text": "And with that I'm going to save the files"
    },
    {
      "start": 1668.65,
      "end": 1671.45,
      "text": "And then just to be on the safest side"
    },
    {
      "start": 1672.67,
      "end": 1675.07,
      "text": "Honestly, this part is a little bit overkill"
    },
    {
      "start": 1675.07,
      "end": 1681.39,
      "text": "But if you look at the environment, I'm going to kind of embarrass myself a little bit by showing some of the"
    },
    {
      "start": 1684.67,
      "end": 1688.51,
      "text": "Terrible work I've done with keeping my environment in a consistent format"
    },
    {
      "start": 1689.07,
      "end": 1691.47,
      "text": "With some of the home room content"
    },
    {
      "start": 1691.47,
      "end": 1696.17,
      "text": "But one of the things I want to be able to do is I want to be able to"
    },
    {
      "start": 1696.89,
      "end": 1700.41,
      "text": "Export the open AI key that we've just created"
    },
    {
      "start": 1700.41,
      "end": 1705.05,
      "text": "So I'm showcasing all of the content that is previously in this virtual environment"
    },
    {
      "start": 1705.61,
      "end": 1708.01,
      "text": "I want to export"
    },
    {
      "start": 1708.01,
      "end": 1709.69,
      "text": "One more variable"
    },
    {
      "start": 1709.69,
      "end": 1712.17,
      "text": "Open AI underscore API key"
    },
    {
      "start": 1715.4,
      "end": 1717.48,
      "text": "And this is again just overkill"
    },
    {
      "start": 1718.28,
      "end": 1722.28,
      "text": "Where I simply I'm creating the API key into my local environment"
    },
    {
      "start": 1724.76,
      "end": 1728.2,
      "text": "So when I've heard that environment now I have it in two different locations"
    },
    {
      "start": 1728.2,
      "end": 1729.64,
      "text": "Again, this is simply"
    },
    {
      "start": 1730.44,
      "end": 1734.36,
      "text": "Being I'm simply playing it safe to be on the safe side"
    },
    {
      "start": 1735.16,
      "end": 1739.56,
      "text": "You can do one tool or the other you can essentially take one step or the other"
    },
    {
      "start": 1740.04,
      "end": 1744.28,
      "text": "But the idea behind this is that when we start running commands"
    },
    {
      "start": 1747.1,
      "end": 1749.9,
      "text": "If you have it to run commands in the command line"
    },
    {
      "start": 1749.9,
      "end": 1752.86,
      "text": "We're going to meet we can essentially easily access them"
    },
    {
      "start": 1753.95,
      "end": 1755.61,
      "text": "Uh"
    },
    {
      "start": 1755.61,
      "end": 1759.61,
      "text": "We could easily access those uh the key itself. So hopefully"
    },
    {
      "start": 1762.36,
      "end": 1765.96,
      "text": "Directly from the command line it should work to just use"
    },
    {
      "start": 1766.84,
      "end": 1769.32,
      "text": "5 on 3 in class example and let's say"
    },
    {
      "start": 1770.28,
      "end": 1771.8,
      "text": "I have a file called"
    },
    {
      "start": 1773.6,
      "end": 1774.84,
      "text": "Uh"
    },
    {
      "start": 1774.84,
      "end": 1776.36,
      "text": "What's the first file called"
    },
    {
      "start": 1777.32,
      "end": 1780.73,
      "text": "And get simple check history"
    },
    {
      "start": 1780.73,
      "end": 1782.73,
      "text": "It's one of them that should work on itself"
    },
    {
      "start": 1786.16,
      "end": 1787.76,
      "text": "And if that file runs"
    },
    {
      "start": 1787.76,
      "end": 1789.76,
      "text": "Yeah"
    },
    {
      "start": 1789.76,
      "end": 1800.67,
      "text": "We're on the right track"
    },
    {
      "start": 1800.67,
      "end": 1803.71,
      "text": "I was in here just for a moment to see if any questions might have come up"
    },
    {
      "start": 1805.04,
      "end": 1807.2,
      "text": "It's a matter of uh struggling with the"
    },
    {
      "start": 1807.2,
      "end": 1808.48,
      "text": "Environment keys"
    },
    {
      "start": 1808.48,
      "end": 1813.05,
      "text": "That's fine"
    },
    {
      "start": 1813.05,
      "end": 1816.57,
      "text": "If even if you know you did everything correctly and it didn't quite work"
    },
    {
      "start": 1816.57,
      "end": 1818.17,
      "text": "The other tool that we can use"
    },
    {
      "start": 1819.05,
      "end": 1822.09,
      "text": "The other tool that I can recommend you use is that right here"
    },
    {
      "start": 1822.17,
      "end": 1824.89,
      "text": "While you're in this virtual environment"
    },
    {
      "start": 1824.89,
      "end": 1827.13,
      "text": "You can also pip install Python"
    },
    {
      "start": 1828.3,
      "end": 1829.26,
      "text": "Um"
    },
    {
      "start": 1829.26,
      "end": 1831.4,
      "text": "That ENV"
    },
    {
      "start": 1831.4,
      "end": 1833.08,
      "text": "Which is the other package"
    },
    {
      "start": 1834.36,
      "end": 1836.92,
      "text": "That you can install that you can uh import"
    },
    {
      "start": 1838.2,
      "end": 1840.04,
      "text": "As a tool"
    },
    {
      "start": 1840.04,
      "end": 1842.76,
      "text": "Into your script to access"
    },
    {
      "start": 1844.64,
      "end": 1847.2,
      "text": "The environments that we created"
    },
    {
      "start": 1847.84,
      "end": 1855.24,
      "text": "So that's another option"
    },
    {
      "start": 1855.24,
      "end": 1857.76,
      "text": "Any questions so far"
    },
    {
      "start": 1857.76,
      "end": 1860.48,
      "text": "Cool okay feeling good all right basic stuff"
    },
    {
      "start": 1860.96,
      "end": 1862.96,
      "text": "Keeping it basic so"
    },
    {
      "start": 1862.96,
      "end": 1864.4,
      "text": "Um"
    },
    {
      "start": 1864.4,
      "end": 1866.16,
      "text": "A little bit behind schedule but that's okay"
    },
    {
      "start": 1866.8,
      "end": 1870.16,
      "text": "The next bit of the components is actually pretty straightforward"
    },
    {
      "start": 1870.16,
      "end": 1873.92,
      "text": "He does at this point I'm getting the sense that you have uh some familiarity with this"
    },
    {
      "start": 1874.4,
      "end": 1876.16,
      "text": "But what we're going to be doing is"
    },
    {
      "start": 1876.16,
      "end": 1879.6,
      "text": "Essentially talking a little bit about large how large language models work"
    },
    {
      "start": 1879.6,
      "end": 1882.56,
      "text": "And in order to understand it we need to know what a large language model is"
    },
    {
      "start": 1882.56,
      "end": 1885.76,
      "text": "If you have a large language model is nothing more than just a"
    },
    {
      "start": 1886.56,
      "end": 1888.56,
      "text": "Tool that allows you to"
    },
    {
      "start": 1890.09,
      "end": 1895.05,
      "text": "Create content or create essentially the structure of one word based on the previous word that exist"
    },
    {
      "start": 1895.05,
      "end": 1897.13,
      "text": "There's a little bit of a statistical"
    },
    {
      "start": 1897.13,
      "end": 1900.73,
      "text": "Physical properties that are happening behind the scenes that we're going to talk about at just a point"
    },
    {
      "start": 1901.21,
      "end": 1906.81,
      "text": "But if you're here it's generally and probably because you are very familiar with what a large language model"
    },
    {
      "start": 1907.29,
      "end": 1911.05,
      "text": "Does I'd like to just for the sake of uh"
    },
    {
      "start": 1912.09,
      "end": 1918.57,
      "text": "Making sure that we're fairly comfortable with the concepts we are going to discuss a little bit about what Markov chains look like and what they do"
    },
    {
      "start": 1919.29,
      "end": 1922.09,
      "text": "But in order to understand how these large language models work"
    },
    {
      "start": 1922.09,
      "end": 1925.61,
      "text": "I also want to be able to address how these large language models operate"
    },
    {
      "start": 1925.61,
      "end": 1927.61,
      "text": "So we're going to talk a little bit about what these messages"
    },
    {
      "start": 1928.41,
      "end": 1931.53,
      "text": "What large language models messages look like when you're"
    },
    {
      "start": 1932.17,
      "end": 1936.09,
      "text": "Creating them which how you're programming them from the very beginning for talk a little bit about what"
    },
    {
      "start": 1936.33,
      "end": 1942.25,
      "text": "Prompting may look like specifically one zero one shot and few shot prompting may look like specifically"
    },
    {
      "start": 1942.25,
      "end": 1944.73,
      "text": "We're going to be addressing more of the prompting components"
    },
    {
      "start": 1945.29,
      "end": 1947.45,
      "text": "Later this week when we integrate them with"
    },
    {
      "start": 1947.85,
      "end": 1950.33,
      "text": "Mike Smith and for blank views and"
    },
    {
      "start": 1950.97,
      "end": 1958.49,
      "text": "Part of our major conversation is just how the prompts and the messages that we use inform our models behaviors"
    },
    {
      "start": 1959.29,
      "end": 1964.01,
      "text": "Finally, we're going to be able to end the class with a little bit about a little bit"
    },
    {
      "start": 1964.33,
      "end": 1966.33,
      "text": "I'll talk about how chat histories"
    },
    {
      "start": 1967.69,
      "end": 1973.45,
      "text": "Essentially help our model remember what has been written out so far"
    },
    {
      "start": 1974.62,
      "end": 1981.82,
      "text": "And if we have time, I'll make sure to kind of address how you can install a lot of if you haven't done so already"
    },
    {
      "start": 1982.46,
      "end": 1986.06,
      "text": "But it's an excellent fun tool to just have on your"
    },
    {
      "start": 1987.82,
      "end": 1991.26,
      "text": "On your local on your local computer if you haven't had a chance to do that all"
    },
    {
      "start": 1993.28,
      "end": 1994.8,
      "text": "So with that said"
    },
    {
      "start": 1994.8,
      "end": 1997.76,
      "text": "Let's talk a little bit about the intuition behind the map alone"
    },
    {
      "start": 1998.4,
      "end": 2000.4,
      "text": "And generally speaking, this is simply a"
    },
    {
      "start": 2001.36,
      "end": 2003.36,
      "text": "ask that has a"
    },
    {
      "start": 2003.36,
      "end": 2004.72,
      "text": "series of"
    },
    {
      "start": 2004.72,
      "end": 2010.64,
      "text": "Understanding what words existed beforehand in order to determine what the next word is going to be"
    },
    {
      "start": 2010.96,
      "end": 2017.44,
      "text": "So it's essentially just a token predictor it essentially books at a specific statistic one analysis based on like"
    },
    {
      "start": 2017.6,
      "end": 2019.6,
      "text": "simple mark off chain where we are"
    },
    {
      "start": 2020.16,
      "end": 2024.88,
      "text": "Determining based on probability what the next word in a sentence might look like"
    },
    {
      "start": 2025.44,
      "end": 2030.48,
      "text": "So this is not essentially going to be a magical tool in any way that gives you all the answers to any of the problems"
    },
    {
      "start": 2030.48,
      "end": 2033.04,
      "text": "We are going to be engineering these tools"
    },
    {
      "start": 2033.44,
      "end": 2034.88,
      "text": "To"
    },
    {
      "start": 2034.88,
      "end": 2042.64,
      "text": "To address what like to kind of address and improve what the next word in our chain or our"
    },
    {
      "start": 2043.84,
      "end": 2047.68,
      "text": "In our structured language might look like I'm going to share and showcase"
    },
    {
      "start": 2049.28,
      "end": 2052.32,
      "text": "This particular notebook we're just going to"
    },
    {
      "start": 2052.88,
      "end": 2054.4,
      "text": "Roll over to"
    },
    {
      "start": 2054.4,
      "end": 2056.72,
      "text": "Google call that for just a hot second"
    },
    {
      "start": 2057.6,
      "end": 2059.12,
      "text": "Because I"
    },
    {
      "start": 2059.12,
      "end": 2062.32,
      "text": "Find these mark off chains to be really exciting for a second"
    },
    {
      "start": 2065.29,
      "end": 2066.49,
      "text": "And"
    },
    {
      "start": 2072.91,
      "end": 2074.91,
      "text": "Okay, well, let's just open that up"
    },
    {
      "start": 2074.91,
      "end": 2087.21,
      "text": "All right, and if you aren't following along on the slides"
    },
    {
      "start": 2088.01,
      "end": 2090.81,
      "text": "Here's the link if this is something you might want to play on your own"
    },
    {
      "start": 2097.13,
      "end": 2103.21,
      "text": "And the idea behind this notebook is just to kind of showcase what how I'm not going to spend too much time on this because"
    },
    {
      "start": 2103.93,
      "end": 2106.41,
      "text": "This should be pretty straightforward to all of you"
    },
    {
      "start": 2106.41,
      "end": 2112.41,
      "text": "But this is a mark off chain class or we are just manually creating a simple mark off chain"
    },
    {
      "start": 2113.05,
      "end": 2115.61,
      "text": "That follows the same properties as the"
    },
    {
      "start": 2117.13,
      "end": 2120.25,
      "text": "As the tool we've built so far now mark off chain"
    },
    {
      "start": 2120.89,
      "end": 2122.89,
      "text": "named after abrade marga"
    },
    {
      "start": 2123.13,
      "end": 2124.89,
      "text": "essentially just"
    },
    {
      "start": 2124.89,
      "end": 2128.01,
      "text": "A mathematician from like the early from the"
    },
    {
      "start": 2128.89,
      "end": 2132.97,
      "text": "May 1800s kind of love this guy you look there look at that beard just kind of"
    },
    {
      "start": 2133.45,
      "end": 2138.57,
      "text": "Kind of that epic work just screams intense Russian mathematician that essentially just"
    },
    {
      "start": 2139.45,
      "end": 2145.61,
      "text": "Got really successful at age 19 and essentially started creating these types of tools that we're still using to get"
    },
    {
      "start": 2145.77,
      "end": 2149.93,
      "text": "So I don't I just like showing this picture cuz seriously look at that beard"
    },
    {
      "start": 2150.25,
      "end": 2156.01,
      "text": "But what a mark off chain is essentially doing is essentially a tool that looks at a particular object"
    },
    {
      "start": 2156.65,
      "end": 2158.81,
      "text": "As it exists and based on a probability"
    },
    {
      "start": 2159.37,
      "end": 2162.41,
      "text": "On the probability of that particular object. It's going to"
    },
    {
      "start": 2162.89,
      "end": 2165.85,
      "text": "Determine whether that object goes in one direction or the other"
    },
    {
      "start": 2166.17,
      "end": 2173.13,
      "text": "Essentially in a in a very simplistic term all we're doing with a mark up chain is we're assigning the probability that it's going to be"
    },
    {
      "start": 2174.28,
      "end": 2179.72,
      "text": "That it's either going to stay within its own value or shift into the next word"
    },
    {
      "start": 2180.67,
      "end": 2185.15,
      "text": "So the way it works behind the scenes just run this anyway"
    },
    {
      "start": 2186.96,
      "end": 2193.68,
      "text": "Is that if you have a specific set of words and that class is run first"
    },
    {
      "start": 2194.83,
      "end": 2196.83,
      "text": "What this mark off chain does"
    },
    {
      "start": 2198.01,
      "end": 2199.77,
      "text": "Is that we are going to build"
    },
    {
      "start": 2200.49,
      "end": 2205.29,
      "text": "Want a specific statement so if we have this statement that says this is a test of the mark off chain"
    },
    {
      "start": 2205.77,
      "end": 2214.57,
      "text": "What a mark off chain does is it's going to tell you what is the probability that if I have one given word that I'm going to return the next word"
    },
    {
      "start": 2215.61,
      "end": 2222.17,
      "text": "And in this small example we have a very structured sentence this is one sentence and the word"
    },
    {
      "start": 2223.05,
      "end": 2227.05,
      "text": "And the order of this word is problem like the probability that the next word"
    },
    {
      "start": 2227.69,
      "end": 2229.21,
      "text": "is pretty well defined"
    },
    {
      "start": 2230.09,
      "end": 2231.61,
      "text": "Given on"
    },
    {
      "start": 2231.61,
      "end": 2236.73,
      "text": "How those words were essentially structured so this is a test of the mark on chain"
    },
    {
      "start": 2238.04,
      "end": 2245.88,
      "text": "And if you're asking any specific mark off chain to generate text based on any particular word or any starting word"
    },
    {
      "start": 2246.28,
      "end": 2252.76,
      "text": "We can expect that the word is going to follow the order of how it was originally created"
    },
    {
      "start": 2254.35,
      "end": 2255.87,
      "text": "If you're with me so far"
    },
    {
      "start": 2255.95,
      "end": 2258.91,
      "text": "You've probably heard this at some point in this profession obviously"
    },
    {
      "start": 2259.47,
      "end": 2266.67,
      "text": "Now as any large language model user knows this is not enough data to actually give us something useful"
    },
    {
      "start": 2267.31,
      "end": 2272.91,
      "text": "Any large language model is based on tons and tons of data and tons and tons of information"
    },
    {
      "start": 2272.91,
      "end": 2276.75,
      "text": "Scrape from all over the internet depending on what type of fully working with"
    },
    {
      "start": 2277.39,
      "end": 2279.15,
      "text": "Open AI of course"
    },
    {
      "start": 2279.15,
      "end": 2283.95,
      "text": "Very well known has essentially scraped pretty much everything you could potentially scrape from the internet itself"
    },
    {
      "start": 2284.43,
      "end": 2287.47,
      "text": "So what I'm doing here is I'm essentially pulling from the Gutenberg"
    },
    {
      "start": 2288.51,
      "end": 2291.95,
      "text": "Text essentially the entire text from the Alice in Wonderland book"
    },
    {
      "start": 2292.59,
      "end": 2294.59,
      "text": "And I'm recreating"
    },
    {
      "start": 2294.59,
      "end": 2297.95,
      "text": "From that text file that same mark of chain file"
    },
    {
      "start": 2298.67,
      "end": 2301.95,
      "text": "And now we have a different set of tools"
    },
    {
      "start": 2301.95,
      "end": 2307.07,
      "text": "In this case the next word reference that may come after we work with the word Alice"
    },
    {
      "start": 2308.48,
      "end": 2310.24,
      "text": "Is now a little bit"
    },
    {
      "start": 2310.24,
      "end": 2318.32,
      "text": "More vague it's there's not just one word that always follows the word Alice in this text Alice is followed by Alice things"
    },
    {
      "start": 2318.64,
      "end": 2327.36,
      "text": "Started Alice after there's now a probability that the word Alice is going to be followed by any one of these potential words"
    },
    {
      "start": 2328.24,
      "end": 2331.2,
      "text": "And when we see that built into a larger"
    },
    {
      "start": 2332.44,
      "end": 2335.0,
      "text": "Generation or a gentler a larger function"
    },
    {
      "start": 2335.64,
      "end": 2339.16,
      "text": "We can essentially begin to create or generate language"
    },
    {
      "start": 2339.72,
      "end": 2341.79,
      "text": "based on the probability"
    },
    {
      "start": 2341.79,
      "end": 2344.27,
      "text": "Of the word Alice starting"
    },
    {
      "start": 2345.07,
      "end": 2349.87,
      "text": "To see what is going to occur next so here's the next text that we can expect"
    },
    {
      "start": 2352.27,
      "end": 2357.79,
      "text": "And so on and so forth after every word there's a probability that that word is going to be followed by a different word"
    },
    {
      "start": 2358.99,
      "end": 2366.43,
      "text": "This is also another example. We essentially start with the word the and we'll see another example of how those words are followed"
    },
    {
      "start": 2367.39,
      "end": 2375.31,
      "text": "So this is just a kind of a simple example just to go over the basics of how mark off chains work because it's important to have the context behind scenes"
    },
    {
      "start": 2377.34,
      "end": 2379.18,
      "text": "But with that in mind"
    },
    {
      "start": 2379.9,
      "end": 2386.22,
      "text": "What we are building is essentially a structure or creating the structure for our llm"
    },
    {
      "start": 2386.7,
      "end": 2388.57,
      "text": "Operate smoothly"
    },
    {
      "start": 2388.57,
      "end": 2391.37,
      "text": "But in order to do that we need to be able to give it direction"
    },
    {
      "start": 2391.85,
      "end": 2394.97,
      "text": "It's not just a matter of giving it a specific text"
    },
    {
      "start": 2395.77,
      "end": 2402.81,
      "text": "It's not just a matter of giving it all of the data and seeing where it seems where it goes because if we just allow it to operate in its own"
    },
    {
      "start": 2403.69,
      "end": 2407.37,
      "text": "Kind of a just randomly assigned structure like you're going to have"
    },
    {
      "start": 2408.54,
      "end": 2410.54,
      "text": "The type of generated text where"
    },
    {
      "start": 2411.71,
      "end": 2417.63,
      "text": "Doesn't necessarily make sense. Well, it's getting she generally gave herself up and I can talk at her full size by this time"
    },
    {
      "start": 2417.95,
      "end": 2422.83,
      "text": "We are essentially randomly assigning words to follow one word after the other"
    },
    {
      "start": 2423.47,
      "end": 2425.47,
      "text": "So we need to be able to give that direction"
    },
    {
      "start": 2426.03,
      "end": 2432.51,
      "text": "We need to be able to give it almost a specific prompt or a specific structure or a specific set of instructions"
    },
    {
      "start": 2433.55,
      "end": 2435.55,
      "text": "So what I want to talk a little bit now"
    },
    {
      "start": 2436.3,
      "end": 2441.42,
      "text": "Well, what I want to talk about here is what is the what are the existing structures"
    },
    {
      "start": 2442.38,
      "end": 2444.38,
      "text": "that are already built into"
    },
    {
      "start": 2444.78,
      "end": 2446.54,
      "text": "Python packages"
    },
    {
      "start": 2446.54,
      "end": 2448.54,
      "text": "because we don't really have to rebuild"
    },
    {
      "start": 2449.5,
      "end": 2452.62,
      "text": "probabilistic nature of open AI to"
    },
    {
      "start": 2453.82,
      "end": 2455.34,
      "text": "generate a"
    },
    {
      "start": 2455.34,
      "end": 2457.1,
      "text": "sentence that makes sense"
    },
    {
      "start": 2457.1,
      "end": 2463.9,
      "text": "But we want to be able to structure and build a an llm that has a specific"
    },
    {
      "start": 2465.2,
      "end": 2467.36,
      "text": "set of instructions for us to work with"
    },
    {
      "start": 2468.22,
      "end": 2469.44,
      "text": "so"
    },
    {
      "start": 2469.44,
      "end": 2471.84,
      "text": "Here's what I'm going to do you can look over this code"
    },
    {
      "start": 2472.48,
      "end": 2475.04,
      "text": "The code is actually also on your"
    },
    {
      "start": 2476.72,
      "end": 2478.72,
      "text": "It's also on your"
    },
    {
      "start": 2479.12,
      "end": 2484.8,
      "text": "On the repo we just downloaded, but I want to talk a little bit about the type of messages that we can build into"
    },
    {
      "start": 2486.56,
      "end": 2488.24,
      "text": "Langchains"
    },
    {
      "start": 2488.24,
      "end": 2490.24,
      "text": "Open AI model"
    },
    {
      "start": 2490.32,
      "end": 2492.08,
      "text": "to essentially"
    },
    {
      "start": 2492.08,
      "end": 2497.68,
      "text": "Direct how that chat or how that open AI model is going to respond"
    },
    {
      "start": 2498.24,
      "end": 2501.76,
      "text": "So there's a few terms that I want to refer to and we're going to have like user messages"
    },
    {
      "start": 2502.16,
      "end": 2504.4,
      "text": "Sometimes they're referred to as human messages"
    },
    {
      "start": 2505.28,
      "end": 2509.44,
      "text": "These are any text or this is the text that we are asking"
    },
    {
      "start": 2510.32,
      "end": 2514.48,
      "text": "Open AI specifically to invoke this is essentially what the human asks"
    },
    {
      "start": 2515.04,
      "end": 2520.56,
      "text": "The system message is the message that we as developers prompt the machine to respond"
    },
    {
      "start": 2521.68,
      "end": 2528.16,
      "text": "In other words, this is what has the most weight because we are giving"
    },
    {
      "start": 2528.8,
      "end": 2533.52,
      "text": "We are putting the guard rails on the open AI language or the open AI model"
    },
    {
      "start": 2534.88,
      "end": 2539.52,
      "text": "In other words, if we ask to build a large language model that doesn't curse"
    },
    {
      "start": 2539.84,
      "end": 2543.68,
      "text": "We incorporate that into the system message because that way whenever a human prompts"
    },
    {
      "start": 2544.48,
      "end": 2546.24,
      "text": "A specific response"
    },
    {
      "start": 2546.96,
      "end": 2548.72,
      "text": "We can essentially"
    },
    {
      "start": 2548.72,
      "end": 2550.88,
      "text": "Create a model that hey"
    },
    {
      "start": 2551.68,
      "end": 2554.48,
      "text": "Make sure that you respond to any of your answers in a way that's not"
    },
    {
      "start": 2555.92,
      "end": 2557.31,
      "text": "cursing"
    },
    {
      "start": 2557.55,
      "end": 2563.31,
      "text": "There's other tools we can essentially ex there's other tools like AI messages which essentially"
    },
    {
      "start": 2564.67,
      "end": 2573.31,
      "text": "Refer to the response from the open AI after or from the LLM after a human has essentially prompted a question"
    },
    {
      "start": 2573.87,
      "end": 2580.43,
      "text": "So the reason for this is that we want to be able to influence the behaviors of these packages and we essentially want to be able to"
    },
    {
      "start": 2581.39,
      "end": 2590.91,
      "text": "Combine both types of messages to create much more accurate or create LLMs that are much more responsive to what we want to do"
    },
    {
      "start": 2591.79,
      "end": 2599.39,
      "text": "So this is an example specifically from the documentation. I'm going to be working in this case in Microsoft VS Code"
    },
    {
      "start": 2599.55,
      "end": 2603.79,
      "text": "There's no requirement in this class as to which IDE you might be more interested in working with"
    },
    {
      "start": 2604.11,
      "end": 2608.11,
      "text": "You can do this at Jupyter Notebook if you feel the most comfortable you can do this on the command line"
    },
    {
      "start": 2608.75,
      "end": 2610.91,
      "text": "But what I'm going to be going over"
    },
    {
      "start": 2611.79,
      "end": 2614.27,
      "text": "Is just going over this specific code"
    },
    {
      "start": 2614.91,
      "end": 2623.31,
      "text": "I'm talking a little bit about how these specific messages work within the framework of our chat open AI model"
    },
    {
      "start": 2624.3,
      "end": 2626.46,
      "text": "I'm going to go ahead and run this within terminal"
    },
    {
      "start": 2627.26,
      "end": 2631.98,
      "text": "Here we go. That one's already set up. This is an environment I had set up earlier"
    },
    {
      "start": 2633.24,
      "end": 2635.32,
      "text": "But here's here's what we're going so"
    },
    {
      "start": 2636.32,
      "end": 2640.96,
      "text": "This is going to come from three of the packages these other packages. We just installed this part of our environment"
    },
    {
      "start": 2642.72,
      "end": 2645.52,
      "text": "You have any problem running any of these cells at any point"
    },
    {
      "start": 2646.48,
      "end": 2649.68,
      "text": "I'm not going to necessarily use a llama in this case"
    },
    {
      "start": 2650.0,
      "end": 2652.48,
      "text": "So I'm going to just comment that out for right now"
    },
    {
      "start": 2653.12,
      "end": 2658.72,
      "text": "But just off of chat a open AI. I'm going to be incorporating a system message and a human message"
    },
    {
      "start": 2658.8,
      "end": 2661.44,
      "text": "I'm just going to showcase both of these tools together"
    },
    {
      "start": 2662.08,
      "end": 2663.04,
      "text": "Chat open AI"
    },
    {
      "start": 2663.6,
      "end": 2665.2,
      "text": "I'm just going to be sticking to"
    },
    {
      "start": 2666.0,
      "end": 2668.56,
      "text": "3.5 turdable model to be fair"
    },
    {
      "start": 2668.56,
      "end": 2671.6,
      "text": "This one's already a little bit out of date at this point if you're using that"
    },
    {
      "start": 2671.68,
      "end": 2674.64,
      "text": "Chat like gpt 4.0 you're more than welcome to"
    },
    {
      "start": 2675.2,
      "end": 2676.8,
      "text": "It's a little bit more pricey at this point"
    },
    {
      "start": 2676.8,
      "end": 2678.48,
      "text": "But the"
    },
    {
      "start": 2678.48,
      "end": 2681.36,
      "text": "I think most of chat you could be right now has moved on to"
    },
    {
      "start": 2682.08,
      "end": 2684.08,
      "text": "version 4.0 in both"
    },
    {
      "start": 2685.44,
      "end": 2688.0,
      "text": "API and in the chat"
    },
    {
      "start": 2688.16,
      "end": 2690.16,
      "text": "It's in the"
    },
    {
      "start": 2690.16,
      "end": 2692.16,
      "text": "Like the website we all access"
    },
    {
      "start": 2692.16,
      "end": 2695.68,
      "text": "But I'm going to be calling this to a to an object called lm"
    },
    {
      "start": 2696.72,
      "end": 2699.12,
      "text": "And I'm going to be calling this as a tool"
    },
    {
      "start": 2701.44,
      "end": 2705.76,
      "text": "That asks the question what would be a good company name for company that makes colorful sucks"
    },
    {
      "start": 2707.2,
      "end": 2711.36,
      "text": "So the first component of langshade is that I'm going to be building"
    },
    {
      "start": 2712.08,
      "end": 2715.68,
      "text": "Calling the open AI not directly from the open AI website"
    },
    {
      "start": 2715.68,
      "end": 2719.28,
      "text": "But from the framework that exists within langshade"
    },
    {
      "start": 2719.95,
      "end": 2724.67,
      "text": "I would think langshade they already have a specific class called human message"
    },
    {
      "start": 2724.67,
      "end": 2728.03,
      "text": "That allows you to specify what that human message is going to look like"
    },
    {
      "start": 2728.03,
      "end": 2730.83,
      "text": "So this is a human message that's coming from us the user"
    },
    {
      "start": 2732.04,
      "end": 2736.6,
      "text": "So my first step I'm simply going to invoke this particular message in a call"
    },
    {
      "start": 2737.4,
      "end": 2740.04,
      "text": "By the way if you're if the last time you work with"
    },
    {
      "start": 2741.0,
      "end": 2744.52,
      "text": "Langshade was about 10 months ago. They did change up a little bit"
    },
    {
      "start": 2746.04,
      "end": 2749.08,
      "text": "The tools that you use to invoke or prompt messages"
    },
    {
      "start": 2749.08,
      "end": 2751.08,
      "text": "So it's no longer"
    },
    {
      "start": 2752.97,
      "end": 2754.49,
      "text": "Chat I believe was the old"
    },
    {
      "start": 2755.37,
      "end": 2757.13,
      "text": "command of four"
    },
    {
      "start": 2757.13,
      "end": 2758.89,
      "text": "prompting messages"
    },
    {
      "start": 2758.89,
      "end": 2760.22,
      "text": "so"
    },
    {
      "start": 2760.22,
      "end": 2762.22,
      "text": "As part of the tool itself"
    },
    {
      "start": 2762.22,
      "end": 2768.14,
      "text": "I'm going to go ahead and run my package python in class example. This is system"
    },
    {
      "start": 2768.86,
      "end": 2770.46,
      "text": "versus user.py"
    },
    {
      "start": 2775.29,
      "end": 2780.49,
      "text": "So that's my first prompt. I've essentially just had one of those brief conversations with"
    },
    {
      "start": 2781.37,
      "end": 2783.37,
      "text": "With chat jbt of those guys"
    },
    {
      "start": 2784.3,
      "end": 2788.94,
      "text": "One quick thing to note specifically because I didn't notice that there's a few of you who'd have mentioned that you are"
    },
    {
      "start": 2789.26,
      "end": 2791.26,
      "text": "Hey, I'm brand new to"
    },
    {
      "start": 2792.22,
      "end": 2796.86,
      "text": "Open AI and I'm brand new to AI. Here's a term that I want to be familiar with"
    },
    {
      "start": 2797.42,
      "end": 2799.74,
      "text": "Because this will come up again down the road"
    },
    {
      "start": 2800.46,
      "end": 2804.46,
      "text": "And it's one of the arguments or one of the attributes that exists within chat open AI"
    },
    {
      "start": 2804.46,
      "end": 2808.86,
      "text": "And you can actually expect to see this at any of the models that we work with whether it's a llama and throbic"
    },
    {
      "start": 2809.26,
      "end": 2812.38,
      "text": "Whatever you want to use one of the concept one of the set one of the"
    },
    {
      "start": 2812.94,
      "end": 2815.34,
      "text": "Attributes that you should be familiar with is temperature"
    },
    {
      "start": 2816.06,
      "end": 2818.22,
      "text": "Temperature specifically refers to"
    },
    {
      "start": 2819.5,
      "end": 2822.94,
      "text": "Literally how hot or cold you want your model to run"
    },
    {
      "start": 2823.82,
      "end": 2825.82,
      "text": "Terrible choke. I'll never say that one again"
    },
    {
      "start": 2825.82,
      "end": 2831.18,
      "text": "But essentially within an argument of zero to one you can determine how"
    },
    {
      "start": 2833.66,
      "end": 2835.66,
      "text": "Random do you want your"
    },
    {
      "start": 2836.46,
      "end": 2838.94,
      "text": "Variables to look like how much"
    },
    {
      "start": 2839.66,
      "end": 2842.94,
      "text": "How much risk do you want it to take when it comes to"
    },
    {
      "start": 2843.82,
      "end": 2847.26,
      "text": "What type of terminology you want the open AI to be"
    },
    {
      "start": 2848.3,
      "end": 2852.54,
      "text": "sending out or or or or or reflecting so in other words"
    },
    {
      "start": 2853.66,
      "end": 2859.74,
      "text": "From the documentation itself if you take a look at the temperature itself. You I think the default is set to"
    },
    {
      "start": 2860.7,
      "end": 2866.14,
      "text": "Incorrect to me if I'm wrong. I think the fault is set to 0.07 which is actually fairly high"
    },
    {
      "start": 2866.94,
      "end": 2870.7,
      "text": "A temperature of 0.07 means that it's just a little bit above a median"
    },
    {
      "start": 2871.66,
      "end": 2873.66,
      "text": "amount where it's going to come up with"
    },
    {
      "start": 2875.13,
      "end": 2877.13,
      "text": "not super structured"
    },
    {
      "start": 2878.92,
      "end": 2880.43,
      "text": "Language"
    },
    {
      "start": 2880.43,
      "end": 2888.83,
      "text": "So just to sort of showcase the set as an example if I have a temperature of 0.01 and I run that same code"
    },
    {
      "start": 2890.4,
      "end": 2892.24,
      "text": "Gonna run this code a couple of times here"
    },
    {
      "start": 2892.96,
      "end": 2895.04,
      "text": "It's going to return rainbow sock.co"
    },
    {
      "start": 2896.3,
      "end": 2898.54,
      "text": "It's gonna return again rainbow sock.co"
    },
    {
      "start": 2898.54,
      "end": 2903.26,
      "text": "It's essentially saying that I want to take less risks with what type of terminology"
    },
    {
      "start": 2904.14,
      "end": 2909.18,
      "text": "Is sent out like the probability that the word I'm going to be showcased is going to be"
    },
    {
      "start": 2910.86,
      "end": 2912.65,
      "text": "Less risky"
    },
    {
      "start": 2912.65,
      "end": 2917.13,
      "text": "Now if I increase that temperature, let's take this up all the way to 0.95"
    },
    {
      "start": 2917.69,
      "end": 2919.69,
      "text": "It means that now my chat"
    },
    {
      "start": 2920.49,
      "end": 2921.93,
      "text": "This chat open AI"
    },
    {
      "start": 2922.81,
      "end": 2927.13,
      "text": "Model from GPT 3.5 is going to be a little bit more creative"
    },
    {
      "start": 2929.08,
      "end": 2930.6,
      "text": "Cool. This one's rainbow footwear"
    },
    {
      "start": 2931.4,
      "end": 2936.6,
      "text": "This next one is again rainbow footwear. Let me try this again. Say if I keep getting into rainbow sock creations"
    },
    {
      "start": 2936.6,
      "end": 2939.32,
      "text": "Okay, getting a little bit of differences here"
    },
    {
      "start": 2939.48,
      "end": 2942.2,
      "text": "So it's essentially if you want to expect different results"
    },
    {
      "start": 2942.84,
      "end": 2945.08,
      "text": "You increase the temperature on the model itself"
    },
    {
      "start": 2946.24,
      "end": 2948.24,
      "text": "So we have a little bit"
    },
    {
      "start": 2949.92,
      "end": 2953.52,
      "text": "So then the question ends up being I'm probably going to go a little bit over time my apologies"
    },
    {
      "start": 2953.76,
      "end": 2955.2,
      "text": "But the question ends up being"
    },
    {
      "start": 2956.38,
      "end": 2960.86,
      "text": "What are some components and this is a question for you. I do you want to hear your voices because I've been talking for about"
    },
    {
      "start": 2961.42,
      "end": 2963.1,
      "text": "Too long right now"
    },
    {
      "start": 2963.1,
      "end": 2969.66,
      "text": "What is a scenario where you might want to have a higher temperature? What kind of output would you want?"
    },
    {
      "start": 2977.39,
      "end": 2983.47,
      "text": "I suppose anything where you have a bias for creativity versus just you know producing like a static answer from a static reset"
    },
    {
      "start": 2983.95,
      "end": 2987.39,
      "text": "Yeah, perfect. Hey, have you ever written a model a novel"
    },
    {
      "start": 2988.44,
      "end": 2992.76,
      "text": "This is how you do it increase the temperature. I do not recommend that you write novels with chat GPT"
    },
    {
      "start": 2993.87,
      "end": 2998.03,
      "text": "Technically, there's you know, I'll leave the ethical part of that. I'm to you. That's not my business"
    },
    {
      "start": 2998.59,
      "end": 3000.59,
      "text": "So on the flip side"
    },
    {
      "start": 3000.59,
      "end": 3007.15,
      "text": "Why would I want my temperature to be much lower? What what kind of work might you you be doing that requires a lower temperature"
    },
    {
      "start": 3009.21,
      "end": 3014.25,
      "text": "Probably like the opposite like legal conformity or something like that where you're researching laws or codes"
    },
    {
      "start": 3015.21,
      "end": 3018.73,
      "text": "Exactly or perhaps another example is something like"
    },
    {
      "start": 3019.13,
      "end": 3025.69,
      "text": "SQL if you want to make sure that you're writing a sequel query you want your temperature to be a much lower scale because you want that"
    },
    {
      "start": 3026.33,
      "end": 3028.97,
      "text": "structure of that same taxity much more"
    },
    {
      "start": 3029.77,
      "end": 3031.84,
      "text": "Composed"
    },
    {
      "start": 3031.92,
      "end": 3037.2,
      "text": "So that's just kind of a general simple overview of like what you can expect from those simple arguments"
    },
    {
      "start": 3037.52,
      "end": 3042.08,
      "text": "But the other component the other attribute that you might want to keep in mind is top E that's another way to"
    },
    {
      "start": 3042.64,
      "end": 3049.92,
      "text": "Kind of control the probability that your your model is going to select the word based on a certain amount of probability"
    },
    {
      "start": 3049.92,
      "end": 3056.72,
      "text": "All in almost like a threshold as to one probability to determine which word to assign to a specific value"
    },
    {
      "start": 3057.44,
      "end": 3064.24,
      "text": "Now that's not quite prompting, but it is something that you need to be familiar with when it comes to prompting a lot of what we're talking about is"
    },
    {
      "start": 3064.64,
      "end": 3071.44,
      "text": "What is the structure that we are giving or one of the instructions that we are giving are of an AI model in this case"
    },
    {
      "start": 3072.24,
      "end": 3076.48,
      "text": "So in this case we started off with just the human message the human prompt"
    },
    {
      "start": 3076.96,
      "end": 3081.6,
      "text": "But what I want to do next is I want to add a little bit"
    },
    {
      "start": 3082.7,
      "end": 3088.14,
      "text": "Of added context, so I'm going to comment out this step three and I'm going to add some system text"
    },
    {
      "start": 3089.34,
      "end": 3092.3,
      "text": "And I've got the set to a sarcastic bond that gives helpful"
    },
    {
      "start": 3093.1,
      "end": 3095.52,
      "text": "advice"
    },
    {
      "start": 3095.92,
      "end": 3100.56,
      "text": "Sometimes I change this so that it's you know the voice of Madonna singing this like"
    },
    {
      "start": 3100.8,
      "end": 3106.4,
      "text": "Some to use pick a pop singer if you want to but as you can see when we add this one of the things that we do"
    },
    {
      "start": 3106.72,
      "end": 3108.88,
      "text": "I'm going to go ahead and kind of doubt this context"
    },
    {
      "start": 3110.67,
      "end": 3114.91,
      "text": "Is that the messages now includes a system message and a human message"
    },
    {
      "start": 3115.31,
      "end": 3117.31,
      "text": "So now I've essentially given the"
    },
    {
      "start": 3117.39,
      "end": 3121.55,
      "text": "Model some framework to operate behind the scenes and in this case, it's going to be a sarcastic bond"
    },
    {
      "start": 3122.65,
      "end": 3125.77,
      "text": "And when I run that code"
    },
    {
      "start": 3125.93,
      "end": 3131.93,
      "text": "The model returns some additional content. Oh wow. What are you going to do? How that's sarcastic? Whatever it says there"
    },
    {
      "start": 3132.33,
      "end": 3134.81,
      "text": "Again, I'm still operating on a high temperature here"
    },
    {
      "start": 3136.08,
      "end": 3138.48,
      "text": "But it is big now. It's giving me a sarcastic answer"
    },
    {
      "start": 3139.63,
      "end": 3145.23,
      "text": "So simply put when we're building our models we want to keep in mind what kind of"
    },
    {
      "start": 3146.03,
      "end": 3149.63,
      "text": "Specific prompts are being built into the model itself"
    },
    {
      "start": 3151.93,
      "end": 3156.57,
      "text": "Any questions or comments about this before I move onto the next bit"
    },
    {
      "start": 3173.44,
      "end": 3177.76,
      "text": "So that's pretty much what a system proper look like the other type of prompts"
    },
    {
      "start": 3177.84,
      "end": 3184.4,
      "text": "I'm not going to code this part too much because it's relatively straightforward and we're going to be doing a lot this a more a lot more of this off third step"
    },
    {
      "start": 3185.04,
      "end": 3190.0,
      "text": "But the other type of prompt that you're going to inform your model is essentially showcasing"
    },
    {
      "start": 3190.72,
      "end": 3197.36,
      "text": "Hey, it's essentially telling any model the more information or any type of like any other type of model the more information"
    },
    {
      "start": 3197.44,
      "end": 3204.56,
      "text": "You give the model the better it's trained the better type of information of the better type of output you can expect"
    },
    {
      "start": 3204.96,
      "end": 3210.64,
      "text": "From a model that has been provided additional prompting additional data additional language to work with"
    },
    {
      "start": 3211.2,
      "end": 3212.48,
      "text": "So the other"
    },
    {
      "start": 3212.48,
      "end": 3219.84,
      "text": "Example that you can expect to see or the some of the terminology that you can expect to hear is the difference between zero shot and view shot prompting"
    },
    {
      "start": 3220.32,
      "end": 3226.64,
      "text": "In other words, the zero shot prompting is any situation where you're just asking the model to operate with just"
    },
    {
      "start": 3227.36,
      "end": 3233.76,
      "text": "Oh with with essentially no learning background. So this zero shot is essentially saying like read a response to the following review"
    },
    {
      "start": 3234.08,
      "end": 3243.28,
      "text": "But you don't actually have any reviews for the model to kind of compare against a few shot type prompt here on the right essentially just says"
    },
    {
      "start": 3243.84,
      "end": 3245.84,
      "text": "Hey, here's a couple of different types of"
    },
    {
      "start": 3248.16,
      "end": 3253.28,
      "text": "Reviews that you can expect to see and here is the response that we've created in the past"
    },
    {
      "start": 3253.68,
      "end": 3260.56,
      "text": "So this is where that combination of system message human message system message human message that's shared it before"
    },
    {
      "start": 3261.9,
      "end": 3264.62,
      "text": "And so with this set of"
    },
    {
      "start": 3265.5,
      "end": 3269.18,
      "text": "Previous prompting we've essentially told the model behind the scenes"
    },
    {
      "start": 3269.74,
      "end": 3272.14,
      "text": "What it can explain like how we can expect"
    },
    {
      "start": 3272.94,
      "end": 3274.14,
      "text": "What kind of"
    },
    {
      "start": 3274.14,
      "end": 3278.78,
      "text": "Gaugard rails were proposing that the model operates with"
    },
    {
      "start": 3279.8,
      "end": 3281.8,
      "text": "So this responding from feedback"
    },
    {
      "start": 3281.8,
      "end": 3286.2,
      "text": "We're glad to hear you had a great experience and then this bottom is sorry didn't hear those product and meet your expectation"
    },
    {
      "start": 3286.6,
      "end": 3291.72,
      "text": "That kind of prompting allows the model to start finding a specific set of patterns to follow"
    },
    {
      "start": 3292.36,
      "end": 3299.16,
      "text": "So not going to spend too much time on this type of prompting quite yet because we're going to talk more about this from Thursday"
    },
    {
      "start": 3300.16,
      "end": 3302.78,
      "text": "Um"
    },
    {
      "start": 3302.78,
      "end": 3306.14,
      "text": "Just take a look and I'd rather talk about chat history. So I'm just kind of gonna"
    },
    {
      "start": 3307.02,
      "end": 3309.18,
      "text": "Handwave you over this next slide"
    },
    {
      "start": 3309.26,
      "end": 3314.86,
      "text": "But if you want in a lot of ways what's happening behind the scenes is that we are"
    },
    {
      "start": 3315.58,
      "end": 3320.06,
      "text": "Specifically defining within the prompt itself some additional context"
    },
    {
      "start": 3320.94,
      "end": 3327.18,
      "text": "So you've probably seen this when you're working with chat GPT on its own that when you create a specific set of instructions"
    },
    {
      "start": 3327.42,
      "end": 3332.14,
      "text": "The model operates a lot more in a lot more in a much more straightforward way"
    },
    {
      "start": 3332.7,
      "end": 3337.66,
      "text": "So any system prompt where you say as a technical support specialist who want to provide clear and concise instructions"
    },
    {
      "start": 3338.14,
      "end": 3343.34,
      "text": "To the user's problem you also want to be able to define what the unique problem is looking"
    },
    {
      "start": 3343.42,
      "end": 3346.14,
      "text": "So this would be what the human looks tries to respond"
    },
    {
      "start": 3346.7,
      "end": 3348.7,
      "text": "Might want to respond as"
    },
    {
      "start": 3348.7,
      "end": 3354.14,
      "text": "But you're going to want to give the system prompt a little bit of additional context behind the scenes"
    },
    {
      "start": 3355.31,
      "end": 3358.43,
      "text": "Another example of how you can decorate your prompt"
    },
    {
      "start": 3358.99,
      "end": 3362.51,
      "text": "Is essentially giving even much more structured card rails"
    },
    {
      "start": 3363.07,
      "end": 3370.59,
      "text": "So if you have a prompt with a schema essentially want the answers to be returned as a almost SQL schema table"
    },
    {
      "start": 3371.31,
      "end": 3376.35,
      "text": "If you have a table where it's defined as employee table you would want the columns to be specified as"
    },
    {
      "start": 3377.9,
      "end": 3384.62,
      "text": "Or creating a specific type of column that's defined with with data type of the particular column you're created"
    },
    {
      "start": 3385.42,
      "end": 3387.42,
      "text": "And this is going to be important when it comes to"
    },
    {
      "start": 3388.52,
      "end": 3390.92,
      "text": "How to how should we say if we have any type of"
    },
    {
      "start": 3391.4,
      "end": 3394.84,
      "text": "input automation that reads emails slack or geratacres"
    },
    {
      "start": 3395.08,
      "end": 3399.0,
      "text": "One of the components that you're going to want to keep in mind is how to create a specific setup prompts"
    },
    {
      "start": 3399.48,
      "end": 3401.48,
      "text": "That follows this general structure"
    },
    {
      "start": 3403.77,
      "end": 3405.77,
      "text": "Won't go into this too much for right now"
    },
    {
      "start": 3406.57,
      "end": 3413.45,
      "text": "Because what's the most important for our particular purposes these days is going to be how to understand chat history"
    },
    {
      "start": 3414.17,
      "end": 3417.69,
      "text": "Now when you ask a chat to be asked a question within that one page"
    },
    {
      "start": 3417.69,
      "end": 3419.93,
      "text": "It's pretty good about keeping track of"
    },
    {
      "start": 3421.21,
      "end": 3423.16,
      "text": "most"
    },
    {
      "start": 3423.16,
      "end": 3427.08,
      "text": "Some or maybe just like two or three questions behind it's pretty good at keeping track"
    },
    {
      "start": 3427.08,
      "end": 3429.8,
      "text": "We're keeping the memory of the specific conversation"
    },
    {
      "start": 3430.36,
      "end": 3433.72,
      "text": "Now that's not you know, that's still one of those things that we are continuous"
    },
    {
      "start": 3435.0,
      "end": 3438.28,
      "text": "I think programmers are all of the first trying to improve wherever they can like"
    },
    {
      "start": 3439.64,
      "end": 3442.84,
      "text": "hallucinations and things like that are still going to be a general problem"
    },
    {
      "start": 3443.48,
      "end": 3448.68,
      "text": "But what I'd like to talk about is how do we essentially build a smaller structure of this on our"
    },
    {
      "start": 3449.48,
      "end": 3451.32,
      "text": "That we know will still work"
    },
    {
      "start": 3452.12,
      "end": 3455.72,
      "text": "Within the model itself where in this case the model for chat you can see will"
    },
    {
      "start": 3456.68,
      "end": 3460.12,
      "text": "Will at least be able to collect some amount of the memory itself"
    },
    {
      "start": 3460.36,
      "end": 3463.72,
      "text": "And we do that using the same components that we've used here so far"
    },
    {
      "start": 3464.28,
      "end": 3466.28,
      "text": "If we're creating a set of messages"
    },
    {
      "start": 3466.76,
      "end": 3470.44,
      "text": "Or if we've created that set of messages with an human message and an AI message"
    },
    {
      "start": 3470.92,
      "end": 3474.6,
      "text": "We've essentially built and prompted a specific set of conversations to"
    },
    {
      "start": 3475.4,
      "end": 3477.53,
      "text": "For our model to operate"
    },
    {
      "start": 3477.53,
      "end": 3479.77,
      "text": "And that means that when we invoke that conversation"
    },
    {
      "start": 3480.57,
      "end": 3484.41,
      "text": "This is a pretty manual way we're going to be seeing some other tools within the lang chain framework"
    },
    {
      "start": 3484.49,
      "end": 3488.25,
      "text": "But I do want you to have something to start working with down the road"
    },
    {
      "start": 3489.82,
      "end": 3494.3,
      "text": "So this code is also um built into your"
    },
    {
      "start": 3494.94,
      "end": 3499.26,
      "text": "Built into the repo so it's exactly what you're seeing in the in the"
    },
    {
      "start": 3500.94,
      "end": 3503.26,
      "text": "In the code for simple chat history"
    },
    {
      "start": 3503.42,
      "end": 3506.78,
      "text": "I'm using the message in order to showcase what a conversational look like"
    },
    {
      "start": 3507.58,
      "end": 3510.54,
      "text": "Says the chat prompt template but I hope we actually used"
    },
    {
      "start": 3511.1,
      "end": 3514.78,
      "text": "Oh, no, I'm using an old one hang on. This is not going to be the second code you add"
    },
    {
      "start": 3517.61,
      "end": 3519.45,
      "text": "Let me make sure that's correct"
    },
    {
      "start": 3520.01,
      "end": 3522.01,
      "text": "It's yeah, that should still work"
    },
    {
      "start": 3523.05,
      "end": 3524.73,
      "text": "All right, so that"
    },
    {
      "start": 3524.73,
      "end": 3526.86,
      "text": "Using an outdated"
    },
    {
      "start": 3526.86,
      "end": 3531.18,
      "text": "All that work this afternoon Jonathan and I still el-gunned up the note the wrong note book"
    },
    {
      "start": 3533.61,
      "end": 3538.41,
      "text": "Awesome, okay, so I'm gonna be calling from AI message human message chat open"
    },
    {
      "start": 3539.13,
      "end": 3541.21,
      "text": "So I'm calling out this conversation"
    },
    {
      "start": 3542.01,
      "end": 3543.53,
      "text": "AI"
    },
    {
      "start": 3543.53,
      "end": 3545.85,
      "text": "Essentially, this is going to be a tool that"
    },
    {
      "start": 3546.57,
      "end": 3548.49,
      "text": "Changes the conversation from"
    },
    {
      "start": 3548.65,
      "end": 3550.65,
      "text": "English to French love programming"
    },
    {
      "start": 3551.37,
      "end": 3556.49,
      "text": "And it says what did you just say so when we have it invoke the messages itself the idea is"
    },
    {
      "start": 3557.05,
      "end": 3560.14,
      "text": "That it should return"
    },
    {
      "start": 3560.14,
      "end": 3562.14,
      "text": "The message that we've just"
    },
    {
      "start": 3562.92,
      "end": 3565.56,
      "text": "Respond into that message should be"
    },
    {
      "start": 3566.52,
      "end": 3568.52,
      "text": "printed into the conversation"
    },
    {
      "start": 3568.76,
      "end": 3573.96,
      "text": "Hopefully we'll be able to see that and we'll be able to create a tool that looks at the message"
    },
    {
      "start": 3574.6,
      "end": 3579.72,
      "text": "Throughout at least see oh, I think I knew what I think I know what I mean wrong here"
    },
    {
      "start": 3580.44,
      "end": 3582.68,
      "text": "So if I take my Python 3 we'll just"
    },
    {
      "start": 3584.44,
      "end": 3586.44,
      "text": "Write that overall"
    },
    {
      "start": 3587.2,
      "end": 3590.48,
      "text": "This might be messy. Yeah, I'm just gonna fix this up"
    },
    {
      "start": 3591.64,
      "end": 3593.64,
      "text": "You can do gonna add"
    },
    {
      "start": 3595.53,
      "end": 3597.21,
      "text": "content"
    },
    {
      "start": 3597.21,
      "end": 3600.38,
      "text": "here"
    },
    {
      "start": 3600.38,
      "end": 3602.38,
      "text": "I'm going to add"
    },
    {
      "start": 3603.88,
      "end": 3605.88,
      "text": "Content there"
    },
    {
      "start": 3607.76,
      "end": 3610.0,
      "text": "Let's give that helps this up all right, so"
    },
    {
      "start": 3611.39,
      "end": 3614.03,
      "text": "Well, since we're here one thing to know all of the managed data"
    },
    {
      "start": 3614.03,
      "end": 3616.51,
      "text": "You can still actually see our results as part of the metadata"
    },
    {
      "start": 3616.51,
      "end": 3621.39,
      "text": "But it also gives you a lot of additional information if that's something that's relevant to you"
    },
    {
      "start": 3621.79,
      "end": 3625.31,
      "text": "With chain framework, we're going to be seeing down the road the string out parser"
    },
    {
      "start": 3625.63,
      "end": 3627.63,
      "text": "But you can also just call"
    },
    {
      "start": 3628.43,
      "end": 3634.27,
      "text": "And limit the content you're pulling from the model itself with the dot content"
    },
    {
      "start": 3635.07,
      "end": 3636.11,
      "text": "Uh"
    },
    {
      "start": 3636.11,
      "end": 3638.99,
      "text": "property here, so let me say that let me try that again"
    },
    {
      "start": 3641.0,
      "end": 3643.0,
      "text": "See try that one more time"
    },
    {
      "start": 3645.64,
      "end": 3648.19,
      "text": "That's a little better"
    },
    {
      "start": 3648.19,
      "end": 3650.28,
      "text": "All right, so"
    },
    {
      "start": 3650.28,
      "end": 3651.64,
      "text": "Let's"
    },
    {
      "start": 3651.64,
      "end": 3655.08,
      "text": "Let me just leave this up from that one more time so that it's a little bit more"
    },
    {
      "start": 3656.97,
      "end": 3658.97,
      "text": "straightforward"
    },
    {
      "start": 3658.97,
      "end": 3666.49,
      "text": "And here we have a conversation so this was the first prompt that I shared that I asked share translate the sentence from English to French"
    },
    {
      "start": 3667.13,
      "end": 3670.17,
      "text": "The the prompt that I asked the AI message to"
    },
    {
      "start": 3671.29,
      "end": 3675.85,
      "text": "Share with us. This was something that I included was to say I adore the programation"
    },
    {
      "start": 3676.49,
      "end": 3681.93,
      "text": "And the response we included that we asked into a vote was what did you just say and there's the memory working"
    },
    {
      "start": 3682.76,
      "end": 3688.6,
      "text": "Behind the scenes and it said I said I adore the programation, which I means I love programming in French"
    },
    {
      "start": 3689.08,
      "end": 3694.12,
      "text": "Not kind of like that because it essentially is telling the model itself that it's operating and still looking back"
    },
    {
      "start": 3694.84,
      "end": 3697.0,
      "text": "That information from"
    },
    {
      "start": 3697.0,
      "end": 3699.24,
      "text": "Previously up from previously included"
    },
    {
      "start": 3699.96,
      "end": 3702.76,
      "text": "So I have the ability to prompt from this particular"
    },
    {
      "start": 3704.04,
      "end": 3708.68,
      "text": "From this particular point on model itself is essentially working so that it"
    },
    {
      "start": 3710.57,
      "end": 3717.29,
      "text": "Adds another human message the human message I'm about to include back into this specific list of"
    },
    {
      "start": 3718.17,
      "end": 3723.77,
      "text": "Human message AI messaging problems. So now it says can you try that in Spanish"
    },
    {
      "start": 3725.26,
      "end": 3728.49,
      "text": "For example, I like to show this prompt because"
    },
    {
      "start": 3729.61,
      "end": 3732.81,
      "text": "Once I asked to say can you try that in Spanish it understands"
    },
    {
      "start": 3733.96,
      "end": 3740.28,
      "text": "The previous amount of language that I created earlier which is essentially just I want to translate this sentence from English to French"
    },
    {
      "start": 3740.92,
      "end": 3743.0,
      "text": "That earlier sentence was I love programming"
    },
    {
      "start": 3743.84,
      "end": 3745.68,
      "text": "This is how the memory begins to be built"
    },
    {
      "start": 3746.4,
      "end": 3752.24,
      "text": "Because the otherwise the other problem you may be fit like that you might be facing if you're just calling or invoking a specific"
    },
    {
      "start": 3754.19,
      "end": 3760.35,
      "text": "A specific prompt from that from open AI is that you'll notice that otherwise it forgets the information quickly"
    },
    {
      "start": 3760.91,
      "end": 3763.95,
      "text": "So for example, let's see how far back this goes"
    },
    {
      "start": 3765.58,
      "end": 3770.62,
      "text": "What was my first question to you"
    },
    {
      "start": 3772.11,
      "end": 3773.31,
      "text": "Let's see how well it does"
    },
    {
      "start": 3775.42,
      "end": 3779.26,
      "text": "This is where I pose that question still keeps all the information from translating"
    },
    {
      "start": 3779.58,
      "end": 3781.84,
      "text": "This first"
    },
    {
      "start": 3782.32,
      "end": 3787.04,
      "text": "This first sentence translate the sentence from English to Spanish. I love programming. That's what I said"
    },
    {
      "start": 3787.2,
      "end": 3789.2,
      "text": "That was my first statement"
    },
    {
      "start": 3790.35,
      "end": 3793.31,
      "text": "But technically speaking my first question to it was indeed"
    },
    {
      "start": 3794.57,
      "end": 3798.49,
      "text": "What did you just say? So your first question was indeed. What did you just say?"
    },
    {
      "start": 3799.5,
      "end": 3802.46,
      "text": "So the memory behind the model itself seems to be"
    },
    {
      "start": 3804.67,
      "end": 3812.11,
      "text": "Working fairly well"
    },
    {
      "start": 3812.59,
      "end": 3817.95,
      "text": "Question there go for it. Why didn't you rap man Conta program are in an AI message?"
    },
    {
      "start": 3817.95,
      "end": 3822.35,
      "text": "Like you did in the previous one when you fed it back into the history say that I'm so sorry say that again"
    },
    {
      "start": 3822.35,
      "end": 3823.55,
      "text": "I kind of missed a little bit of that there"
    },
    {
      "start": 3825.15,
      "end": 3831.71,
      "text": "Why didn't you you when you feed everything back into the history you rap all the all the prompts into human messages"
    },
    {
      "start": 3832.19,
      "end": 3838.11,
      "text": "But you don't wrap the responses back into AI messages. Why not? Oh, so what"
    },
    {
      "start": 3840.17,
      "end": 3843.53,
      "text": "So why did that why did I not wrap the oh so the AI"
    },
    {
      "start": 3844.09,
      "end": 3845.48,
      "text": "message"
    },
    {
      "start": 3845.48,
      "end": 3847.48,
      "text": "Let me make sure I understand"
    },
    {
      "start": 3847.48,
      "end": 3852.2,
      "text": "But use it to do the right AI message. Yeah, I said a human message. Can you"
    },
    {
      "start": 3852.76,
      "end": 3853.8,
      "text": "Think of them?"
    },
    {
      "start": 3853.8,
      "end": 3855.26,
      "text": "You true"
    },
    {
      "start": 3855.26,
      "end": 3856.78,
      "text": "Why did I not wrap it?"
    },
    {
      "start": 3856.78,
      "end": 3858.78,
      "text": "Good question. Why did I not wrap it?"
    },
    {
      "start": 3859.1,
      "end": 3861.1,
      "text": "Probably because I forgot"
    },
    {
      "start": 3861.1,
      "end": 3866.22,
      "text": "This looks like kind of an error on my part of like not having wrapped it up as part of a human message component there"
    },
    {
      "start": 3868.09,
      "end": 3871.18,
      "text": "Let's see"
    },
    {
      "start": 3871.18,
      "end": 3873.98,
      "text": "But I miss there human message responses"
    },
    {
      "start": 3874.94,
      "end": 3877.93,
      "text": "Although I think"
    },
    {
      "start": 3877.93,
      "end": 3879.74,
      "text": "Let me just make sure"
    },
    {
      "start": 3879.74,
      "end": 3885.9,
      "text": "You've been message problem with this question to the human message. Yeah, I guess I just forgot to actually specify that I wanted it to"
    },
    {
      "start": 3887.61,
      "end": 3889.93,
      "text": "Yeah, and that would actually create now. You're absolutely right"
    },
    {
      "start": 3889.93,
      "end": 3893.21,
      "text": "I did forget to like wrap that up as an AI message like literally just forgot"
    },
    {
      "start": 3894.17,
      "end": 3897.85,
      "text": "Essentially and the problem that might cause down the road is that"
    },
    {
      "start": 3898.57,
      "end": 3903.45,
      "text": "That's actually try that out. Thank you for calling that out. I'll make sure that's fixed up for the next session"
    },
    {
      "start": 3903.93,
      "end": 3906.76,
      "text": "but let's try"
    },
    {
      "start": 3906.92,
      "end": 3909.08,
      "text": "What that might end up"
    },
    {
      "start": 3909.08,
      "end": 3910.86,
      "text": "Happening"
    },
    {
      "start": 3910.86,
      "end": 3913.1,
      "text": "Is that let's say in try that in Spanish"
    },
    {
      "start": 3916.46,
      "end": 3917.66,
      "text": "Do-do-do"
    },
    {
      "start": 3917.66,
      "end": 3919.9,
      "text": "Says human who can try that mean come to that we're gonna show"
    },
    {
      "start": 3920.54,
      "end": 3922.78,
      "text": "Let's say what was the last thing"
    },
    {
      "start": 3925.68,
      "end": 3928.28,
      "text": "You wrote"
    },
    {
      "start": 3928.28,
      "end": 3931.48,
      "text": "I mean"
    },
    {
      "start": 3931.48,
      "end": 3933.48,
      "text": "Okay, it still recognizes that it's"
    },
    {
      "start": 3934.12,
      "end": 3937.64,
      "text": "I think that might be a little bit of a flip, but I believe that with additional contents that"
    },
    {
      "start": 3938.2,
      "end": 3943.56,
      "text": "That AI message might actually start to hallucinate fairly quickly after giving enough information"
    },
    {
      "start": 3943.88,
      "end": 3946.36,
      "text": "So for example if I were to try what was"
    },
    {
      "start": 3948.48,
      "end": 3950.89,
      "text": "The second thing you wrote"
    },
    {
      "start": 3951.69,
      "end": 3954.27,
      "text": "See if it keeps track of that"
    },
    {
      "start": 3954.27,
      "end": 3956.27,
      "text": "Okay, still okay for the most part"
    },
    {
      "start": 3957.77,
      "end": 3964.73,
      "text": "So I don't want to say that it's not necessary, but I do want to say that it's it like it's probably it's a good practice to make sure to specify that we're possible"
    },
    {
      "start": 3965.29,
      "end": 3972.79,
      "text": "So to make sure I'm following Doug's point we want to wrap the messages append on 16 and 28 with the AI message like"
    },
    {
      "start": 3973.19,
      "end": 3975.99,
      "text": "In situation situation correct where possible and this is just something that"
    },
    {
      "start": 3976.55,
      "end": 3978.07,
      "text": "That we did that I that"
    },
    {
      "start": 3978.94,
      "end": 3985.46,
      "text": "Passion I didn't properly build into the into that while statement. It's a good call out. We'll fix that next time right Jonathan"
    },
    {
      "start": 3987.98,
      "end": 3989.98,
      "text": "Yeah"
    },
    {
      "start": 3990.22,
      "end": 3992.22,
      "text": "Good call out I was like that"
    },
    {
      "start": 3993.67,
      "end": 3995.67,
      "text": "In the end very much"
    },
    {
      "start": 3995.67,
      "end": 3997.99,
      "text": "Fairly soon. This is just like"
    },
    {
      "start": 3998.87,
      "end": 4002.23,
      "text": "Here's what I like about this session. This is very basic and from here"
    },
    {
      "start": 4002.23,
      "end": 4009.99,
      "text": "You can essentially already take this and start to implement this at some of the programming like any programming tool that you might be interested in a lot of ways"
    },
    {
      "start": 4010.39,
      "end": 4020.63,
      "text": "This is kind of your base level needs for a lot of different components if all your need is an easy access to chat gpt's to open a eyes chat gpt's"
    },
    {
      "start": 4022.15,
      "end": 4025.51,
      "text": "3.5 model. You've got something to get started with"
    },
    {
      "start": 4026.34,
      "end": 4031.22,
      "text": "um and so the idea behind this is again just kind of"
    },
    {
      "start": 4032.14,
      "end": 4034.97,
      "text": "Understanding how"
    },
    {
      "start": 4035.05,
      "end": 4043.05,
      "text": "It like how easily it fits into kind of a Python framework as we move forward starting next Thursday or two days from now"
    },
    {
      "start": 4043.77,
      "end": 4048.49,
      "text": "A lot of how we frame and these type of messages are you know"
    },
    {
      "start": 4048.49,
      "end": 4054.81,
      "text": "It's not going to be good practice for us to kind of store this on a local computer or store this and kind of a temporary variable"
    },
    {
      "start": 4055.13,
      "end": 4062.01,
      "text": "You're probably more likely and not going to be pulling this information from an online rack or like you're going to be pulling from a specific document"
    },
    {
      "start": 4062.57,
      "end": 4070.41,
      "text": "This information is going to be existing on AWS the cloud langs that whatever you want to like wherever you're going to be pulling this information from you're going to expect to have a standing"
    },
    {
      "start": 4071.13,
      "end": 4076.73,
      "text": "But I want to make sure that you're familiar with essentially the step-by-step process that was happened across the states"
    },
    {
      "start": 4079.13,
      "end": 4081.93,
      "text": "So with that said the other tool that you have access to"
    },
    {
      "start": 4082.73,
      "end": 4085.93,
      "text": "If you're not interested in keeping together a"
    },
    {
      "start": 4087.19,
      "end": 4089.19,
      "text": "ongoing"
    },
    {
      "start": 4090.47,
      "end": 4095.03,
      "text": "An ongoing connection to open AI is that wow I am so pastime my apologies"
    },
    {
      "start": 4095.59,
      "end": 4102.23,
      "text": "Is that a llama is super easy to download except for the fact that it's something like six gigabytes of data"
    },
    {
      "start": 4103.11,
      "end": 4109.99,
      "text": "But the framework I know but the framework is fairly simple with a llama and with llama you can essentially"
    },
    {
      "start": 4110.39,
      "end": 4117.99,
      "text": "Create a specific tool that runs on your local computer it runs a little bit slower than most but it is another option"
    },
    {
      "start": 4118.79,
      "end": 4120.79,
      "text": "or"
    },
    {
      "start": 4122.95,
      "end": 4127.35,
      "text": "A specific chatbot if that's what you're invoking questions from a specific chatbot"
    },
    {
      "start": 4128.07,
      "end": 4136.47,
      "text": "But in a lot of ways the only thing that you need to keep in mind is that the structure of those system messages might vary depending on the type of model you're working with"
    },
    {
      "start": 4137.34,
      "end": 4144.14,
      "text": "So keep that in mind like the documentation for that in my chain is actually pretty like is somewhat standardize"
    },
    {
      "start": 4144.22,
      "end": 4146.14,
      "text": "But it's the sort of thing that you might want to be"
    },
    {
      "start": 4146.86,
      "end": 4151.66,
      "text": "Aware of as you're building as as you're building or choosing a model to work with"
    },
    {
      "start": 4154.54,
      "end": 4155.74,
      "text": "um so with that said"
    },
    {
      "start": 4157.69,
      "end": 4164.97,
      "text": "If you want to have a bit something to practice or something to build you act like with what we've covered here so far honestly"
    },
    {
      "start": 4166.12,
      "end": 4171.56,
      "text": "With just this kind of context and with a different model of your choice"
    },
    {
      "start": 4172.52,
      "end": 4178.44,
      "text": "Um, and if it's not a llama you can also use you know create an API key with something like a throbic or any of the other"
    },
    {
      "start": 4179.4,
      "end": 4186.68,
      "text": "Models that are existing within the language and framework you can create a specific tool of you know how models talk to each other"
    },
    {
      "start": 4187.46,
      "end": 4189.46,
      "text": "Which is pretty straightforward"
    },
    {
      "start": 4189.62,
      "end": 4196.58,
      "text": "But if it's something that you want to practice after having gone and seen these tools as we work on them together"
    },
    {
      "start": 4197.54,
      "end": 4204.26,
      "text": "Today you can essentially try to see how well you can keep that chat history going generally speaking like if you get it"
    },
    {
      "start": 4205.06,
      "end": 4209.78,
      "text": "Remember content from three or four levels in you've done pretty well"
    },
    {
      "start": 4212.58,
      "end": 4215.74,
      "text": "Um"
    },
    {
      "start": 4215.74,
      "end": 4218.06,
      "text": "That having been said I am over time"
    },
    {
      "start": 4218.94,
      "end": 4223.58,
      "text": "So I do not expect you to hang out any longer than you have already thank you for your patience"
    },
    {
      "start": 4224.58,
      "end": 4226.42,
      "text": "With that said you are three to go"
    },
    {
      "start": 4226.5,
      "end": 4232.58,
      "text": "But if you have questions I'm here for another 15-20 minutes and happy to answer any questions whether it's about like hey"
    },
    {
      "start": 4232.58,
      "end": 4238.5,
      "text": "What are we doing again or I'm sorry my computer just laid on fire the second I tried to run an environment variable"
    },
    {
      "start": 4239.06,
      "end": 4247.62,
      "text": "We're here for either one of those purposes, but I just want to say thank you or first aid boss great question done"
    },
    {
      "start": 4248.79,
      "end": 4251.35,
      "text": "Something always always being proven content"
    },
    {
      "start": 4251.99,
      "end": 4252.79,
      "text": "Uh"
    },
    {
      "start": 4253.03,
      "end": 4256.55,
      "text": "Yeah, and I'm by the way. Yeah, always call me out if you see messy count"
    },
    {
      "start": 4257.77,
      "end": 4259.77,
      "text": "It's gonna be uh, it's gonna be a thing"
    },
    {
      "start": 4260.25,
      "end": 4261.77,
      "text": "um"
    },
    {
      "start": 4261.77,
      "end": 4263.83,
      "text": "Yeah, if you have any questions"
    },
    {
      "start": 4263.83,
      "end": 4268.39,
      "text": "Feel free to stick around to otherwise have a wonderful evening if you're on the United States enjoy the holiday tomorrow"
    },
    {
      "start": 4268.95,
      "end": 4275.91,
      "text": "Thank you for your time"
    },
    {
      "start": 4275.91,
      "end": 4282.17,
      "text": "Thank you. Thank you. Thank you. Thank you. Thank you. Good night. Have a good night. Thank you. Thank you"
    },
    {
      "start": 4285.8,
      "end": 4286.92,
      "text": "You"
    },
    {
      "start": 4287.48,
      "end": 4297.22,
      "text": "You too. Thanks folks. Oh just in case there are questions. I'm gonna go ahead and pull up the syllabus here"
    },
    {
      "start": 4309.54,
      "end": 4310.74,
      "text": "Sorry"
    },
    {
      "start": 4310.74,
      "end": 4316.62,
      "text": "All right, can you go to the last um the last one? Yeah"
    },
    {
      "start": 4317.26,
      "end": 4322.58,
      "text": "Sorry, you know the last um how do you say it like the last slide? Yeah, yeah, exactly sorry"
    },
    {
      "start": 4324.1,
      "end": 4325.46,
      "text": "Um"
    },
    {
      "start": 4325.46,
      "end": 4330.98,
      "text": "The one of the homework. Yeah, well when when where where it says consider approaches such as summarization"
    },
    {
      "start": 4331.78,
      "end": 4339.54,
      "text": "Practice content context. I don't know if you already said it, but um what would be summarization and proactive context management"
    },
    {
      "start": 4339.94,
      "end": 4341.94,
      "text": "Yeah, let's start let"
    },
    {
      "start": 4342.66,
      "end": 4346.26,
      "text": "Yeah, since we stuck around let's talk about props there a little bit so"
    },
    {
      "start": 4347.94,
      "end": 4351.62,
      "text": "Here's our systems. Okay. Yeah, this is a great example. So"
    },
    {
      "start": 4352.5,
      "end": 4356.34,
      "text": "Let me just go ahead and share this code on the slack channel"
    },
    {
      "start": 4357.77,
      "end": 4360.02,
      "text": "extra"
    },
    {
      "start": 4360.02,
      "end": 4362.02,
      "text": "So extra code for prompts"
    },
    {
      "start": 4363.67,
      "end": 4365.43,
      "text": "So i'm going to share on the slack channel"
    },
    {
      "start": 4367.11,
      "end": 4370.7,
      "text": "this code"
    },
    {
      "start": 4370.7,
      "end": 4373.58,
      "text": "And i'm simply going to work with a couple of those steps here"
    },
    {
      "start": 4374.14,
      "end": 4376.22,
      "text": "You'll see that there's a little bit of things commented out"
    },
    {
      "start": 4376.3,
      "end": 4381.82,
      "text": "So i don't know what i would ask you to do is just create a pie file called prompts.py"
    },
    {
      "start": 4382.86,
      "end": 4384.38,
      "text": "and uh"
    },
    {
      "start": 4384.38,
      "end": 4391.21,
      "text": "What we'll do here is essentially showcase what that summarization look like so um"
    },
    {
      "start": 4393.35,
      "end": 4395.11,
      "text": "Okay, so"
    },
    {
      "start": 4395.27,
      "end": 4399.03,
      "text": "This is pretty much the standard stuff you have uh if you're if you want to use a llama"
    },
    {
      "start": 4399.03,
      "end": 4403.59,
      "text": "You can actually import that from along itself and using chat to be t. Here's my system message"
    },
    {
      "start": 4404.07,
      "end": 4407.67,
      "text": "And the system message like i said earlier. It's like i'm telling the open AI. Hey"
    },
    {
      "start": 4408.15,
      "end": 4410.87,
      "text": "I want you to work like a technical support specialist"
    },
    {
      "start": 4410.87,
      "end": 4414.39,
      "text": "I want you to predict like provide clear and concise instruction, right?"
    },
    {
      "start": 4415.03,
      "end": 4419.03,
      "text": "And what i want the human to respond is hey my computer's running slow freezing properly"
    },
    {
      "start": 4419.51,
      "end": 4421.35,
      "text": "That's what should i do response"
    },
    {
      "start": 4421.35,
      "end": 4422.95,
      "text": "This is what i'm going to have it in vogue"
    },
    {
      "start": 4423.67,
      "end": 4424.79,
      "text": "And"
    },
    {
      "start": 4424.79,
      "end": 4428.47,
      "text": "We actually don't need to worry about step two for right now. So i'm going to leave that commented out"
    },
    {
      "start": 4428.95,
      "end": 4431.24,
      "text": "So if we run this code"
    },
    {
      "start": 4431.56,
      "end": 4435.13,
      "text": "close that up"
    },
    {
      "start": 4435.13,
      "end": 4436.65,
      "text": "Close that up"
    },
    {
      "start": 4436.65,
      "end": 4438.09,
      "text": "Let's clear that out"
    },
    {
      "start": 4438.09,
      "end": 4440.65,
      "text": "So this is going to be python three prompts"
    },
    {
      "start": 4442.52,
      "end": 4444.28,
      "text": "In class example prompts pie"
    },
    {
      "start": 4445.4,
      "end": 4448.44,
      "text": "It's going to take a second and i just want to showcase what that means"
    },
    {
      "start": 4452.95,
      "end": 4456.55,
      "text": "So oh i forgot the included content here. Oh my gosh"
    },
    {
      "start": 4456.55,
      "end": 4458.79,
      "text": "I wonder what i did there so in vogue"
    },
    {
      "start": 4459.67,
      "end": 4464.44,
      "text": "include contents"
    },
    {
      "start": 4464.44,
      "end": 4466.2,
      "text": "Contents"
    },
    {
      "start": 4466.2,
      "end": 4468.2,
      "text": "Let's just make us not messy"
    },
    {
      "start": 4468.2,
      "end": 4475.83,
      "text": "Let's give that a second"
    },
    {
      "start": 4476.92,
      "end": 4480.6,
      "text": "All right that looks a little bit better. All right, so when i asked it for this prompt"
    },
    {
      "start": 4481.4,
      "end": 4485.88,
      "text": "What did it do? Dress your computer running slow freezing frequently. You can you you"
    },
    {
      "start": 4486.6,
      "end": 4491.72,
      "text": "You can try the following steps. Here's step one step two all of these are folded values"
    },
    {
      "start": 4492.28,
      "end": 4494.28,
      "text": "Step three step four step five. Okay"
    },
    {
      "start": 4494.92,
      "end": 4496.34,
      "text": "Cool"
    },
    {
      "start": 4496.34,
      "end": 4498.74,
      "text": "But let's say that i want to try something like"
    },
    {
      "start": 4500.82,
      "end": 4503.14,
      "text": "Hey, i don't want 10 steps here"
    },
    {
      "start": 4504.39,
      "end": 4506.39,
      "text": "What of the components you can do is"
    },
    {
      "start": 4506.87,
      "end": 4514.15,
      "text": "Essentially think about how you would return instructions and i'm like you want instructions in a much more concise manner for example"
    },
    {
      "start": 4514.15,
      "end": 4516.89,
      "text": "So let's say keep your"
    },
    {
      "start": 4517.77,
      "end": 4520.25,
      "text": "or"
    },
    {
      "start": 4520.25,
      "end": 4522.82,
      "text": "respond"
    },
    {
      "start": 4522.82,
      "end": 4524.66,
      "text": "Two questions"
    },
    {
      "start": 4524.66,
      "end": 4526.66,
      "text": "With only one step"
    },
    {
      "start": 4527.94,
      "end": 4533.46,
      "text": "So here i'm adding a little bit of added like context to the what to the system prompt itself"
    },
    {
      "start": 4534.26,
      "end": 4536.26,
      "text": "And it's as simple as that so"
    },
    {
      "start": 4536.5,
      "end": 4538.98,
      "text": "That means that if i'm going to now try"
    },
    {
      "start": 4539.94,
      "end": 4541.3,
      "text": "the same"
    },
    {
      "start": 4541.3,
      "end": 4542.71,
      "text": "question"
    },
    {
      "start": 4542.71,
      "end": 4547.45,
      "text": "Let's see if it works"
    },
    {
      "start": 4547.45,
      "end": 4552.73,
      "text": "Instead of nine steps. I essentially it's just on a trip performing a discipline up to free up space in your computer and group its performance"
    },
    {
      "start": 4554.12,
      "end": 4557.24,
      "text": "So i think i could be like oh sorry, no go ahead"
    },
    {
      "start": 4558.55,
      "end": 4564.55,
      "text": "I was thinking the idea would be like how we can tailor down our own prompt so that like there is we can actually use the"
    },
    {
      "start": 4564.55,
      "end": 4566.23,
      "text": "response as they input for"
    },
    {
      "start": 4566.23,
      "end": 4568.23,
      "text": "For sending the message to the other"
    },
    {
      "start": 4569.51,
      "end": 4573.27,
      "text": "Exactly so the idea exactly so the idea is like"
    },
    {
      "start": 4574.87,
      "end": 4582.12,
      "text": "Uh"
    },
    {
      "start": 4582.12,
      "end": 4588.2,
      "text": "You want to get really weird like you can just like essentially like the answer like the answer is essentially just"
    },
    {
      "start": 4589.74,
      "end": 4593.5,
      "text": "How do you how do you just like how do you create the response in a way that like"
    },
    {
      "start": 4594.78,
      "end": 4599.42,
      "text": "It's your specific needs and if it's in the if it's in the format that essentially is made to"
    },
    {
      "start": 4600.38,
      "end": 4604.94,
      "text": "That you need it to like respond to your to your LLM in a much more"
    },
    {
      "start": 4605.74,
      "end": 4608.31,
      "text": "like"
    },
    {
      "start": 4608.31,
      "end": 4610.31,
      "text": "in uh in a"
    },
    {
      "start": 4611.83,
      "end": 4617.11,
      "text": "In an AI or human message you do want to be able to like like recognize that these messages need to be"
    },
    {
      "start": 4617.67,
      "end": 4623.19,
      "text": "Delivered to the other AI and uh kind of in a structure that the other AI would be able to like understand it"
    },
    {
      "start": 4625.3,
      "end": 4631.62,
      "text": "Great question"
    },
    {
      "start": 4631.94,
      "end": 4636.1,
      "text": "And when you say in order to avoid the conversation going stable be like"
    },
    {
      "start": 4637.93,
      "end": 4639.93,
      "text": "not staying in terms of"
    },
    {
      "start": 4641.4,
      "end": 4648.28,
      "text": "Like hanging is just like diverging into something that it's completely different to what we were actually looking for"
    },
    {
      "start": 4648.76,
      "end": 4656.12,
      "text": "So it's less about so part of what you can expect and this is one of those hard things to avoid with the tools that we're using here so far"
    },
    {
      "start": 4656.12,
      "end": 4658.12,
      "text": "Unless you're connected to like"
    },
    {
      "start": 4658.6,
      "end": 4662.44,
      "text": "Um a lot of data like and especially as we go further in the classes"
    },
    {
      "start": 4662.52,
      "end": 4665.08,
      "text": "Becomes like it like we'll learn ways to prevent this"
    },
    {
      "start": 4665.56,
      "end": 4669.24,
      "text": "But if you're just kind of keeping this simple to like some of the simple"
    },
    {
      "start": 4670.44,
      "end": 4673.88,
      "text": "Like some of these simple steps one of the things are going to notice is that"
    },
    {
      "start": 4674.68,
      "end": 4678.36,
      "text": "Um, let's say that we have two LLM's talking about video games"
    },
    {
      "start": 4678.92,
      "end": 4681.48,
      "text": "What happens what starts to happen is that the"
    },
    {
      "start": 4682.12,
      "end": 4686.76,
      "text": "Conversation start repeating the set themselves the messages start a start to"
    },
    {
      "start": 4687.72,
      "end": 4689.72,
      "text": "Be almost repetitive"
    },
    {
      "start": 4689.8,
      "end": 4693.24,
      "text": "And so like when it repeats itself, it sort of like gets echoed back"
    },
    {
      "start": 4693.96,
      "end": 4699.4,
      "text": "To the LLM and I had an example that I was going to try to show this and like I couldn't get it to work because"
    },
    {
      "start": 4700.36,
      "end": 4701.88,
      "text": "Just didn't have time unfortunately"
    },
    {
      "start": 4701.88,
      "end": 4707.72,
      "text": "But the idea is that what you're going to start to notice is that if you try to create a conversation with these two LLMs"
    },
    {
      "start": 4707.72,
      "end": 4709.72,
      "text": "You could expect to see a very"
    },
    {
      "start": 4710.36,
      "end": 4711.64,
      "text": "Um"
    },
    {
      "start": 4711.64,
      "end": 4715.32,
      "text": "Yeah, like we're essentially a quest to start to repeat themselves over and over again"
    },
    {
      "start": 4716.2,
      "end": 4718.2,
      "text": "Unless you prompted to like stop"
    },
    {
      "start": 4721.14,
      "end": 4723.3,
      "text": "Doesn't work because I was working with a different library"
    },
    {
      "start": 4724.47,
      "end": 4726.47,
      "text": "Sorry, yeah"
    },
    {
      "start": 4726.47,
      "end": 4727.51,
      "text": "Excellent"
    },
    {
      "start": 4727.51,
      "end": 4729.19,
      "text": "No, thank you"
    },
    {
      "start": 4729.19,
      "end": 4732.15,
      "text": "Thank you. So I'll I'll go away then"
    },
    {
      "start": 4732.15,
      "end": 4735.19,
      "text": "No, thank you. You're all right. You've got me for another 10 minutes. So"
    },
    {
      "start": 4735.99,
      "end": 4738.55,
      "text": "You have another process for him, but otherwise have a wonderful evening"
    },
    {
      "start": 4740.44,
      "end": 4742.84,
      "text": "Wonder if full evening for you two guys"
    },
    {
      "start": 4744.22,
      "end": 4750.76,
      "text": "Like"
    },
    {
      "start": 4750.76,
      "end": 4753.32,
      "text": "I kind of like a general question like about the course"
    },
    {
      "start": 4754.04,
      "end": 4755.0,
      "text": "um"
    },
    {
      "start": 4755.0,
      "end": 4756.28,
      "text": "so"
    },
    {
      "start": 4756.28,
      "end": 4760.84,
      "text": "Like right now I'm on my personal computer and I know the capstone project like"
    },
    {
      "start": 4760.84,
      "end": 4765.96,
      "text": "The goal is to do something like for my company for my team. Is it okay? Like is this course"
    },
    {
      "start": 4766.68,
      "end": 4769.48,
      "text": "Fine if I just do the entire thing on my personal computer"
    },
    {
      "start": 4770.6,
      "end": 4775.0,
      "text": "Okay, cool. Oh, yeah. No, like I'm doing this off of a MacBook. Okay. Cool. Yeah"
    },
    {
      "start": 4777.45,
      "end": 4780.65,
      "text": "Yeah, I think the only like the generally the things you might want to be"
    },
    {
      "start": 4782.14,
      "end": 4784.22,
      "text": "Concern on it if you're doing this on your personal computer"
    },
    {
      "start": 4784.86,
      "end": 4787.42,
      "text": "And you're using sensitive data from your company that might not be"
    },
    {
      "start": 4788.38,
      "end": 4790.38,
      "text": "Yeah, definitely not. Yeah"
    },
    {
      "start": 4792.06,
      "end": 4805.54,
      "text": "Cool. Okay. Thank you. Thank you. That was cool about the history. I'd never"
    },
    {
      "start": 4806.58,
      "end": 4810.02,
      "text": "thought like keeping the history like that. I'd never"
    },
    {
      "start": 4810.98,
      "end": 4815.7,
      "text": "Because I've experimented before but it was always like a one prompt kind of thing and I didn't even know"
    },
    {
      "start": 4816.58,
      "end": 4818.65,
      "text": "I was doing"
    },
    {
      "start": 4818.73,
      "end": 4825.85,
      "text": "The few shot prompts. I just like like I gave it a few examples to base off like the generator response off of"
    },
    {
      "start": 4826.49,
      "end": 4832.89,
      "text": "But I was curious like how to make it more of like a historical context to if I wanted to continue asking that same questions"
    },
    {
      "start": 4832.89,
      "end": 4836.65,
      "text": "That was cool. Yeah. Yeah. Yeah. No, I mean and again, this is"
    },
    {
      "start": 4837.45,
      "end": 4840.81,
      "text": "This is just using like we're not even using all of"
    },
    {
      "start": 4842.09,
      "end": 4846.33,
      "text": "We're not even using the the line chain framework yet for that. This is just"
    },
    {
      "start": 4847.42,
      "end": 4852.3,
      "text": "Sticking with Python. Yeah, basic content. So yeah, it gets better"
    },
    {
      "start": 4853.18,
      "end": 4865.21,
      "text": "Cool. Yeah, sweet. And then it's tomorrow"
    },
    {
      "start": 4866.17,
      "end": 4867.37,
      "text": "like"
    },
    {
      "start": 4867.37,
      "end": 4870.01,
      "text": "I think is it just like office hours kind of deal"
    },
    {
      "start": 4874.3,
      "end": 4876.3,
      "text": "Yeah, tomorrow the"
    },
    {
      "start": 4876.38,
      "end": 4879.58,
      "text": "The office hours will be at the same time 80 Stern"
    },
    {
      "start": 4880.38,
      "end": 4884.46,
      "text": "And like I was saying earlier we'll go over the first 10 to 20 minutes"
    },
    {
      "start": 4885.58,
      "end": 4889.26,
      "text": "Solution to the homework and the rest of it will basically"
    },
    {
      "start": 4889.9,
      "end": 4892.14,
      "text": "The bring your questions and okay"
    },
    {
      "start": 4893.74,
      "end": 4898.14,
      "text": "Completely optional, but definitely encourage for anybody that then wants to come"
    },
    {
      "start": 4900.5,
      "end": 4903.62,
      "text": "Sweet"
    },
    {
      "start": 4903.62,
      "end": 4907.06,
      "text": "All right, awesome. That was it for me. Thank you guys very much"
    },
    {
      "start": 4908.82,
      "end": 4910.82,
      "text": "Yeah, have a good night. Yeah, you too"
    }
  ],
  "metadata": {},
  "file_size_mb": 161.0305528640747,
  "model_used": "base",
  "timestamp": "2025-08-14 13:49:24"
}